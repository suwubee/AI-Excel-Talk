import pandas as pd
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
from openpyxl.worksheet.table import Table, TableStyleInfo
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
import io
import tempfile
import os

class SmartExcelAnalyzer:
    """æ™ºèƒ½Excelåˆ†æå™¨ - è‡ªåŠ¨è¯†åˆ«å’Œå¤„ç†å„ç§Excelæ–‡ä»¶ç»“æ„"""
    
    def __init__(self):
        self.workbook = None
        self.analysis_result = {}
    
    def analyze_excel_structure(self, file_path: str) -> Dict[str, Any]:
        """å…¨é¢åˆ†æExcelæ–‡ä»¶ç»“æ„"""
        try:
            self.workbook = openpyxl.load_workbook(file_path, data_only=True)
            
            analysis = {
                'file_path': file_path,
                'total_sheets': len(self.workbook.sheetnames),
                'sheet_names': self.workbook.sheetnames,
                'sheets_analysis': {},
                'cross_sheet_analysis': {},
                'ai_prompt_data': {}
            }
            
            for sheet_name in self.workbook.sheetnames:
                sheet_analysis = self._analyze_worksheet(sheet_name)
                analysis['sheets_analysis'][sheet_name] = sheet_analysis
            
            # è·¨å·¥ä½œè¡¨å…³è”åˆ†æ
            analysis['cross_sheet_analysis'] = self._analyze_cross_sheet_relationships(analysis['sheets_analysis'])
            
            # ç”ŸæˆAIæç¤ºè¯æ•°æ®
            analysis['ai_prompt_data'] = self._generate_ai_prompt_data(analysis)
            
            self.analysis_result = analysis
            return analysis
            
        except Exception as e:
            raise Exception(f"åˆ†æExcelæ–‡ä»¶ç»“æ„æ—¶å‡ºé”™: {str(e)}")
    
    def _analyze_worksheet(self, sheet_name: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå·¥ä½œè¡¨çš„ç»“æ„"""
        ws = self.workbook[sheet_name]
        
        # åŸºæœ¬ä¿¡æ¯åˆ†æ
        basic_info = {
            'max_row': ws.max_row,
            'max_column': ws.max_column,
            'total_cells': ws.max_row * ws.max_column
        }
        
        # åˆå¹¶å•å…ƒæ ¼åˆ†æ
        merged_analysis = self._analyze_merged_cells(ws)
        
        # æ•°æ®åŒºåŸŸæ£€æµ‹
        data_regions = self._detect_data_regions(ws)
        
        # è¡¨æ ¼ç»“æ„è¯†åˆ«
        table_structures = self._identify_table_structures(ws, data_regions)
        
        # å­—æ®µåˆ†æï¼ˆæ ¸å¿ƒæ–°å¢åŠŸèƒ½ï¼‰
        field_analysis = self._analyze_fields_and_content(ws, table_structures)
        
        # ç”Ÿæˆè¯»å–å»ºè®®
        read_suggestions = self._generate_read_suggestions(table_structures, merged_analysis)
        
        return {
            'basic_info': basic_info,
            'merged_cells': merged_analysis,
            'data_regions': data_regions,
            'table_structures': table_structures,
            'field_analysis': field_analysis,  # æ–°å¢
            'read_suggestions': read_suggestions
        }
    
    def _analyze_fields_and_content(self, ws, table_structures: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå­—æ®µåå’Œæ•°æ®å†…å®¹ - æ ¸å¿ƒåŠŸèƒ½"""
        if not table_structures:
            return {"status": "no_table_detected", "fields": [], "data_samples": []}
        
        main_table = table_structures[0]
        region = main_table['region']
        header_analysis = main_table['header_analysis']
        
        min_row, max_row = region['min_row'], region['max_row']
        min_col, max_col = region['min_col'], region['max_col']
        header_row = header_analysis['suggested_header_row']
        
        # æå–å­—æ®µä¿¡æ¯
        fields_info = []
        
        for col in range(min_col, max_col + 1):
            field_info = self._analyze_single_field(ws, col, header_row, max_row)
            if field_info:
                fields_info.append(field_info)
        
        # æå–æ•°æ®æ ·æœ¬
        data_samples = self._extract_data_samples(ws, header_row + 1, max_row, min_col, max_col, limit=10)
        
        # æ•°æ®æ¦‚è¿°
        data_summary = self._generate_data_summary(fields_info, data_samples)
        
        return {
            "status": "success",
            "header_row": header_row,
            "fields_count": len(fields_info),
            "data_rows_count": max_row - header_row,
            "fields": fields_info,
            "data_samples": data_samples,
            "data_summary": data_summary
        }
    
    def _analyze_single_field(self, ws, col: int, header_row: int, max_row: int) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå­—æ®µçš„è¯¦ç»†ä¿¡æ¯"""
        # è·å–å­—æ®µå
        header_cell = ws.cell(row=header_row, column=col)
        field_name = str(header_cell.value).strip() if header_cell.value else f"å­—æ®µ{col}"
        
        # æ”¶é›†è¯¥åˆ—çš„æ•°æ®æ ·æœ¬ï¼ˆæœ€å¤š50ä¸ªï¼‰
        data_values = []
        sample_limit = min(50, max_row - header_row)
        
        for row in range(header_row + 1, header_row + 1 + sample_limit):
            if row > max_row:
                break
            cell = ws.cell(row=row, column=col)
            if cell.value is not None:
                data_values.append(cell.value)
        
        # åˆ†ææ•°æ®ç±»å‹å’Œç‰¹å¾
        field_analysis = self._deep_analyze_field_data(data_values)
        
        return {
            "column_index": col,
            "column_letter": get_column_letter(col),
            "field_name": field_name,
            "original_field_name": str(header_cell.value) if header_cell.value else "",
            "data_count": len(data_values),
            "sample_values": [str(v)[:50] for v in data_values[:5]],  # å‰5ä¸ªæ ·æœ¬
            "data_type_analysis": field_analysis,
            "is_key_field": self._is_potential_key_field(field_name, data_values),
            "field_patterns": self._detect_field_patterns(data_values)
        }
    
    def _deep_analyze_field_data(self, values: List[Any]) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æå­—æ®µæ•°æ®ç±»å‹å’Œç‰¹å¾"""
        if not values:
            return {"primary_type": "empty", "confidence": 1.0, "characteristics": []}
        
        analysis = {
            "total_values": len(values),
            "unique_values": len(set(str(v) for v in values)),
            "null_count": 0,
            "type_distribution": {"numeric": 0, "date": 0, "text": 0, "boolean": 0, "id": 0},
            "characteristics": [],
            "value_range": {},
            "common_values": []
        }
        
        # ç±»å‹æ£€æµ‹
        for value in values:
            if value is None or str(value).strip() == "":
                analysis["null_count"] += 1
                continue
                
            str_value = str(value).strip()
            
            # IDç±»å‹æ£€æµ‹
            if self._looks_like_id(str_value):
                analysis["type_distribution"]["id"] += 1
            # æ•°å€¼æ£€æµ‹
            elif self._is_numeric(value):
                analysis["type_distribution"]["numeric"] += 1
            # æ—¥æœŸæ£€æµ‹
            elif self._is_date_like(str_value):
                analysis["type_distribution"]["date"] += 1
            # å¸ƒå°”å€¼æ£€æµ‹
            elif self._is_boolean_like(str_value):
                analysis["type_distribution"]["boolean"] += 1
            else:
                analysis["type_distribution"]["text"] += 1
        
        # ç¡®å®šä¸»è¦ç±»å‹
        valid_count = len(values) - analysis["null_count"]
        if valid_count > 0:
            primary_type = max(analysis["type_distribution"].keys(), 
                             key=lambda k: analysis["type_distribution"][k])
            confidence = analysis["type_distribution"][primary_type] / valid_count
            
            # æ•°å€¼åˆ†æ
            if primary_type == "numeric":
                numeric_values = [float(v) for v in values if self._is_numeric(v)]
                if numeric_values:
                    analysis["value_range"] = {
                        "min": min(numeric_values),
                        "max": max(numeric_values),
                        "avg": sum(numeric_values) / len(numeric_values)
                    }
            
            # æ–‡æœ¬é•¿åº¦åˆ†æ
            if primary_type == "text":
                text_lengths = [len(str(v)) for v in values if v is not None]
                if text_lengths:
                    analysis["value_range"] = {
                        "min_length": min(text_lengths),
                        "max_length": max(text_lengths),
                        "avg_length": sum(text_lengths) / len(text_lengths)
                    }
            
            # å¸¸è§å€¼ç»Ÿè®¡
            from collections import Counter
            value_counts = Counter(str(v) for v in values if v is not None)
            analysis["common_values"] = value_counts.most_common(3)
            
            # ç‰¹å¾åˆ†æ
            if analysis["unique_values"] == analysis["total_values"] - analysis["null_count"]:
                analysis["characteristics"].append("å”¯ä¸€å€¼å­—æ®µ")
            
            if primary_type == "id":
                analysis["characteristics"].append("ç–‘ä¼¼IDå­—æ®µ")
            
            if analysis["null_count"] > len(values) * 0.3:
                analysis["characteristics"].append("å­˜åœ¨å¤§é‡ç©ºå€¼")
                
        else:
            primary_type = "empty"
            confidence = 1.0
        
        analysis["primary_type"] = primary_type
        analysis["confidence"] = confidence
        
        return analysis
    
    def _analyze_merged_cells(self, ws) -> Dict[str, Any]:
        """åˆ†æåˆå¹¶å•å…ƒæ ¼æ¨¡å¼"""
        merged_ranges = list(ws.merged_cells.ranges)
        
        analysis = {
            'count': len(merged_ranges),
            'header_merges': 0,
            'data_merges': 0,
            'complex_structure': False,
            'ranges_info': []
        }
        
        for merged_range in merged_ranges:
            range_info = {
                'range': str(merged_range),
                'start_row': merged_range.min_row,
                'end_row': merged_range.max_row,
                'start_col': merged_range.min_col,
                'end_col': merged_range.max_col,
                'span_rows': merged_range.max_row - merged_range.min_row + 1,
                'span_cols': merged_range.max_col - merged_range.min_col + 1
            }
            
            # è·å–åˆå¹¶å•å…ƒæ ¼çš„å€¼
            top_left_cell = ws.cell(row=merged_range.min_row, column=merged_range.min_col)
            range_info['value'] = str(top_left_cell.value)[:50] if top_left_cell.value else ""
            
            # åˆ¤æ–­æ˜¯æ ‡é¢˜åˆå¹¶è¿˜æ˜¯æ•°æ®åˆå¹¶
            if merged_range.min_row <= 5:
                analysis['header_merges'] += 1
                range_info['type'] = 'header'
            else:
                analysis['data_merges'] += 1
                range_info['type'] = 'data'
            
            analysis['ranges_info'].append(range_info)
        
        analysis['complex_structure'] = len(merged_ranges) > 10 or analysis['header_merges'] > 5
        
        return analysis
    
    def _detect_data_regions(self, ws) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ£€æµ‹æ•°æ®åŒºåŸŸ"""
        data_cells = []
        
        # æ‰«æå·¥ä½œè¡¨ï¼Œé™åˆ¶æ‰«æèŒƒå›´ä»¥æé«˜æ€§èƒ½
        max_scan_row = min(ws.max_row, 500)
        max_scan_col = min(ws.max_column, 100)
        
        for row in range(1, max_scan_row + 1):
            for col in range(1, max_scan_col + 1):
                cell = ws.cell(row=row, column=col)
                if cell.value is not None and str(cell.value).strip():
                    data_cells.append((row, col, str(cell.value).strip()))
        
        if not data_cells:
            return []
        
        # åˆ†ææ•°æ®åˆ†å¸ƒæ¨¡å¼
        return self._cluster_data_regions(data_cells)
    
    def _cluster_data_regions(self, data_cells: List[Tuple[int, int, str]]) -> List[Dict[str, Any]]:
        """èšç±»æ•°æ®å•å…ƒæ ¼å½¢æˆæœ‰æ„ä¹‰çš„åŒºåŸŸ"""
        if not data_cells:
            return []
        
        sorted_cells = sorted(data_cells, key=lambda x: (x[0], x[1]))
        
        # è®¡ç®—æ•´ä½“æ•°æ®è¾¹ç•Œ
        min_row = min(cell[0] for cell in sorted_cells)
        max_row = max(cell[0] for cell in sorted_cells)
        min_col = min(cell[1] for cell in sorted_cells)
        max_col = max(cell[1] for cell in sorted_cells)
        
        # è®¡ç®—æ•°æ®å¯†åº¦
        total_area = (max_row - min_row + 1) * (max_col - min_col + 1)
        data_density = len(data_cells) / total_area if total_area > 0 else 0
        
        # åˆ†æè¡Œåˆ—æ•°æ®åˆ†å¸ƒ
        row_data_count = {}
        col_data_count = {}
        
        for row, col, value in data_cells:
            row_data_count[row] = row_data_count.get(row, 0) + 1
            col_data_count[col] = col_data_count.get(col, 0) + 1
        
        # è¯†åˆ«ä¸»è¦æ•°æ®åŒºåŸŸ
        main_region = {
            'id': 1,
            'min_row': min_row,
            'max_row': max_row,
            'min_col': min_col,
            'max_col': max_col,
            'cell_count': len(data_cells),
            'density': data_density,
            'row_distribution': row_data_count,
            'col_distribution': col_data_count,
            'type': self._classify_region_type(data_density, row_data_count, col_data_count),
            'sample_values': [cell[2][:30] for cell in sorted_cells[:5]]
        }
        
        return [main_region]
    
    def _classify_region_type(self, density: float, row_dist: Dict[int, int], col_dist: Dict[int, int]) -> str:
        """åˆ†ç±»æ•°æ®åŒºåŸŸç±»å‹"""
        if density > 0.7:
            return "dense_table"
        elif density > 0.3:
            return "standard_table"
        else:
            return "sparse_data"
    
    def _identify_table_structures(self, ws, data_regions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«è¡¨æ ¼ç»“æ„"""
        table_structures = []
        
        for region in data_regions:
            table_info = self._analyze_table_structure(ws, region)
            if table_info:
                table_structures.append(table_info)
        
        return table_structures
    
    def _analyze_table_structure(self, ws, region: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æè¡¨æ ¼ç»“æ„"""
        min_row, max_row = region['min_row'], region['max_row']
        min_col, max_col = region['min_col'], region['max_col']
        
        # æ™ºèƒ½è¯†åˆ«è¡¨å¤´ä½ç½®
        header_analysis = self._smart_header_detection(ws, min_row, max_row, min_col, max_col)
        
        # æ£€æµ‹æ•°æ®å¼€å§‹è¡Œ
        data_start_row = self._find_data_start_row(ws, min_row, max_row, min_col, max_col, header_analysis)
        
        # åˆ†æè¡¨æ ¼ç±»å‹å’Œæ–¹å‘
        table_type = self._classify_table_type(max_row - min_row + 1, max_col - min_col + 1)
        
        # åˆ—æ•°æ®ç±»å‹åˆ†æ
        column_analysis = self._analyze_column_types(ws, data_start_row, max_row, min_col, max_col)
        
        return {
            'region': region,
            'header_analysis': header_analysis,
            'data_start_row': data_start_row,
            'table_type': table_type,
            'column_analysis': column_analysis,
            'data_quality': self._assess_data_quality(ws, data_start_row, max_row, min_col, max_col)
        }
    
    def _smart_header_detection(self, ws, min_row: int, max_row: int, min_col: int, max_col: int) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€æµ‹è¡¨å¤´"""
        header_candidates = []
        
        # æ£€æŸ¥å‰10è¡Œçš„å†…å®¹ç‰¹å¾
        for row in range(min_row, min(min_row + 10, max_row + 1)):
            row_analysis = {
                'row': row,
                'text_ratio': 0,
                'unique_values': 0,
                'formatting_score': 0,
                'content_sample': []
            }
            
            values_in_row = []
            text_count = 0
            
            for col in range(min_col, max_col + 1):
                cell = ws.cell(row=row, column=col)
                if cell.value is not None:
                    value = str(cell.value).strip()
                    values_in_row.append(value)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬ï¼ˆéçº¯æ•°å­—ï¼‰
                    try:
                        float(value)
                    except (ValueError, TypeError):
                        text_count += 1
                    
                    # æ£€æŸ¥æ ¼å¼ç‰¹å¾ï¼ˆç²—ä½“ã€é¢œè‰²ç­‰ï¼‰
                    if hasattr(cell, 'font') and cell.font and cell.font.bold:
                        row_analysis['formatting_score'] += 1
            
            if values_in_row:
                row_analysis['text_ratio'] = text_count / len(values_in_row)
                row_analysis['unique_values'] = len(set(values_in_row))
                row_analysis['content_sample'] = values_in_row[:5]
                
                # åˆ¤æ–­æ˜¯å¦å¯èƒ½æ˜¯è¡¨å¤´
                if (row_analysis['text_ratio'] > 0.6 or  # å¤§éƒ¨åˆ†æ˜¯æ–‡æœ¬
                    row_analysis['formatting_score'] > 0 or  # æœ‰æ ¼å¼ç‰¹å¾
                    len(set(values_in_row)) == len(values_in_row)):  # å…¨éƒ¨éƒ½æ˜¯å”¯ä¸€å€¼
                    header_candidates.append(row_analysis)
        
        # é€‰æ‹©æœ€ä½³è¡¨å¤´è¡Œ
        best_header_row = min_row
        if header_candidates:
            # ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä½³è¡¨å¤´
            best_candidate = max(header_candidates, 
                               key=lambda x: x['text_ratio'] * 0.4 + 
                                           (x['formatting_score'] > 0) * 0.3 + 
                                           (x['unique_values'] > 1) * 0.3)
            best_header_row = best_candidate['row']
        
        return {
            'candidates': header_candidates,
            'suggested_header_row': best_header_row,
            'confidence': len(header_candidates) / max(1, min(10, max_row - min_row + 1))
        }
    
    def _find_data_start_row(self, ws, min_row: int, max_row: int, min_col: int, max_col: int, header_analysis: Dict) -> int:
        """æ‰¾åˆ°æ•°æ®å¼€å§‹è¡Œ"""
        suggested_header = header_analysis.get('suggested_header_row', min_row)
        
        # ä»å»ºè®®çš„è¡¨å¤´è¡Œåå¼€å§‹æŸ¥æ‰¾æ•°æ®
        for row in range(suggested_header + 1, max_row + 1):
            consecutive_data = 0
            for col in range(min_col, max_col + 1):
                cell = ws.cell(row=row, column=col)
                if cell.value is not None:
                    consecutive_data += 1
                else:
                    break
            
            # å¦‚æœè¯¥è¡Œæœ‰è¿ç»­çš„æ•°æ®ï¼Œè®¤ä¸ºæ˜¯æ•°æ®å¼€å§‹è¡Œ
            if consecutive_data >= (max_col - min_col + 1) * 0.5:
                return row
        
        return suggested_header + 1 if suggested_header < max_row else min_row
    
    def _classify_table_type(self, rows: int, cols: int) -> str:
        """åˆ†ç±»è¡¨æ ¼ç±»å‹"""
        if rows > cols * 4:
            return "vertical_long"  # çºµå‘é•¿è¡¨
        elif cols > rows * 4:
            return "horizontal_wide"  # æ¨ªå‘å®½è¡¨
        elif rows > 50 and cols > 20:
            return "large_matrix"  # å¤§å‹çŸ©é˜µè¡¨
        elif rows <= 10 or cols <= 3:
            return "small_table"  # å°è¡¨æ ¼
        else:
            return "standard_table"  # æ ‡å‡†è¡¨æ ¼
    
    def _analyze_column_types(self, ws, start_row: int, end_row: int, start_col: int, end_col: int) -> List[Dict[str, Any]]:
        """åˆ†ææ¯åˆ—çš„æ•°æ®ç±»å‹"""
        columns_info = []
        
        for col in range(start_col, end_col + 1):
            values = []
            # é‡‡æ ·åˆ†æï¼ˆæœ€å¤šå–50ä¸ªå€¼ï¼‰
            sample_rows = list(range(start_row, min(start_row + 50, end_row + 1)))
            
            for row in sample_rows:
                cell = ws.cell(row=row, column=col)
                if cell.value is not None:
                    values.append(cell.value)
            
            if values:
                col_info = {
                    'column_index': col,
                    'column_letter': get_column_letter(col),
                    'sample_size': len(values),
                    'data_type': self._detect_column_data_type(values),
                    'null_ratio': (len(sample_rows) - len(values)) / len(sample_rows),
                    'unique_ratio': len(set(values)) / len(values),
                    'sample_values': [str(v)[:20] for v in values[:3]]
                }
                columns_info.append(col_info)
        
        return columns_info
    
    def _detect_column_data_type(self, values: List[Any]) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€æµ‹åˆ—æ•°æ®ç±»å‹"""
        if not values:
            return {"primary_type": "empty", "confidence": 1.0}
        
        type_counts = {"numeric": 0, "date": 0, "text": 0, "boolean": 0, "mixed": 0}
        
        for value in values[:20]:  # æ£€æŸ¥å‰20ä¸ªå€¼
            str_value = str(value).strip().lower()
            
            # æ•°å€¼æ£€æµ‹
            try:
                float(value)
                type_counts["numeric"] += 1
                continue
            except (ValueError, TypeError):
                pass
            
            # å¸ƒå°”å€¼æ£€æµ‹
            if str_value in ['true', 'false', 'æ˜¯', 'å¦', 'yes', 'no', '1', '0']:
                type_counts["boolean"] += 1
                continue
            
            # æ—¥æœŸæ£€æµ‹
            try:
                pd.to_datetime(str_value)
                type_counts["date"] += 1
                continue
            except:
                pass
            
            # é»˜è®¤ä¸ºæ–‡æœ¬
            type_counts["text"] += 1
        
        # ç¡®å®šä¸»è¦ç±»å‹
        total_checked = sum(type_counts.values())
        primary_type = max(type_counts.keys(), key=lambda k: type_counts[k])
        confidence = type_counts[primary_type] / total_checked if total_checked > 0 else 0
        
        return {
            "primary_type": primary_type,
            "confidence": confidence,
            "type_distribution": type_counts
        }
    
    def _assess_data_quality(self, ws, start_row: int, end_row: int, start_col: int, end_col: int) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        total_cells = (end_row - start_row + 1) * (end_col - start_col + 1)
        empty_cells = 0
        
        # é‡‡æ ·æ£€æŸ¥æ•°æ®è´¨é‡
        sample_size = min(1000, total_cells)
        checked_cells = 0
        
        for row in range(start_row, end_row + 1):
            for col in range(start_col, end_col + 1):
                if checked_cells >= sample_size:
                    break
                
                cell = ws.cell(row=row, column=col)
                if cell.value is None or str(cell.value).strip() == "":
                    empty_cells += 1
                
                checked_cells += 1
            
            if checked_cells >= sample_size:
                break
        
        completeness = 1 - (empty_cells / checked_cells) if checked_cells > 0 else 0
        
        return {
            "completeness": completeness,
            "total_cells_sampled": checked_cells,
            "empty_cells": empty_cells,
            "quality_score": completeness  # å¯ä»¥æ‰©å±•æ›´å¤šè´¨é‡æŒ‡æ ‡
        }
    
    def _generate_read_suggestions(self, table_structures: List[Dict[str, Any]], merged_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½è¯»å–å»ºè®®"""
        if not table_structures:
            return {
                "method": "simple_read",
                "parameters": {},
                "warnings": ["æ— æ³•æ£€æµ‹åˆ°æœ‰æ•ˆçš„è¡¨æ ¼ç»“æ„"]
            }
        
        main_table = table_structures[0]
        header_analysis = main_table.get('header_analysis', {})
        
        suggestions = {
            "method": "intelligent_read",
            "parameters": {
                "header": header_analysis.get('suggested_header_row', 1) - 1,  # pandasä½¿ç”¨0-based
                "skiprows": header_analysis.get('suggested_header_row', 1) - 1,
                "nrows": None,
                "usecols": None
            },
            "preprocessing_steps": [],
            "warnings": [],
            "confidence": header_analysis.get('confidence', 0.5)
        }
        
        # åŸºäºè¡¨æ ¼ç±»å‹æ·»åŠ å»ºè®®
        table_type = main_table.get('table_type', 'standard_table')
        if table_type == "horizontal_wide":
            suggestions["preprocessing_steps"].append("è€ƒè™‘è½¬ç½®è¡¨æ ¼")
            suggestions["warnings"].append("æ£€æµ‹åˆ°å®½è¡¨æ ¼ï¼Œå¯èƒ½éœ€è¦è½¬ç½®å¤„ç†")
        elif table_type == "large_matrix":
            suggestions["preprocessing_steps"].append("åˆ†å—è¯»å–å¤§å‹æ•°æ®")
            suggestions["warnings"].append("æ£€æµ‹åˆ°å¤§å‹è¡¨æ ¼ï¼Œå»ºè®®åˆ†æ‰¹å¤„ç†")
        
        # åŸºäºåˆå¹¶å•å…ƒæ ¼æƒ…å†µæ·»åŠ å»ºè®®
        if merged_analysis.get('complex_structure', False):
            suggestions["preprocessing_steps"].append("å¤„ç†å¤æ‚çš„åˆå¹¶å•å…ƒæ ¼")
            suggestions["warnings"].append("å­˜åœ¨å¤æ‚çš„åˆå¹¶å•å…ƒæ ¼ç»“æ„ï¼Œå¯èƒ½å½±å“æ•°æ®è¯»å–")
        
        # åŸºäºæ•°æ®è´¨é‡æ·»åŠ å»ºè®®
        data_quality = main_table.get('data_quality', {})
        if data_quality.get('completeness', 1) < 0.8:
            suggestions["preprocessing_steps"].append("å¤„ç†ç¼ºå¤±å€¼")
            suggestions["warnings"].append(f"æ•°æ®å®Œæ•´æ€§è¾ƒä½({data_quality.get('completeness', 0):.1%})ï¼Œå­˜åœ¨è¾ƒå¤šç©ºå€¼")
        
        return suggestions
    
    def _is_numeric(self, value) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ•°å€¼"""
        try:
            float(value)
            return True
        except (ValueError, TypeError):
            return False
    
    def _is_date_like(self, value: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åƒæ—¥æœŸ"""
        try:
            pd.to_datetime(value)
            return True
        except:
            # æ£€æŸ¥å¸¸è§æ—¥æœŸæ ¼å¼
            import re
            date_patterns = [
                r'\d{4}-\d{1,2}-\d{1,2}',
                r'\d{4}/\d{1,2}/\d{1,2}',
                r'\d{1,2}-\d{1,2}-\d{4}',
                r'\d{1,2}/\d{1,2}/\d{4}'
            ]
            return any(re.match(pattern, value) for pattern in date_patterns)
    
    def _is_boolean_like(self, value: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åƒå¸ƒå°”å€¼"""
        return value.lower() in ['true', 'false', 'æ˜¯', 'å¦', 'yes', 'no', '1', '0', 'âœ“', 'Ã—']
    
    def _looks_like_id(self, value: str) -> bool:
        """æ£€æµ‹æ˜¯å¦åƒID"""
        import re
        # IDç‰¹å¾ï¼šè¿ç»­æ•°å­—ã€åŒ…å«å­—æ¯æ•°å­—ç»„åˆã€ç‰¹å®šæ ¼å¼
        patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[A-Za-z]\d+$',  # å­—æ¯+æ•°å­—
            r'^[A-Za-z]{2,}\d+$',  # å¤šå­—æ¯+æ•°å­—
            r'^\d+-\d+$',  # æ•°å­—-æ•°å­—
            r'^[A-Za-z0-9]{8,}$'  # é•¿å­—ç¬¦ä¸²
        ]
        return any(re.match(pattern, value) for pattern in patterns)
    
    def _is_potential_key_field(self, field_name: str, values: List[Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ½œåœ¨çš„å…³é”®å­—æ®µ"""
        field_name_lower = field_name.lower()
        key_indicators = ['id', 'key', 'code', 'ç¼–å·', 'ä»£ç ', 'åºå·', 'uuid', 'guid']
        
        # å­—æ®µååŒ…å«å…³é”®æŒ‡ç¤ºè¯
        name_indicates_key = any(indicator in field_name_lower for indicator in key_indicators)
        
        # æ•°æ®ç‰¹å¾åˆ†æ
        unique_ratio = len(set(str(v) for v in values)) / len(values) if values else 0
        is_mostly_unique = unique_ratio > 0.8
        
        return name_indicates_key or is_mostly_unique
    
    def _detect_field_patterns(self, values: List[Any]) -> List[str]:
        """æ£€æµ‹å­—æ®µçš„æ¨¡å¼ç‰¹å¾"""
        patterns = []
        
        if not values:
            return patterns
        
        # æ£€æŸ¥å¸¸è§æ¨¡å¼
        str_values = [str(v) for v in values if v is not None]
        
        if str_values:
            # é•¿åº¦ä¸€è‡´æ€§
            lengths = [len(s) for s in str_values]
            if len(set(lengths)) == 1:
                patterns.append(f"å›ºå®šé•¿åº¦({lengths[0]}å­—ç¬¦)")
            
            # æ ¼å¼æ¨¡å¼
            import re
            if all(re.match(r'^\d{4}-\d{2}-\d{2}$', s) for s in str_values[:5]):
                patterns.append("æ—¥æœŸæ ¼å¼(YYYY-MM-DD)")
            elif all(re.match(r'^\d+$', s) for s in str_values[:5]):
                patterns.append("çº¯æ•°å­—")
            elif all(re.match(r'^[A-Za-z]+\d+$', s) for s in str_values[:5]):
                patterns.append("å­—æ¯+æ•°å­—ç»„åˆ")
        
        return patterns
    
    def _extract_data_samples(self, ws, start_row: int, end_row: int, start_col: int, end_col: int, limit: int = 10) -> List[Dict[str, Any]]:
        """æå–æ•°æ®æ ·æœ¬"""
        samples = []
        
        # è·å–è¡¨å¤´
        headers = []
        for col in range(start_col, end_col + 1):
            header_cell = ws.cell(row=start_row - 1, column=col)
            header = str(header_cell.value).strip() if header_cell.value else f"åˆ—{col}"
            headers.append(header)
        
        # æå–æ•°æ®è¡Œ
        sample_count = 0
        for row in range(start_row, end_row + 1):
            if sample_count >= limit:
                break
            
            row_data = {}
            has_data = False
            
            for i, col in enumerate(range(start_col, end_col + 1)):
                cell = ws.cell(row=row, column=col)
                value = cell.value
                
                if value is not None:
                    has_data = True
                    # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                    if isinstance(value, str) and len(value) > 100:
                        value = value[:100] + "..."
                
                row_data[headers[i]] = value
            
            if has_data:
                row_data["_row_number"] = row
                samples.append(row_data)
                sample_count += 1
        
        return samples
    
    def _generate_data_summary(self, fields_info: List[Dict], data_samples: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ¦‚è¿°"""
        summary = {
            "total_fields": len(fields_info),
            "key_fields": [],
            "numeric_fields": [],
            "text_fields": [],
            "date_fields": [],
            "id_fields": [],
            "fields_with_issues": [],
            "sample_row_count": len(data_samples)
        }
        
        for field in fields_info:
            field_name = field["field_name"]
            data_type = field["data_type_analysis"]["primary_type"]
            characteristics = field["data_type_analysis"]["characteristics"]
            
            # åˆ†ç±»å­—æ®µ
            if field["is_key_field"]:
                summary["key_fields"].append(field_name)
            
            if data_type == "numeric":
                summary["numeric_fields"].append(field_name)
            elif data_type == "text":
                summary["text_fields"].append(field_name)
            elif data_type == "date":
                summary["date_fields"].append(field_name)
            elif data_type == "id":
                summary["id_fields"].append(field_name)
            
            # é—®é¢˜å­—æ®µ
            if "å­˜åœ¨å¤§é‡ç©ºå€¼" in characteristics:
                summary["fields_with_issues"].append({
                    "field": field_name,
                    "issue": "å­˜åœ¨å¤§é‡ç©ºå€¼",
                    "null_ratio": field["data_type_analysis"]["null_count"] / field["data_type_analysis"]["total_values"]
                })
        
        return summary
    
    def _analyze_cross_sheet_relationships(self, sheets_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè·¨å·¥ä½œè¡¨å…³è”å…³ç³»"""
        relationships = {
            "potential_links": [],
            "common_fields": [],
            "sheet_dependencies": [],
            "data_flow_analysis": {}
        }
        
        # æ”¶é›†æ‰€æœ‰å­—æ®µä¿¡æ¯
        all_fields = {}
        for sheet_name, analysis in sheets_analysis.items():
            field_analysis = analysis.get('field_analysis', {})
            if field_analysis.get('status') == 'success':
                fields = field_analysis.get('fields', [])
                all_fields[sheet_name] = [f["field_name"] for f in fields]
        
        # å¯»æ‰¾ç›¸ä¼¼å­—æ®µå
        for sheet1, fields1 in all_fields.items():
            for sheet2, fields2 in all_fields.items():
                if sheet1 >= sheet2:  # é¿å…é‡å¤æ¯”è¾ƒ
                    continue
                
                common = set(fields1) & set(fields2)
                if common:
                    relationships["common_fields"].append({
                        "sheet1": sheet1,
                        "sheet2": sheet2,
                        "common_fields": list(common),
                        "similarity_score": len(common) / max(len(fields1), len(fields2))
                    })
        
        # åˆ†ææ½œåœ¨çš„ä¸»ä»å…³ç³»
        relationships["data_flow_analysis"] = self._analyze_data_flow_patterns(sheets_analysis)
        
        return relationships
    
    def _analyze_data_flow_patterns(self, sheets_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®æµæ¨¡å¼"""
        patterns = {
            "source_sheets": [],  # æºæ•°æ®è¡¨
            "summary_sheets": [],  # æ±‡æ€»è¡¨
            "detail_sheets": [],  # æ˜ç»†è¡¨
            "template_sheets": []  # æ¨¡æ¿è¡¨
        }
        
        for sheet_name, analysis in sheets_analysis.items():
            basic_info = analysis.get('basic_info', {})
            field_analysis = analysis.get('field_analysis', {})
            
            row_count = basic_info.get('max_row', 0)
            col_count = basic_info.get('max_column', 0)
            
            # åŸºäºæ•°æ®é‡å’Œå­—æ®µç‰¹å¾åˆ¤æ–­è¡¨ç±»å‹
            if row_count > 1000:
                patterns["source_sheets"].append({
                    "sheet": sheet_name,
                    "reason": f"å¤§é‡æ•°æ®è¡Œ({row_count}è¡Œ)",
                    "rows": row_count,
                    "cols": col_count
                })
            elif "æ±‡æ€»" in sheet_name or "æ€»è®¡" in sheet_name or "ç»Ÿè®¡" in sheet_name:
                patterns["summary_sheets"].append({
                    "sheet": sheet_name,
                    "reason": "è¡¨ååŒ…å«æ±‡æ€»å…³é”®è¯",
                    "rows": row_count,
                    "cols": col_count
                })
            elif "æ¨¡æ¿" in sheet_name or "template" in sheet_name.lower():
                patterns["template_sheets"].append({
                    "sheet": sheet_name,
                    "reason": "è¡¨ååŒ…å«æ¨¡æ¿å…³é”®è¯",
                    "rows": row_count,
                    "cols": col_count
                })
            elif row_count < 100 and col_count > 10:
                patterns["detail_sheets"].append({
                    "sheet": sheet_name,
                    "reason": "å®½è¡¨æ ¼å¼ï¼Œå¯èƒ½æ˜¯æ˜ç»†æˆ–é…ç½®è¡¨",
                    "rows": row_count,
                    "cols": col_count
                })
        
        return patterns
    
    def _generate_ai_prompt_data(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆä¾›AIåˆ†æçš„å®Œæ•´æç¤ºè¯æ•°æ®"""
        ai_data = {
            "file_overview": {
                "file_path": analysis['file_path'],
                "total_sheets": analysis['total_sheets'],
                "sheet_names": analysis['sheet_names']
            },
            "detailed_sheet_analysis": {},
            "cross_sheet_relationships": analysis['cross_sheet_analysis'],
            "analysis_suggestions": [],
            "potential_analysis_tasks": [],
            "generated_prompt": ""
        }
        
        # è¯¦ç»†çš„å·¥ä½œè¡¨åˆ†æ
        for sheet_name, sheet_analysis in analysis['sheets_analysis'].items():
            field_analysis = sheet_analysis.get('field_analysis', {})
            
            if field_analysis.get('status') == 'success':
                sheet_data = {
                    "sheet_name": sheet_name,
                    "dimensions": f"{sheet_analysis['basic_info']['max_row']}è¡Œ Ã— {sheet_analysis['basic_info']['max_column']}åˆ—",
                    "field_count": field_analysis['fields_count'],
                    "data_rows": field_analysis['data_rows_count'],
                    "fields_detail": [],
                    "data_samples": field_analysis['data_samples'][:3],  # å‰3è¡Œæ ·æœ¬
                    "data_summary": field_analysis['data_summary'],
                    "special_notes": []
                }
                
                # å­—æ®µè¯¦æƒ…
                for field in field_analysis['fields']:
                    field_detail = {
                        "name": field['field_name'],
                        "type": field['data_type_analysis']['primary_type'],
                        "confidence": field['data_type_analysis']['confidence'],
                        "characteristics": field['data_type_analysis']['characteristics'],
                        "sample_values": field['sample_values'],
                        "is_key": field['is_key_field']
                    }
                    sheet_data["fields_detail"].append(field_detail)
                
                # ç‰¹æ®Šæ³¨æ„äº‹é¡¹
                merged_info = sheet_analysis.get('merged_cells', {})
                if merged_info.get('count', 0) > 0:
                    sheet_data["special_notes"].append(f"å­˜åœ¨{merged_info['count']}ä¸ªåˆå¹¶å•å…ƒæ ¼")
                
                if merged_info.get('complex_structure', False):
                    sheet_data["special_notes"].append("å¤æ‚çš„åˆå¹¶å•å…ƒæ ¼ç»“æ„ï¼Œå¯èƒ½å½±å“æ•°æ®è¯»å–")
                
                ai_data["detailed_sheet_analysis"][sheet_name] = sheet_data
        
        # ç”Ÿæˆåˆ†æå»ºè®®
        ai_data["analysis_suggestions"] = self._generate_analysis_suggestions(analysis)
        
        # ç”Ÿæˆæ½œåœ¨åˆ†æä»»åŠ¡
        ai_data["potential_analysis_tasks"] = self._generate_potential_tasks(analysis)
        
        # ç”Ÿæˆç»“æ„åŒ–æç¤ºè¯
        ai_data["generated_prompt"] = self._generate_structured_prompt(ai_data)
        
        return ai_data
    
    def _generate_analysis_suggestions(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        suggestions = []
        
        # åŸºäºè·¨è¡¨å…³ç³»ç”Ÿæˆå»ºè®®
        cross_analysis = analysis.get('cross_sheet_analysis', {})
        common_fields = cross_analysis.get('common_fields', [])
        
        if common_fields:
            suggestions.append("æ£€æµ‹åˆ°å¤šä¸ªå·¥ä½œè¡¨å­˜åœ¨ç›¸åŒå­—æ®µï¼Œå»ºè®®åˆ†æè¡¨é—´å…³è”å…³ç³»")
        
        # åŸºäºæ•°æ®æµåˆ†æç”Ÿæˆå»ºè®®
        data_flow = cross_analysis.get('data_flow_analysis', {})
        if data_flow.get('source_sheets'):
            suggestions.append("æ£€æµ‹åˆ°æºæ•°æ®è¡¨ï¼Œå»ºè®®ä½œä¸ºä¸»è¦åˆ†æå¯¹è±¡")
        
        if data_flow.get('summary_sheets'):
            suggestions.append("æ£€æµ‹åˆ°æ±‡æ€»è¡¨ï¼Œå»ºè®®ç”¨äºéªŒè¯åˆ†æç»“æœ")
        
        return suggestions
    
    def _generate_potential_tasks(self, analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆæ½œåœ¨çš„åˆ†æä»»åŠ¡"""
        tasks = []
        
        for sheet_name, sheet_analysis in analysis['sheets_analysis'].items():
            field_analysis = sheet_analysis.get('field_analysis', {})
            
            if field_analysis.get('status') == 'success':
                summary = field_analysis.get('data_summary', {})
                
                # æ•°æ®æ¸…æ´—ä»»åŠ¡
                if summary.get('fields_with_issues'):
                    tasks.append({
                        "type": "æ•°æ®æ¸…æ´—",
                        "description": f"å¤„ç†{sheet_name}ä¸­çš„æ•°æ®è´¨é‡é—®é¢˜",
                        "sheet": sheet_name
                    })
                
                # ç»Ÿè®¡åˆ†æä»»åŠ¡
                if summary.get('numeric_fields'):
                    tasks.append({
                        "type": "ç»Ÿè®¡åˆ†æ",
                        "description": f"å¯¹{sheet_name}ä¸­çš„æ•°å€¼å­—æ®µè¿›è¡Œç»Ÿè®¡åˆ†æ",
                        "sheet": sheet_name
                    })
                
                # å…³è”åˆ†æä»»åŠ¡
                if summary.get('key_fields'):
                    tasks.append({
                        "type": "å…³è”åˆ†æ",
                        "description": f"åŸºäº{sheet_name}çš„å…³é”®å­—æ®µè¿›è¡Œè·¨è¡¨å…³è”",
                        "sheet": sheet_name
                    })
        
        return tasks
    
    def _generate_structured_prompt(self, ai_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»“æ„åŒ–çš„AIåˆ†ææç¤ºè¯"""
        prompt_parts = []
        
        # æ–‡ä»¶æ¦‚è¿°
        overview = ai_data["file_overview"]
        prompt_parts.append(f"# Excelæ–‡ä»¶åˆ†æ - {overview['file_path']}")
        prompt_parts.append(f"æ–‡ä»¶åŒ…å«{overview['total_sheets']}ä¸ªå·¥ä½œè¡¨ï¼š{', '.join(overview['sheet_names'])}")
        prompt_parts.append("")
        
        # è¯¦ç»†å·¥ä½œè¡¨ä¿¡æ¯
        prompt_parts.append("## å·¥ä½œè¡¨è¯¦ç»†ä¿¡æ¯")
        
        for sheet_name, sheet_data in ai_data["detailed_sheet_analysis"].items():
            prompt_parts.append(f"### ã€{sheet_name}ã€‘")
            prompt_parts.append(f"- **è§„æ¨¡**: {sheet_data['dimensions']}")
            prompt_parts.append(f"- **å­—æ®µæ•°é‡**: {sheet_data['field_count']}")
            prompt_parts.append(f"- **æ•°æ®è¡Œæ•°**: {sheet_data['data_rows']}")
            
            # å­—æ®µä¿¡æ¯
            prompt_parts.append("- **å­—æ®µè¯¦æƒ…**:")
            for field in sheet_data["fields_detail"]:
                characteristics = ", ".join(field["characteristics"]) if field["characteristics"] else "æ— ç‰¹æ®Šç‰¹å¾"
                prompt_parts.append(f"  - `{field['name']}`: {field['type']} (ç½®ä¿¡åº¦:{field['confidence']:.1%}) - {characteristics}")
                if field["sample_values"]:
                    sample_str = ", ".join([f'"{v}"' for v in field["sample_values"][:3]])
                    prompt_parts.append(f"    æ ·æœ¬å€¼: {sample_str}")
            
            # æ•°æ®æ ·æœ¬
            if sheet_data["data_samples"]:
                prompt_parts.append("- **æ•°æ®æ ·æœ¬**:")
                for i, sample in enumerate(sheet_data["data_samples"][:2], 1):
                    sample_str = ", ".join([f"{k}={v}" for k, v in list(sample.items())[:5] if k != "_row_number"])
                    prompt_parts.append(f"  æ ·æœ¬{i}: {sample_str}")
            
            # ç‰¹æ®Šæ³¨æ„äº‹é¡¹
            if sheet_data["special_notes"]:
                prompt_parts.append("- **æ³¨æ„äº‹é¡¹**: " + "; ".join(sheet_data["special_notes"]))
            
            prompt_parts.append("")
        
        # è·¨è¡¨å…³ç³»
        relationships = ai_data["cross_sheet_relationships"]
        if relationships.get("common_fields"):
            prompt_parts.append("## å·¥ä½œè¡¨å…³è”å…³ç³»")
            for rel in relationships["common_fields"]:
                prompt_parts.append(f"- {rel['sheet1']} â†” {rel['sheet2']}: å…±åŒå­—æ®µ {rel['common_fields']}")
            prompt_parts.append("")
        
        # åˆ†æå»ºè®®
        if ai_data["analysis_suggestions"]:
            prompt_parts.append("## åˆ†æå»ºè®®")
            for suggestion in ai_data["analysis_suggestions"]:
                prompt_parts.append(f"- {suggestion}")
            prompt_parts.append("")
        
        # æ½œåœ¨ä»»åŠ¡
        if ai_data["potential_analysis_tasks"]:
            prompt_parts.append("## å»ºè®®çš„åˆ†æä»»åŠ¡")
            for task in ai_data["potential_analysis_tasks"]:
                prompt_parts.append(f"- **{task['type']}**: {task['description']}")
            prompt_parts.append("")
        
        return "\n".join(prompt_parts)

class AdvancedExcelProcessor:
    """å¢å¼ºç‰ˆExcelå¤„ç†ç±» - æ•´åˆæ™ºèƒ½åˆ†æåŠŸèƒ½"""
    
    def __init__(self):
        self.workbook = None
        self.file_path = None
        self.modified_data = {}
        self.analyzer = SmartExcelAnalyzer()
        self.structure_analysis = None
    
    def load_excel(self, file_path: str) -> Dict[str, pd.DataFrame]:
        """åŠ è½½Excelæ–‡ä»¶ï¼Œä¿è¯æ•°æ®å®Œæ•´æ€§ï¼Œä¸è·³è¿‡ä»»ä½•è¡Œ"""
        self.file_path = file_path
        
        # é¦–å…ˆè¿›è¡Œç»“æ„åˆ†æï¼ˆä½†ä¸å½±å“æ•°æ®è¯»å–ï¼‰
        print("ğŸ” æ­£åœ¨åˆ†æExcelæ–‡ä»¶ç»“æ„...")
        try:
            self.structure_analysis = self.analyzer.analyze_excel_structure(file_path)
        except Exception as e:
            print(f"âš ï¸ ç»“æ„åˆ†æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€è¯»å–: {str(e)}")
            self.structure_analysis = None
        
        # ä½¿ç”¨å®Œæ•´æ•°æ®è¯»å–æ–¹æ³•ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•æ•°æ®
        excel_data = {}
        
        # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
        wb = openpyxl.load_workbook(file_path, data_only=True)
        sheet_names = wb.sheetnames
        
        for sheet_name in sheet_names:
            print(f"ğŸ“‹ æ­£åœ¨å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
            
            try:
                # ä½¿ç”¨å®Œæ•´è¯»å–æ–¹æ³•ï¼Œç¡®ä¿ä»ç¬¬ä¸€è¡Œå¼€å§‹è¯»å–æ‰€æœ‰æ•°æ®
                df, markdown = self.read_excel_with_merged_cells_complete(file_path, sheet_name)
                excel_data[sheet_name] = df
                self.modified_data[sheet_name] = df.copy()
                
                # æ‰“å°è¯»å–æ‘˜è¦
                print(f"  âœ… æˆåŠŸè¯»å–: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ— (å®Œæ•´æ•°æ®)")
                
            except Exception as e:
                print(f"  âŒ è¯»å–å‡ºé”™: {str(e)}")
                # æœ€åçš„å›é€€æ–¹æ³•
                try:
                    df, markdown = self.read_excel_with_merged_cells(file_path, sheet_name)
                    excel_data[sheet_name] = df
                    self.modified_data[sheet_name] = df.copy()
                    print(f"  âœ… å›é€€æ–¹æ³•æˆåŠŸ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
                except Exception as e2:
                    print(f"  âŒ å›é€€æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                    # åˆ›å»ºç©ºDataFrame
                    excel_data[sheet_name] = pd.DataFrame()
                    self.modified_data[sheet_name] = pd.DataFrame()
        
        return excel_data
    
    def _smart_read_sheet(self, file_path: str, sheet_name: str, suggestions: Dict[str, Any]) -> Tuple[pd.DataFrame, str]:
        """åŸºäºæ™ºèƒ½åˆ†æå»ºè®®è¯»å–å·¥ä½œè¡¨"""
        parameters = suggestions.get('parameters', {})
        
        # æ„å»ºpandasè¯»å–å‚æ•°
        pd_params = {
            'io': file_path,
            'sheet_name': sheet_name,
            'engine': 'openpyxl'
        }
        
        # åº”ç”¨æ™ºèƒ½å»ºè®®çš„å‚æ•°
        if parameters.get('header') is not None:
            pd_params['header'] = parameters['header']
        
        if parameters.get('skiprows') is not None and parameters['skiprows'] > 0:
            pd_params['skiprows'] = parameters['skiprows']
        
        if parameters.get('nrows') is not None:
            pd_params['nrows'] = parameters['nrows']
        
        if parameters.get('usecols') is not None:
            pd_params['usecols'] = parameters['usecols']
        
        # è¯»å–æ•°æ®
        df = pd.read_excel(**pd_params)
        
        # å¦‚æœéœ€è¦ï¼Œåº”ç”¨é¢„å¤„ç†æ­¥éª¤
        preprocessing_steps = suggestions.get('preprocessing_steps', [])
        for step in preprocessing_steps:
            if "è½¬ç½®" in step:
                df = df.transpose()
            elif "ç¼ºå¤±å€¼" in step:
                df = self._handle_missing_values(df)
        
        # ç”Ÿæˆmarkdowné¢„è§ˆ
        markdown = self.df_to_markdown(df, sheet_name)
        
        return df, markdown
    
    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½å¤„ç†ç¼ºå¤±å€¼"""
        # ç®€å•çš„ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
        # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
        # æ–‡æœ¬åˆ—ï¼šç”¨"æœªçŸ¥"å¡«å……
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                df[col].fillna(df[col].median(), inplace=True)
            else:
                df[col].fillna('æœªçŸ¥', inplace=True)
        
        return df
    
    def get_structure_analysis(self) -> Dict[str, Any]:
        """è·å–ç»“æ„åˆ†æç»“æœ"""
        return self.structure_analysis
    
    def print_analysis_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.structure_analysis:
            print("âŒ æ²¡æœ‰ç»“æ„åˆ†æç»“æœ")
            return
        
        analysis = self.structure_analysis
        print(f"\nğŸ“Š Excelæ–‡ä»¶ç»“æ„åˆ†ææ‘˜è¦")
        print(f"ğŸ“„ æ–‡ä»¶: {analysis['file_path']}")
        print(f"ğŸ“‹ å·¥ä½œè¡¨æ•°é‡: {analysis['total_sheets']}")
        print(f"ğŸ“‹ å·¥ä½œè¡¨åç§°: {', '.join(analysis['sheet_names'])}")
        
        for sheet_name, sheet_analysis in analysis['sheets_analysis'].items():
            print(f"\n  ğŸ“„ ã€{sheet_name}ã€‘")
            
            basic = sheet_analysis['basic_info']
            print(f"    ğŸ“ å¤§å°: {basic['max_row']}è¡Œ Ã— {basic['max_column']}åˆ—")
            
            merged = sheet_analysis['merged_cells']
            if merged['count'] > 0:
                print(f"    ğŸ”— åˆå¹¶å•å…ƒæ ¼: {merged['count']}ä¸ª (è¡¨å¤´:{merged['header_merges']}, æ•°æ®:{merged['data_merges']})")
            
            regions = sheet_analysis['data_regions']
            if regions:
                region = regions[0]
                print(f"    ğŸ“Š æ•°æ®å¯†åº¦: {region['density']:.1%}")
                print(f"    ğŸ“ æ•°æ®èŒƒå›´: ç¬¬{region['min_row']}-{region['max_row']}è¡Œ, ç¬¬{region['min_col']}-{region['max_col']}åˆ—")
            
            tables = sheet_analysis['table_structures']
            if tables:
                table = tables[0]
                header_info = table['header_analysis']
                print(f"    ğŸ“‹ å»ºè®®è¡¨å¤´è¡Œ: ç¬¬{header_info['suggested_header_row']}è¡Œ (ç½®ä¿¡åº¦:{header_info['confidence']:.1%})")
                print(f"    ğŸ“Š è¡¨æ ¼ç±»å‹: {table['table_type']}")
                
                quality = table.get('data_quality', {})
                if quality:
                    print(f"    âœ… æ•°æ®å®Œæ•´æ€§: {quality['completeness']:.1%}")
            
            suggestions = sheet_analysis['read_suggestions']
            if suggestions.get('warnings'):
                print(f"    âš ï¸  æ³¨æ„äº‹é¡¹:")
                for warning in suggestions['warnings']:
                    print(f"        â€¢ {warning}")
    
    @staticmethod
    def read_excel_with_merged_cells_complete(file_path: str, sheet_name: str = None, max_rows: int = 1000) -> Tuple[pd.DataFrame, str]:
        """è¯»å–Excelæ–‡ä»¶å¹¶å¤„ç†åˆå¹¶å•å…ƒæ ¼ï¼Œç¡®ä¿ä»ç¬¬ä¸€è¡Œå¼€å§‹è¯»å–æ‰€æœ‰æ•°æ®ï¼Œä¸è·³è¿‡ä»»ä½•å†…å®¹"""
        try:
            wb = openpyxl.load_workbook(file_path, data_only=True)
            
            if sheet_name is None:
                sheet_name = wb.sheetnames[0]
            
            ws = wb[sheet_name]
            
            # è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
            merged_cells = ws.merged_cells.ranges
            merged_dict = {}
            
            # åˆ›å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„
            for merged_range in merged_cells:
                top_left_cell = ws.cell(row=merged_range.min_row, column=merged_range.min_col)
                top_left_value = top_left_cell.value
                
                for row in range(merged_range.min_row, merged_range.max_row + 1):
                    for col in range(merged_range.min_col, merged_range.max_col + 1):
                        merged_dict[(row, col)] = top_left_value
            
            # è¯»å–æ•°æ® - ä»ç¬¬ä¸€è¡Œå¼€å§‹ï¼Œä¸è·³è¿‡ä»»ä½•è¡Œ
            data = []
            max_col = min(ws.max_column, 100)  # é™åˆ¶æœ€å¤§åˆ—æ•°
            max_row = min(ws.max_row, max_rows)
            
            for row in range(1, max_row + 1):
                row_data = []
                for col in range(1, max_col + 1):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶å•å…ƒæ ¼
                    if (row, col) in merged_dict:
                        value = merged_dict[(row, col)]
                    else:
                        cell = ws.cell(row=row, column=col)
                        value = cell.value
                    
                    row_data.append(value if value is not None else "")
                
                data.append(row_data)
            
            # æ™ºèƒ½å¤„ç†åˆ—åï¼šå°è¯•ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºå­—æ®µå
            if data:
                if len(data) > 1:
                    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦çœ‹èµ·æ¥åƒå­—æ®µå
                    first_row = data[0]
                    looks_like_headers = True
                    
                    # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦ä¸»è¦åŒ…å«æ–‡æœ¬ä¸”ä¸å…¨ä¸ºç©º
                    non_empty_count = 0
                    text_count = 0
                    
                    for cell_value in first_row:
                        if cell_value is not None and str(cell_value).strip():
                            non_empty_count += 1
                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬ï¼ˆéçº¯æ•°å­—ï¼‰
                            try:
                                float(str(cell_value))
                            except (ValueError, TypeError):
                                text_count += 1
                    
                    # å¦‚æœç¬¬ä¸€è¡Œæœ‰è¶³å¤Ÿçš„æ–‡æœ¬å†…å®¹ï¼Œè®¤ä¸ºæ˜¯å­—æ®µå
                    if non_empty_count > 0 and text_count / non_empty_count > 0.5:
                        # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
                        columns = []
                        column_counts = {}
                        
                        for i, col_value in enumerate(first_row):
                            if col_value is None or str(col_value).strip() == "":
                                base_name = f"åˆ—{i+1}"
                            else:
                                base_name = str(col_value).strip()
                            
                            # å¤„ç†é‡å¤åˆ—å
                            if base_name in column_counts:
                                column_counts[base_name] += 1
                                unique_name = f"{base_name}_{column_counts[base_name]}"
                            else:
                                column_counts[base_name] = 0
                                unique_name = base_name
                            
                            columns.append(unique_name)
                        
                        # ä»ç¬¬äºŒè¡Œå¼€å§‹ä½œä¸ºæ•°æ®
                        data_rows = data[1:] if len(data) > 1 else []
                        df = pd.DataFrame(data_rows, columns=columns)
                    else:
                        # ç¬¬ä¸€è¡Œä¸åƒå­—æ®µåï¼Œç”Ÿæˆé»˜è®¤åˆ—åå¹¶ä¿ç•™æ‰€æœ‰æ•°æ®
                        columns = [f"åˆ—{i+1}" for i in range(len(data[0]))]
                        df = pd.DataFrame(data, columns=columns)
                else:
                    # åªæœ‰ä¸€è¡Œæ•°æ®ï¼Œç”Ÿæˆé»˜è®¤åˆ—å
                    columns = [f"åˆ—{i+1}" for i in range(len(data[0]))]
                    df = pd.DataFrame(data, columns=columns)
            else:
                df = pd.DataFrame()
            
            # æ¸…ç†æ•°æ® - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                if not df.empty:
                    # å¯¹æ¯åˆ—è¿›è¡Œæ•°æ®ç±»å‹è§„èŒƒåŒ–ï¼Œé¿å…æ··åˆç±»å‹
                    for col in df.columns:
                        try:
                            # æ£€æŸ¥åˆ—æ˜¯å¦åŒ…å«æ··åˆç±»å‹
                            if df[col].dtype == 'object':
                                # å°è¯•å°†å…¨éƒ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…æ··åˆç±»å‹
                                df[col] = df[col].astype(str)
                                # å°†å­—ç¬¦ä¸²'nan'å’Œç©ºå­—ç¬¦ä¸²ä¿æŒåŸæ ·
                                df[col] = df[col].replace('nan', '')
                        except Exception as e:
                            print(f"åˆ— {col} æ•°æ®ç±»å‹è½¬æ¢è­¦å‘Š: {e}")
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            df[col] = df[col].astype(str).replace('nan', '')
                    
                    # åº”ç”¨infer_objects
                    df = df.infer_objects(copy=False)
            except Exception as e:
                print(f"æ•°æ®æ¸…ç†æ—¶å‡ºç°é—®é¢˜: {e}")
            
            # ç”Ÿæˆmarkdownæ ¼å¼é¢„è§ˆ
            markdown_content = AdvancedExcelProcessor.df_to_markdown(df, sheet_name)
            
            return df, markdown_content
            
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    @staticmethod
    def read_excel_with_merged_cells(file_path: str, sheet_name: str = None, max_rows: int = 1000) -> Tuple[pd.DataFrame, str]:
        """è¯»å–Excelæ–‡ä»¶å¹¶å¤„ç†åˆå¹¶å•å…ƒæ ¼ï¼Œæ”¯æŒæ›´å¤šè¡Œæ•°"""
        try:
            wb = openpyxl.load_workbook(file_path, data_only=True)
            
            if sheet_name is None:
                sheet_name = wb.sheetnames[0]
            
            ws = wb[sheet_name]
            
            # è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
            merged_cells = ws.merged_cells.ranges
            merged_dict = {}
            
            # åˆ›å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„
            for merged_range in merged_cells:
                top_left_cell = ws.cell(row=merged_range.min_row, column=merged_range.min_col)
                top_left_value = top_left_cell.value
                
                for row in range(merged_range.min_row, merged_range.max_row + 1):
                    for col in range(merged_range.min_col, merged_range.max_col + 1):
                        merged_dict[(row, col)] = top_left_value
            
            # è¯»å–æ•°æ®
            data = []
            max_col = min(ws.max_column, 100)  # é™åˆ¶æœ€å¤§åˆ—æ•°
            max_row = min(ws.max_row, max_rows)
            
            for row in range(1, max_row + 1):
                row_data = []
                for col in range(1, max_col + 1):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆå¹¶å•å…ƒæ ¼
                    if (row, col) in merged_dict:
                        value = merged_dict[(row, col)]
                    else:
                        cell = ws.cell(row=row, column=col)
                        value = cell.value
                    
                    row_data.append(value if value is not None else "")
                
                data.append(row_data)
            
            # è½¬æ¢ä¸ºDataFrame
            if data and len(data) > 1:
                # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—åï¼Œç¡®ä¿åˆ—åæ˜¯å­—ç¬¦ä¸²
                columns = []
                column_counts = {}  # ç”¨äºè·Ÿè¸ªé‡å¤åˆ—å
                
                for i, col in enumerate(data[0]):
                    if col is None or str(col).strip() == "":
                        base_name = f"åˆ—{i+1}"
                    else:
                        base_name = str(col).strip()
                    
                    # å¤„ç†é‡å¤åˆ—å
                    if base_name in column_counts:
                        column_counts[base_name] += 1
                        unique_name = f"{base_name}_{column_counts[base_name]}"
                    else:
                        column_counts[base_name] = 0
                        unique_name = base_name
                    
                    columns.append(unique_name)
                
                # åˆ›å»ºDataFrameï¼Œç¡®ä¿æ•°æ®è¡Œå­˜åœ¨
                data_rows = data[1:] if len(data) > 1 else []
                if data_rows:
                    df = pd.DataFrame(data_rows, columns=columns)
                else:
                    df = pd.DataFrame(columns=columns)
            else:
                df = pd.DataFrame()
            
            # æ¸…ç†æ•°æ® - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                if not df.empty:
                    # ä¿®å¤pandas FutureWarningå¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                    df = df.replace('', np.nan)
                    
                    # å¯¹æ¯åˆ—è¿›è¡Œæ•°æ®ç±»å‹è§„èŒƒåŒ–ï¼Œé¿å…æ··åˆç±»å‹
                    for col in df.columns:
                        try:
                            # æ£€æŸ¥åˆ—æ˜¯å¦åŒ…å«æ··åˆç±»å‹
                            if df[col].dtype == 'object':
                                # å°è¯•å°†å…¨éƒ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…æ··åˆç±»å‹
                                df[col] = df[col].astype(str)
                                # å°†å­—ç¬¦ä¸²'nan'é‡æ–°æ›¿æ¢ä¸ºçœŸæ­£çš„NaN
                                df[col] = df[col].replace('nan', np.nan)
                        except Exception as e:
                            print(f"åˆ— {col} æ•°æ®ç±»å‹è½¬æ¢è­¦å‘Š: {e}")
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            df[col] = df[col].astype(str).replace('nan', np.nan)
                    
                    # æœ€ååº”ç”¨infer_objects
                    df = df.infer_objects(copy=False)
                    
                    # æ¸…ç†åˆ—åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                    df.columns = [str(col).replace('\n', ' ').replace('\r', ' ').strip() for col in df.columns]
            except Exception as e:
                print(f"æ•°æ®æ¸…ç†æ—¶å‡ºç°é—®é¢˜: {e}")
                # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè‡³å°‘ç¡®ä¿åŸºæœ¬çš„æ•°æ®ç»“æ„
                if not df.empty:
                    try:
                        df.columns = [str(col).replace('\n', ' ').replace('\r', ' ').strip() for col in df.columns]
                    except:
                        pass
            
            # ç”Ÿæˆmarkdownæ ¼å¼é¢„è§ˆ
            markdown_content = AdvancedExcelProcessor.df_to_markdown(df, sheet_name)
            
            return df, markdown_content
            
        except Exception as e:
            raise Exception(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
    
    @staticmethod
    def df_to_markdown(df: pd.DataFrame, sheet_name: str = "") -> str:
        """å°†DataFrameè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œæ”¯æŒæ›´å¥½çš„æ ¼å¼åŒ–"""
        if df.empty:
            return f"## ğŸ“‹ {sheet_name}\n\n*æ­¤å·¥ä½œè¡¨ä¸ºç©º*\n\n"
        
        markdown = f"## ğŸ“‹ {sheet_name}\n\n"
        
        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        markdown += f"**æ•°æ®æ¦‚è§ˆ**: {len(df)} è¡Œ Ã— {len(df.columns)} åˆ—\n\n"
        
        # é™åˆ¶æ˜¾ç¤ºçš„è¡Œæ•°å’Œåˆ—æ•°
        display_rows = min(20, len(df))
        display_cols = min(8, len(df.columns))
        display_df = df.head(display_rows).iloc[:, :display_cols]
        
        # å¤„ç†åˆ—åï¼Œç¡®ä¿ä¸ä¼šå¤ªé•¿
        headers = ["#"] + [str(col)[:15] + "..." if len(str(col)) > 15 else str(col) for col in display_df.columns]
        markdown += "| " + " | ".join(headers) + " |\n"
        markdown += "| " + " | ".join(["---"] * len(headers)) + " |\n"
        
        # æ·»åŠ æ•°æ®è¡Œ
        for idx, row in display_df.iterrows():
            row_data = [str(idx + 1)]  # è¡Œå·ä»1å¼€å§‹
            for val in row:
                str_val = str(val) if pd.notna(val) else ""
                # é™åˆ¶å•å…ƒæ ¼å†…å®¹é•¿åº¦
                if len(str_val) > 20:
                    str_val = str_val[:17] + "..."
                row_data.append(str_val)
            markdown += "| " + " | ".join(row_data) + " |\n"
        
        # æ·»åŠ çœç•¥ä¿¡æ¯
        if len(df) > display_rows:
            markdown += f"\n*ğŸ“ ä»…æ˜¾ç¤ºå‰{display_rows}è¡Œï¼Œæ€»å…±{len(df)}è¡Œ*\n"
        
        if len(df.columns) > display_cols:
            markdown += f"*ğŸ“Š ä»…æ˜¾ç¤ºå‰{display_cols}åˆ—ï¼Œæ€»å…±{len(df.columns)}åˆ—*\n"
        
        # æ·»åŠ æ•°æ®ç±»å‹ä¿¡æ¯ - ä¿®å¤é”™è¯¯å¤„ç†
        markdown += "\n### ğŸ“ˆ æ•°æ®ç±»å‹æ¦‚è§ˆ\n"
        try:
            cols_to_show = list(df.columns)[:5]  # åªæ˜¾ç¤ºå‰5åˆ—çš„ç±»å‹
            for col in cols_to_show:
                try:
                    if col in df.columns:
                        dtype = str(df[col].dtype)
                        non_null_count = df[col].count()
                        markdown += f"- **{col}**: {dtype} ({non_null_count}/{len(df)} éç©º)\n"
                except Exception as e:
                    # å¦‚æœè·å–æŸåˆ—çš„æ•°æ®ç±»å‹å¤±è´¥ï¼Œè·³è¿‡è¯¥åˆ—
                    markdown += f"- **{col}**: æœªçŸ¥ç±»å‹\n"
            
            if len(df.columns) > 5:
                markdown += f"- *... è¿˜æœ‰ {len(df.columns) - 5} åˆ—*\n"
        except Exception as e:
            markdown += "- *æ•°æ®ç±»å‹ä¿¡æ¯è·å–å¤±è´¥*\n"
        
        return markdown
    
    @staticmethod
    def df_to_pure_markdown(df: pd.DataFrame) -> str:
        """å°†DataFrameè½¬æ¢ä¸ºçº¯å‡€çš„Markdownè¡¨æ ¼æ ¼å¼ï¼Œä¸åŒ…å«ä»»ä½•åˆ†æä¿¡æ¯"""
        if df.empty:
            return "*æ­¤å·¥ä½œè¡¨ä¸ºç©º*\n"
        
        # æ„å»ºmarkdownè¡¨æ ¼
        markdown_lines = []
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
        # å¦‚æœåˆ—åæ˜¯"åˆ—1","åˆ—2"è¿™æ ·çš„é€šç”¨åç§°ï¼Œè¯´æ˜ç¬¬ä¸€è¡Œæ˜¯çœŸæ­£çš„è¡¨å¤´æ•°æ®
        use_first_row_as_header = all(col.startswith("åˆ—") and col[1:].isdigit() for col in df.columns)
        
        if use_first_row_as_header and len(df) > 0:
            # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
            first_row = df.iloc[0]
            headers = []
            for val in first_row:
                if pd.isna(val) or str(val).strip() == "":
                    headers.append("")
                else:
                    header_str = str(val).replace("|", "\\|").replace("\n", " ").replace("\r", "").strip()
                    headers.append(header_str)
            
            markdown_lines.append("| " + " | ".join(headers) + " |")
            markdown_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
            
            # ä»ç¬¬äºŒè¡Œå¼€å§‹ä½œä¸ºæ•°æ®è¡Œ
            data_rows = df.iloc[1:]
        else:
            # ä½¿ç”¨DataFrameçš„åˆ—åä½œä¸ºè¡¨å¤´
            headers = [str(col) for col in df.columns]
            markdown_lines.append("| " + " | ".join(headers) + " |")
            markdown_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
            
            # æ‰€æœ‰è¡Œéƒ½ä½œä¸ºæ•°æ®è¡Œ
            data_rows = df
        
        # æ•°æ®è¡Œ
        for idx, row in data_rows.iterrows():
            row_data = []
            for val in row:
                # å¤„ç†ç©ºå€¼å’Œæ ¼å¼åŒ–
                if pd.isna(val):
                    str_val = ""
                elif isinstance(val, float) and val.is_integer():
                    str_val = str(int(val))
                else:
                    str_val = str(val)
                
                # å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿markdownè¡¨æ ¼æ ¼å¼æ­£ç¡®
                str_val = str_val.replace("|", "\\|").replace("\n", " ").replace("\r", "")
                row_data.append(str_val)
            
            markdown_lines.append("| " + " | ".join(row_data) + " |")
        
        return "\n".join(markdown_lines)
    
    def update_dataframe(self, sheet_name: str, df: pd.DataFrame):
        """æ›´æ–°æŒ‡å®šå·¥ä½œè¡¨çš„æ•°æ®"""
        self.modified_data[sheet_name] = df
    
    def add_calculated_column(self, sheet_name: str, column_name: str, formula_func, description: str = ""):
        """æ·»åŠ è®¡ç®—åˆ—"""
        if sheet_name in self.modified_data:
            df = self.modified_data[sheet_name]
            try:
                df[column_name] = formula_func(df)
                self.modified_data[sheet_name] = df
                return True, f"æˆåŠŸæ·»åŠ è®¡ç®—åˆ—: {column_name}"
            except Exception as e:
                return False, f"æ·»åŠ è®¡ç®—åˆ—å¤±è´¥: {str(e)}"
        return False, "å·¥ä½œè¡¨ä¸å­˜åœ¨"
    
    def fill_missing_values(self, sheet_name: str, column: str, method: str = "mean", custom_value=None):
        """å¡«å……ç¼ºå¤±å€¼"""
        if sheet_name not in self.modified_data:
            return False, "å·¥ä½œè¡¨ä¸å­˜åœ¨"
        
        df = self.modified_data[sheet_name]
        if column not in df.columns:
            return False, f"åˆ— '{column}' ä¸å­˜åœ¨"
        
        try:
            if method == "mean" and pd.api.types.is_numeric_dtype(df[column]):
                df[column].fillna(df[column].mean(), inplace=True)
            elif method == "median" and pd.api.types.is_numeric_dtype(df[column]):
                df[column].fillna(df[column].median(), inplace=True)
            elif method == "mode":
                mode_value = df[column].mode().iloc[0] if not df[column].mode().empty else ""
                df[column].fillna(mode_value, inplace=True)
            elif method == "forward":
                df[column].fillna(method='ffill', inplace=True)
            elif method == "backward":
                df[column].fillna(method='bfill', inplace=True)
            elif method == "custom" and custom_value is not None:
                df[column].fillna(custom_value, inplace=True)
            else:
                return False, f"ä¸æ”¯æŒçš„å¡«å……æ–¹æ³•: {method}"
            
            self.modified_data[sheet_name] = df
            return True, f"æˆåŠŸå¡«å……åˆ— '{column}' çš„ç¼ºå¤±å€¼"
            
        except Exception as e:
            return False, f"å¡«å……å¤±è´¥: {str(e)}"
    
    def add_summary_statistics(self, sheet_name: str, target_columns: List[str] = None):
        """æ·»åŠ æ±‡æ€»ç»Ÿè®¡"""
        if sheet_name not in self.modified_data:
            return False, "å·¥ä½œè¡¨ä¸å­˜åœ¨"
        
        df = self.modified_data[sheet_name]
        
        try:
            if target_columns is None:
                target_columns = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if not target_columns:
                return False, "æ²¡æœ‰æ‰¾åˆ°æ•°å€¼åˆ—"
            
            # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡
            summary_data = []
            for col in target_columns:
                if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
                    stats = {
                        'åˆ—å': col,
                        'è®¡æ•°': df[col].count(),
                        'å‡å€¼': df[col].mean(),
                        'ä¸­ä½æ•°': df[col].median(),
                        'æ ‡å‡†å·®': df[col].std(),
                        'æœ€å°å€¼': df[col].min(),
                        'æœ€å¤§å€¼': df[col].max(),
                        'ç¼ºå¤±å€¼': df[col].isnull().sum()
                    }
                    summary_data.append(stats)
            
            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                summary_sheet_name = f"{sheet_name}_ç»Ÿè®¡æ±‡æ€»"
                self.modified_data[summary_sheet_name] = summary_df
                return True, f"æˆåŠŸåˆ›å»ºç»Ÿè®¡æ±‡æ€»å·¥ä½œè¡¨: {summary_sheet_name}"
            else:
                return False, "æ²¡æœ‰å¯ç»Ÿè®¡çš„æ•°å€¼æ•°æ®"
                
        except Exception as e:
            return False, f"åˆ›å»ºç»Ÿè®¡æ±‡æ€»å¤±è´¥: {str(e)}"
    
    def export_to_excel(self, output_path: str = None) -> str:
        """å¯¼å‡ºä¿®æ”¹åçš„æ•°æ®åˆ°Excelæ–‡ä»¶"""
        try:
            if output_path is None:
                timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"modified_excel_{timestamp}.xlsx"
            
            # åˆ›å»ºæ–°çš„å·¥ä½œç°¿
            wb = openpyxl.Workbook()
            wb.remove(wb.active)  # åˆ é™¤é»˜è®¤å·¥ä½œè¡¨
            
            for sheet_name, df in self.modified_data.items():
                ws = wb.create_sheet(title=sheet_name)
                
                # å†™å…¥åˆ—æ ‡é¢˜
                for col_idx, column in enumerate(df.columns, 1):
                    cell = ws.cell(row=1, column=col_idx, value=column)
                    cell.font = Font(bold=True)
                    cell.fill = PatternFill(start_color="E6E6FA", end_color="E6E6FA", fill_type="solid")
                
                # å†™å…¥æ•°æ®
                for row_idx, row in df.iterrows():
                    for col_idx, value in enumerate(row, 1):
                        ws.cell(row=row_idx + 2, column=col_idx, value=value)
                
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                for column in ws.columns:
                    max_length = 0
                    column_letter = get_column_letter(column[0].column)
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    ws.column_dimensions[column_letter].width = adjusted_width
            
            # ä¿å­˜æ–‡ä»¶
            wb.save(output_path)
            return output_path
            
        except Exception as e:
            raise Exception(f"å¯¼å‡ºExcelæ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get_data_preview(self, sheet_name: str) -> str:
        """è·å–æ•°æ®é¢„è§ˆ"""
        if sheet_name in self.modified_data:
            df = self.modified_data[sheet_name]
            return self.df_to_markdown(df, sheet_name)
        return "å·¥ä½œè¡¨ä¸å­˜åœ¨"

class DataAnalyzer:
    """æ•°æ®åˆ†æç±»"""
    
    @staticmethod
    def detect_data_types(df: pd.DataFrame) -> Dict[str, str]:
        """æ£€æµ‹æ•°æ®ç±»å‹"""
        type_info = {}
        try:
            for col in df.columns:
                try:
                    if col in df.columns and not df[col].empty:
                        if pd.api.types.is_numeric_dtype(df[col]):
                            if df[col].dtype == 'int64':
                                type_info[col] = "æ•´æ•°"
                            else:
                                type_info[col] = "æµ®ç‚¹æ•°"
                        elif pd.api.types.is_datetime64_any_dtype(df[col]):
                            type_info[col] = "æ—¥æœŸæ—¶é—´"
                        elif pd.api.types.is_bool_dtype(df[col]):
                            type_info[col] = "å¸ƒå°”å€¼"
                        else:
                            type_info[col] = "æ–‡æœ¬"
                    else:
                        type_info[col] = "æœªçŸ¥"
                except Exception as e:
                    type_info[col] = "æ£€æµ‹å¤±è´¥"
        except Exception as e:
            print(f"æ•°æ®ç±»å‹æ£€æµ‹å‡ºé”™: {e}")
        return type_info
    
    @staticmethod
    def find_duplicates(df: pd.DataFrame) -> pd.DataFrame:
        """æŸ¥æ‰¾é‡å¤è¡Œ"""
        try:
            if df.empty:
                return pd.DataFrame()
            duplicates = df[df.duplicated(keep=False)]
            return duplicates
        except Exception as e:
            print(f"æŸ¥æ‰¾é‡å¤æ•°æ®æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
    
    @staticmethod
    def get_missing_value_report(df: pd.DataFrame) -> pd.DataFrame:
        """è·å–ç¼ºå¤±å€¼æŠ¥å‘Š"""
        missing_data = []
        try:
            for col in df.columns:
                try:
                    missing_count = df[col].isnull().sum()
                    missing_percent = (missing_count / len(df)) * 100 if len(df) > 0 else 0
                    missing_data.append({
                        'åˆ—å': str(col),
                        'ç¼ºå¤±æ•°é‡': missing_count,
                        'ç¼ºå¤±ç™¾åˆ†æ¯”': f"{missing_percent:.2f}%",
                        'æ•°æ®ç±»å‹': str(df[col].dtype) if col in df.columns else "æœªçŸ¥"
                    })
                except Exception as e:
                    missing_data.append({
                        'åˆ—å': str(col),
                        'ç¼ºå¤±æ•°é‡': 0,
                        'ç¼ºå¤±ç™¾åˆ†æ¯”': "0.00%",
                        'æ•°æ®ç±»å‹': "æ£€æµ‹å¤±è´¥"
                    })
        except Exception as e:
            print(f"è·å–ç¼ºå¤±å€¼æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        
        return pd.DataFrame(missing_data)
    
    @staticmethod
    def detect_outliers(df: pd.DataFrame, method: str = "iqr") -> Dict[str, List]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        outliers = {}
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                try:
                    if method == "iqr":
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()
                        outliers[col] = outlier_indices
                    elif method == "zscore":
                        try:
                            from scipy import stats
                            z_scores = np.abs(stats.zscore(df[col].dropna()))
                            outlier_indices = df[col].dropna().index[z_scores > 3].tolist()
                            outliers[col] = outlier_indices
                        except ImportError:
                            # å¦‚æœscipyä¸å¯ç”¨ï¼Œå›é€€åˆ°IQRæ–¹æ³•
                            Q1 = df[col].quantile(0.25)
                            Q3 = df[col].quantile(0.75)
                            IQR = Q3 - Q1
                            lower_bound = Q1 - 1.5 * IQR
                            upper_bound = Q3 + 1.5 * IQR
                            outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index.tolist()
                            outliers[col] = outlier_indices
                except Exception as e:
                    print(f"æ£€æµ‹åˆ— {col} çš„å¼‚å¸¸å€¼æ—¶å‡ºé”™: {e}")
                    outliers[col] = []
        except Exception as e:
            print(f"å¼‚å¸¸å€¼æ£€æµ‹å‡ºé”™: {e}")
        
        return outliers 

class LightweightExcelAnalyzer:
    """è½»é‡çº§Excelåˆ†æå™¨ - ä¸“ä¸ºAIæ™ºèƒ½åˆ†ætabè®¾è®¡ï¼Œç”Ÿæˆç²¾ç®€æç¤ºè¯"""
    
    def __init__(self):
        self.workbook = None
        self.analysis_cache = None
    
    def quick_analyze(self, file_path: str) -> Dict[str, Any]:
        """å¿«é€Ÿåˆ†æExcelæ–‡ä»¶ï¼Œç”ŸæˆAIåˆ†ææ‰€éœ€çš„æ ¸å¿ƒä¿¡æ¯"""
        try:
            self.workbook = openpyxl.load_workbook(file_path, data_only=True)
            
            analysis = {
                'file_info': {
                    'filename': file_path.split('/')[-1],
                    'sheet_count': len(self.workbook.sheetnames),
                    'sheet_names': self.workbook.sheetnames
                },
                'sheets_summary': {},
                'ai_prompt': ""
            }
            
            # åˆ†ææ¯ä¸ªå·¥ä½œè¡¨
            for sheet_name in self.workbook.sheetnames:
                sheet_summary = self._quick_analyze_sheet(sheet_name)
                analysis['sheets_summary'][sheet_name] = sheet_summary
            
            # ç”Ÿæˆç²¾ç®€AIæç¤ºè¯
            analysis['ai_prompt'] = self._generate_compact_prompt(analysis)
            
            self.analysis_cache = analysis
            return analysis
            
        except Exception as e:
            return {
                'error': f"åˆ†æå¤±è´¥: {str(e)}",
                'file_info': {'filename': file_path.split('/')[-1]},
                'ai_prompt': f"æ–‡ä»¶ {file_path.split('/')[-1]} åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def _quick_analyze_sheet(self, sheet_name: str) -> Dict[str, Any]:
        """å¿«é€Ÿåˆ†æå•ä¸ªå·¥ä½œè¡¨"""
        ws = self.workbook[sheet_name]
        
        # åŸºæœ¬ä¿¡æ¯
        basic_info = {
            'rows': ws.max_row,
            'columns': ws.max_column,
            'has_merged_cells': len(list(ws.merged_cells.ranges)) > 0,
            'merged_count': len(list(ws.merged_cells.ranges))
        }
        
        # æ™ºèƒ½æ£€æµ‹è¡¨å¤´å’Œæ•°æ®åŒºåŸŸ
        header_info = self._smart_detect_headers(ws)
        
        # æå–å…³é”®å­—æ®µä¿¡æ¯
        fields_info = self._extract_key_fields(ws, header_info)
        
        # æ•°æ®æ ·æœ¬
        data_samples = self._get_compact_samples(ws, header_info, limit=3)
        
        return {
            'basic': basic_info,
            'header_analysis': header_info,
            'fields': fields_info,
            'samples': data_samples,
            'sheet_type': self._classify_sheet_type(sheet_name, basic_info, fields_info)
        }
    
    def _smart_detect_headers(self, ws) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€æµ‹è¡¨å¤´ï¼Œç‰¹åˆ«å¤„ç†å¤æ‚çš„åˆå¹¶å•å…ƒæ ¼ç»“æ„"""
        merged_ranges = list(ws.merged_cells.ranges)
        
        # åˆ†æå‰10è¡Œï¼Œæ‰¾å‡ºæœ€å¯èƒ½çš„è¡¨å¤´è¡Œ
        header_candidates = []
        
        for row_num in range(1, min(11, ws.max_row + 1)):
            row_score = 0
            non_empty_count = 0
            text_count = 0
            
            for col in range(1, min(ws.max_column + 1, 50)):  # é™åˆ¶æ£€æŸ¥å‰50åˆ—
                cell = ws.cell(row_num, col)
                if cell.value is not None:
                    non_empty_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡æœ¬ï¼ˆè¡¨å¤´é€šå¸¸æ˜¯æ–‡æœ¬ï¼‰
                    try:
                        float(cell.value)
                    except (ValueError, TypeError):
                        text_count += 1
                        row_score += 1
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨åˆå¹¶å•å…ƒæ ¼ä¸­ï¼ˆè¡¨å¤´å¸¸æœ‰åˆå¹¶ï¼‰
                    for merged_range in merged_ranges:
                        if (merged_range.min_row <= row_num <= merged_range.max_row and 
                            merged_range.min_col <= col <= merged_range.max_col):
                            row_score += 0.5
                            break
            
            if non_empty_count > 0:
                text_ratio = text_count / non_empty_count
                if text_ratio > 0.5 and non_empty_count >= 3:  # è‡³å°‘3ä¸ªæœ‰å†…å®¹çš„å•å…ƒæ ¼ï¼Œä¸”å¤§éƒ¨åˆ†æ˜¯æ–‡æœ¬
                    header_candidates.append({
                        'row': row_num,
                        'score': row_score,
                        'non_empty': non_empty_count,
                        'text_ratio': text_ratio
                    })
        
        # é€‰æ‹©æœ€ä½³è¡¨å¤´è¡Œ
        best_header_row = 1
        if header_candidates:
            # ç»¼åˆè¯„åˆ†ï¼šå†…å®¹æ•°é‡ + æ–‡æœ¬æ¯”ä¾‹ + åˆå¹¶å•å…ƒæ ¼åŠ åˆ†
            best_candidate = max(header_candidates, 
                               key=lambda x: x['non_empty'] * x['text_ratio'] + x['score'])
            best_header_row = best_candidate['row']
        
        return {
            'suggested_header_row': best_header_row,
            'candidates': header_candidates,
            'has_complex_structure': len(merged_ranges) > 5,
            'merged_cell_analysis': self._analyze_header_merges(ws, merged_ranges, best_header_row)
        }
    
    def _analyze_header_merges(self, ws, merged_ranges: list, header_row: int) -> Dict[str, Any]:
        """åˆ†æè¡¨å¤´åŒºåŸŸçš„åˆå¹¶å•å…ƒæ ¼"""
        header_merges = []
        
        # æŸ¥æ‰¾è¡¨å¤´é™„è¿‘çš„åˆå¹¶å•å…ƒæ ¼
        for merged_range in merged_ranges:
            if abs(merged_range.min_row - header_row) <= 2:  # è¡¨å¤´å‰å2è¡Œå†…
                top_left_cell = ws.cell(merged_range.min_row, merged_range.min_col)
                merge_info = {
                    'range': str(merged_range),
                    'value': str(top_left_cell.value)[:30] if top_left_cell.value else "",
                    'span_cols': merged_range.max_col - merged_range.min_col + 1,
                    'span_rows': merged_range.max_row - merged_range.min_row + 1
                }
                header_merges.append(merge_info)
        
        return {
            'header_merges_count': len(header_merges),
            'header_merges': header_merges[:5],  # åªä¿ç•™å‰5ä¸ª
            'has_multi_level_headers': any(m['span_rows'] > 1 for m in header_merges)
        }
    
    def _extract_key_fields(self, ws, header_info: Dict[str, Any]) -> List[Dict[str, str]]:
        """æå–å…³é”®å­—æ®µä¿¡æ¯ï¼ˆç²¾ç®€ç‰ˆï¼‰"""
        header_row = header_info['suggested_header_row']
        fields = []
        
        # æ‰«æè¡¨å¤´è¡Œï¼Œæå–å­—æ®µå
        for col in range(1, min(ws.max_column + 1, 100)):  # é™åˆ¶å‰100åˆ—
            cell = ws.cell(header_row, col)
            
            if cell.value is not None:
                field_name = str(cell.value).strip()
                
                # å¦‚æœå­—æ®µåå¤ªçŸ­æˆ–ä¸ºç©ºï¼Œå°è¯•å‘ä¸ŠæŸ¥æ‰¾ï¼ˆå¤„ç†å¤šå±‚è¡¨å¤´ï¼‰
                if len(field_name) < 2:
                    for up_row in range(header_row - 1, max(0, header_row - 4), -1):
                        up_cell = ws.cell(up_row, col)
                        if up_cell.value and len(str(up_cell.value).strip()) > 1:
                            field_name = str(up_cell.value).strip()
                            break
                
                # å¿«é€Ÿæ£€æµ‹æ•°æ®ç±»å‹ï¼ˆåªçœ‹å‰å‡ ä¸ªæ•°æ®ï¼‰
                data_type = self._quick_detect_type(ws, header_row + 1, col)
                
                fields.append({
                    'name': field_name,
                    'column': get_column_letter(col),
                    'type': data_type
                })
            
            # å¦‚æœè¿ç»­é‡åˆ°å¤šä¸ªç©ºåˆ—ï¼Œå¯èƒ½å·²ç»è¶…å‡ºæ•°æ®èŒƒå›´
            elif col > 10:  # å‰10åˆ—ä¹‹åæ‰å¼€å§‹è¿™ä¸ªåˆ¤æ–­
                empty_count = 0
                for check_col in range(col, min(col + 5, ws.max_column + 1)):
                    if ws.cell(header_row, check_col).value is None:
                        empty_count += 1
                if empty_count >= 5:  # è¿ç»­5ä¸ªç©ºåˆ—
                    break
        
        return fields
    
    def _quick_detect_type(self, ws, start_row: int, col: int) -> str:
        """å¿«é€Ÿæ£€æµ‹å­—æ®µç±»å‹"""
        sample_values = []
        
        # åªæ£€æŸ¥å‰10ä¸ªæ•°æ®
        for row in range(start_row, min(start_row + 10, ws.max_row + 1)):
            cell = ws.cell(row, col)
            if cell.value is not None:
                sample_values.append(cell.value)
        
        if not sample_values:
            return "ç©ºå€¼"
        
        # ç®€å•ç±»å‹æ£€æµ‹
        numeric_count = 0
        for value in sample_values:
            try:
                float(value)
                numeric_count += 1
            except (ValueError, TypeError):
                pass
        
        if numeric_count > len(sample_values) * 0.7:
            return "æ•°å€¼"
        else:
            return "æ–‡æœ¬"
    
    def _get_compact_samples(self, ws, header_info: Dict[str, Any], limit: int = 3) -> List[Dict[str, Any]]:
        """è·å–ç´§å‡‘çš„æ•°æ®æ ·æœ¬"""
        header_row = header_info['suggested_header_row']
        samples = []
        
        # è·å–å­—æ®µå
        field_names = []
        for col in range(1, min(ws.max_column + 1, 20)):  # åªå–å‰20åˆ—
            cell = ws.cell(header_row, col)
            field_name = str(cell.value).strip() if cell.value else f"åˆ—{col}"
            field_names.append(field_name)
        
        # è·å–æ ·æœ¬æ•°æ®
        sample_count = 0
        for row in range(header_row + 1, ws.max_row + 1):
            if sample_count >= limit:
                break
            
            sample = {}
            has_data = False
            
            for i, col in enumerate(range(1, min(len(field_names) + 1, 21))):
                cell = ws.cell(row, col)
                value = cell.value
                
                if value is not None:
                    has_data = True
                    # é™åˆ¶å€¼çš„é•¿åº¦
                    if isinstance(value, str) and len(value) > 30:
                        value = value[:30] + "..."
                
                sample[field_names[i]] = value
            
            if has_data:
                samples.append(sample)
                sample_count += 1
        
        return samples
    
    def _classify_sheet_type(self, sheet_name: str, basic_info: Dict, fields_info: List) -> str:
        """åˆ†ç±»å·¥ä½œè¡¨ç±»å‹"""
        name_lower = sheet_name.lower()
        
        # åŸºäºåç§°åˆ¤æ–­
        if any(keyword in name_lower for keyword in ['æºæ•°æ®', 'source', 'data', 'æ˜ç»†']):
            return "æºæ•°æ®è¡¨"
        elif any(keyword in name_lower for keyword in ['æ±‡æ€»', 'summary', 'æ€»è¡¨', 'ç»Ÿè®¡']):
            return "æ±‡æ€»è¡¨"
        elif any(keyword in name_lower for keyword in ['æ¨¡æ¿', 'template', 'é…ç½®']):
            return "æ¨¡æ¿è¡¨"
        elif any(keyword in name_lower for keyword in ['æ¡†æ¶', 'framework', 'ç»“æ„']):
            return "æ¡†æ¶è¡¨"
        
        # åŸºäºæ•°æ®ç‰¹å¾åˆ¤æ–­
        rows, cols = basic_info['rows'], basic_info['columns']
        
        if rows > 1000:
            return "å¤§æ•°æ®è¡¨"
        elif cols > 50:
            return "å®½è¡¨"
        elif rows < 50 and cols < 10:
            return "é…ç½®è¡¨"
        else:
            return "æ ‡å‡†è¡¨"
    
    def _generate_compact_prompt(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆç²¾ç®€çš„AIåˆ†ææç¤ºè¯"""
        file_info = analysis['file_info']
        sheets_summary = analysis['sheets_summary']
        
        prompt_parts = []
        
        # æ–‡ä»¶æ¦‚è¿°
        prompt_parts.append(f"# Excelæ–‡ä»¶: {file_info['filename']}")
        prompt_parts.append(f"åŒ…å« {file_info['sheet_count']} ä¸ªå·¥ä½œè¡¨: {', '.join(file_info['sheet_names'])}")
        prompt_parts.append("")
        
        # å·¥ä½œè¡¨æ¦‚è¿°
        prompt_parts.append("## å·¥ä½œè¡¨æ¦‚è¿°")
        for sheet_name, summary in sheets_summary.items():
            basic = summary['basic']
            sheet_type = summary['sheet_type']
            fields_count = len(summary['fields'])
            
            prompt_parts.append(f"### {sheet_name} ({sheet_type})")
            prompt_parts.append(f"- è§„æ¨¡: {basic['rows']}è¡Œ Ã— {basic['columns']}åˆ—")
            prompt_parts.append(f"- å­—æ®µæ•°: {fields_count}")
            
            if basic['has_merged_cells']:
                prompt_parts.append(f"- âš ï¸ åŒ…å«{basic['merged_count']}ä¸ªåˆå¹¶å•å…ƒæ ¼")
            
            # å…³é”®å­—æ®µï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªï¼‰
            key_fields = summary['fields'][:10]
            if key_fields:
                field_names = [f['name'] for f in key_fields if f['name'] and len(f['name']) > 1]
                if field_names:
                    prompt_parts.append(f"- ä¸»è¦å­—æ®µ: {', '.join(field_names[:8])}")
            
            # æ•°æ®æ ·æœ¬ï¼ˆåªæ˜¾ç¤ºç¬¬ä¸€è¡Œï¼‰
            if summary['samples']:
                sample = summary['samples'][0]
                sample_items = []
                for k, v in list(sample.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
                    if v is not None and str(v).strip():
                        sample_items.append(f"{k}={v}")
                if sample_items:
                    prompt_parts.append(f"- æ ·æœ¬: {', '.join(sample_items)}")
            
            prompt_parts.append("")
        
        # åˆ†æå»ºè®®
        prompt_parts.append("## åˆ†æå»ºè®®")
        
        # æ‰¾å‡ºå¯èƒ½çš„ä¸»è¡¨
        main_sheets = [name for name, summary in sheets_summary.items() 
                      if summary['sheet_type'] in ['æºæ•°æ®è¡¨', 'å¤§æ•°æ®è¡¨']]
        if main_sheets:
            prompt_parts.append(f"- ä¸»è¦æ•°æ®æº: {', '.join(main_sheets)}")
        
        # è¯†åˆ«å…³è”å…³ç³»
        all_fields = {}
        for sheet_name, summary in sheets_summary.items():
            field_names = [f['name'] for f in summary['fields'] if f['name'] and len(f['name']) > 1]
            all_fields[sheet_name] = field_names
        
        # æŸ¥æ‰¾å…±åŒå­—æ®µ
        common_fields_found = False
        for sheet1, fields1 in all_fields.items():
            for sheet2, fields2 in all_fields.items():
                if sheet1 >= sheet2:
                    continue
                common = set(fields1) & set(fields2)
                if len(common) >= 2:
                    prompt_parts.append(f"- {sheet1} ä¸ {sheet2} å¯é€šè¿‡å­—æ®µå…³è”: {', '.join(list(common)[:3])}")
                    common_fields_found = True
        
        if not common_fields_found:
            prompt_parts.append("- æœªå‘ç°æ˜æ˜¾çš„è¡¨é—´å…³è”å­—æ®µ")
        
        # æ•°æ®è´¨é‡æé†’
        complex_sheets = [name for name, summary in sheets_summary.items() 
                         if summary['basic']['has_merged_cells']]
        if complex_sheets:
            prompt_parts.append(f"- âš ï¸ æ³¨æ„: {', '.join(complex_sheets)} å­˜åœ¨åˆå¹¶å•å…ƒæ ¼ï¼Œå¤„ç†æ—¶éœ€ç‰¹åˆ«æ³¨æ„")
        
        return "\n".join(prompt_parts)
    
    def get_analysis_cache(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        return self.analysis_cache or {} 