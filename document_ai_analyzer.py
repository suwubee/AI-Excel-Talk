#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£AIæ™ºèƒ½åˆ†æå™¨ - ä¸“é—¨ç”¨äºdocxã€pdfæ–‡æ¡£çš„AIåˆ†æ
ç±»ä¼¼äºExcelåˆ†æä¸­çš„EnhancedAIAnalyzerï¼Œæä¾›æ–‡æ¡£çš„æ·±åº¦AIåˆ†æåŠŸèƒ½
"""

import openai
import json
from typing import Dict, List, Any, Optional
from document_utils import AdvancedDocumentProcessor

class EnhancedDocumentAIAnalyzer:
    """å¢å¼ºå‹æ–‡æ¡£AIåˆ†æå™¨ - æä¾›æ·±åº¦æ–‡æ¡£åˆ†æå’ŒAIå¯¹è¯åŠŸèƒ½"""
    
    def __init__(self, api_key: str, base_url: str = None, model: str = "gpt-4o-mini"):
        """
        åˆå§‹åŒ–AIåˆ†æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹
        """
        self.api_key = api_key
        self.base_url = base_url or "https://api.openai.com/v1"
        self.model = model
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # æ–‡æ¡£å¤„ç†å™¨
        self.document_processor = AdvancedDocumentProcessor()
    
    def analyze_document_structure(self, document_analysis: Dict[str, Any]) -> str:
        """
        ä½¿ç”¨AIåˆ†ææ–‡æ¡£ç»“æ„å’Œå†…å®¹
        
        Args:
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            AIåˆ†ææŠ¥å‘Š
        """
        try:
            # æ„å»ºAIåˆ†ææç¤ºè¯
            prompt = self._build_document_analysis_prompt(document_analysis)
            
            # è°ƒç”¨AIè¿›è¡Œåˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç†è§£å’Œåˆ†æå„ç§ç±»å‹çš„æ–‡æ¡£ã€‚
è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦ä¸šåŠ¡åˆ†æï¼ŒåŒ…æ‹¬ï¼š

1. æ–‡æ¡£ç±»å‹å’Œç”¨é€”è¯†åˆ«
2. å†…å®¹ä¸»é¢˜å’Œä¸šåŠ¡åœºæ™¯åˆ†æ
3. æ–‡æ¡£ç»“æ„ç‰¹ç‚¹åˆ†æ
4. å…³é”®ä¿¡æ¯æå–å»ºè®®
5. å¯èƒ½çš„åˆ†ææ–¹å‘å’Œä»·å€¼å‘ç°

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ï¼Œæä¾›å®ç”¨çš„æ´å¯Ÿå’Œå»ºè®®ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"AIåˆ†æå¤±è´¥: {str(e)}"
    
    def _build_document_analysis_prompt(self, document_analysis: Dict[str, Any]) -> str:
        """æ„å»ºæ–‡æ¡£åˆ†æçš„AIæç¤ºè¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        preview_data = document_analysis.get('preview_data', {})
        ai_data = document_analysis.get('ai_analysis_data', {})
        
        prompt_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        prompt_parts.append(f"# æ–‡æ¡£åˆ†æè¯·æ±‚")
        prompt_parts.append(f"")
        prompt_parts.append(f"## æ–‡æ¡£åŸºæœ¬ä¿¡æ¯")
        prompt_parts.append(f"- æ–‡ä»¶å: {file_info.get('name', 'Unknown')}")
        prompt_parts.append(f"- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}")
        prompt_parts.append(f"- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB")
        
        # æ–‡æ¡£æ‘˜è¦
        doc_summary = ai_data.get('document_summary', {})
        if doc_summary:
            prompt_parts.append(f"- ä¼°ç®—é¡µæ•°: {doc_summary.get('estimated_pages', 'Unknown')}")
            prompt_parts.append(f"- å­—æ•°ç»Ÿè®¡: {doc_summary.get('word_count', 0)} è¯")
        
        # ç»“æ„åˆ†æ
        prompt_parts.append(f"")
        prompt_parts.append(f"## æ–‡æ¡£ç»“æ„ç‰¹å¾")
        
        if file_info.get('type') == 'docx':
            prompt_parts.append(f"- æ®µè½æ•°: {structure.get('total_paragraphs', 0)}")
            prompt_parts.append(f"- è¡¨æ ¼æ•°: {structure.get('tables_count', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        elif file_info.get('type') == 'pdf':
            prompt_parts.append(f"- é¡µæ•°: {structure.get('total_pages', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        
        # æ ‡é¢˜å±‚çº§ç»“æ„
        headings = structure.get('headings', {})
        if headings:
            prompt_parts.append(f"")
            prompt_parts.append(f"## æ ‡é¢˜å±‚çº§ç»“æ„")
            for level in sorted(headings.keys()):
                heading_list = headings[level]
                prompt_parts.append(f"### {level}çº§æ ‡é¢˜ (å…±{len(heading_list)}ä¸ª)")
                for heading in heading_list[:3]:  # æ˜¾ç¤ºå‰3ä¸ªä½œä¸ºç¤ºä¾‹
                    text = heading.get('text', str(heading))[:100]
                    prompt_parts.append(f"- {text}")
                if len(heading_list) > 3:
                    prompt_parts.append(f"... è¿˜æœ‰{len(heading_list) - 3}ä¸ª")
        
        # å­—ä½“ä¿¡æ¯
        fonts = structure.get('fonts_used', [])
        if fonts:
            prompt_parts.append(f"")
            prompt_parts.append(f"## å­—ä½“ä½¿ç”¨æƒ…å†µ")
            prompt_parts.append(f"- ä½¿ç”¨äº†{len(fonts)}ç§å­—ä½“")
            prompt_parts.append(f"- ä¸»è¦å­—ä½“: {', '.join(fonts[:5])}")
        
        # å†…å®¹é¢„è§ˆï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        if preview_data.get('status') == 'success':
            content = preview_data.get('content', '')
            if content:
                preview_content = content[:2000]  # é™åˆ¶åœ¨2000å­—ç¬¦
                prompt_parts.append(f"")
                prompt_parts.append(f"## æ–‡æ¡£å†…å®¹é¢„è§ˆï¼ˆå‰2000å­—ç¬¦ï¼‰")
                prompt_parts.append(f"```")
                prompt_parts.append(preview_content)
                prompt_parts.append(f"```")
        
        return "\n".join(prompt_parts)
    
    def chat_with_document(self, message: str, document_analysis: Dict[str, Any], context: str = "") -> str:
        """
        ä¸æ–‡æ¡£è¿›è¡ŒAIå¯¹è¯
        
        Args:
            message: ç”¨æˆ·é—®é¢˜
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            context: å¯¹è¯ä¸Šä¸‹æ–‡
            
        Returns:
            AIå›å¤
        """
        try:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            system_prompt = self._build_chat_system_prompt(document_analysis)
            
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                }
            ]
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if context:
                messages.append({
                    "role": "assistant", 
                    "content": f"åŸºäºä¹‹å‰çš„åˆ†æï¼š\n{context[:1000]}"  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                })
            
            # æ·»åŠ ç”¨æˆ·é—®é¢˜
            messages.append({
                "role": "user",
                "content": message
            })
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"AIå¯¹è¯å¤±è´¥: {str(e)}"
    
    def _build_chat_system_prompt(self, document_analysis: Dict[str, Any]) -> str:
        """æ„å»ºå¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·åˆ†æä¸€ä¸ª{file_info.get('type', 'Unknown').upper()}æ–‡æ¡£ã€‚

æ–‡æ¡£åŸºæœ¬ä¿¡æ¯ï¼š
- æ–‡ä»¶å: {file_info.get('name', 'Unknown')}
- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}
- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»“åˆæ–‡æ¡£çš„ç»“æ„å’Œå†…å®¹ä¿¡æ¯ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚

å›ç­”æ—¶è¯·ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. æä¾›å…·ä½“çš„åˆ†æå’Œå»ºè®®
3. å¿…è¦æ—¶ç»™å‡ºæ“ä½œæ­¥éª¤
4. ä¿æŒä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€é£æ ¼"""

        return system_prompt
    
    def generate_document_code_solution(self, task_description: str, document_analysis: Dict[str, Any], filename: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£å¤„ç†çš„ä»£ç è§£å†³æ–¹æ¡ˆ
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            filename: æ–‡æ¡£æ–‡ä»¶å
            
        Returns:
            ç”Ÿæˆçš„Pythonä»£ç 
        """
        try:
            # æ„å»ºä»£ç ç”Ÿæˆæç¤ºè¯
            prompt = self._build_code_generation_prompt(task_description, document_analysis, filename)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½Pythonç¼–ç¨‹ä¸“å®¶ï¼Œæ“…é•¿æ–‡æ¡£å¤„ç†å’Œæ•°æ®åˆ†æã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œç”Ÿæˆå®Œæ•´ã€å¯æ‰§è¡Œçš„Pythonä»£ç ã€‚

ä»£ç è¦æ±‚ï¼š
1. ä½¿ç”¨document_analyzerå’Œdocument_utilsæ¨¡å—
2. åŒ…å«å®Œæ•´çš„å¯¼å…¥è¯­å¥å’Œé”™è¯¯å¤„ç†
3. æä¾›è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š
4. ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
5. åŒ…å«ç»“æœè¾“å‡ºå’Œä¿å­˜åŠŸèƒ½

ä¸“æ³¨äºå®ç”¨æ€§å’Œå¯æ‰§è¡Œæ€§ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"# ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}\n\n# è¯·æ£€æŸ¥APIé…ç½®æˆ–ç½‘ç»œè¿æ¥"
    
    def _build_code_generation_prompt(self, task_description: str, document_analysis: Dict[str, Any], filename: str) -> str:
        """æ„å»ºä»£ç ç”Ÿæˆçš„æç¤ºè¯ï¼ŒåŒ…å«å®Œæ•´çš„å¯ç”¨å˜é‡ä¿¡æ¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        prompt_parts = []
        
        prompt_parts.append(f"# ä»£ç ç”Ÿæˆè¯·æ±‚")
        prompt_parts.append(f"")
        prompt_parts.append(f"## ä»»åŠ¡æè¿°")
        prompt_parts.append(f"{task_description}")
        prompt_parts.append(f"")
        prompt_parts.append(f"## ç›®æ ‡æ–‡æ¡£ä¿¡æ¯")
        prompt_parts.append(f"- æ–‡ä»¶å: {filename}")
        prompt_parts.append(f"- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}")
        prompt_parts.append(f"- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB")
        
        # æä¾›æ–‡æ¡£ç»“æ„ä¿¡æ¯
        if file_info.get('type') == 'docx':
            prompt_parts.append(f"- æ®µè½æ•°: {structure.get('total_paragraphs', 0)}")
            prompt_parts.append(f"- è¡¨æ ¼æ•°: {structure.get('tables_count', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        elif file_info.get('type') == 'pdf':
            prompt_parts.append(f"- é¡µæ•°: {structure.get('total_pages', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        
        # æ ‡é¢˜ä¿¡æ¯
        headings = structure.get('headings', {})
        if headings:
            prompt_parts.append(f"")
            prompt_parts.append(f"## æ–‡æ¡£æ ‡é¢˜ç»“æ„")
            for level in sorted(headings.keys()):
                heading_list = headings[level]
                prompt_parts.append(f"- {level}çº§æ ‡é¢˜: {len(heading_list)}ä¸ª")
                if heading_list:
                    # æä¾›æ ‡é¢˜ç¤ºä¾‹
                    examples = [h.get('text', str(h))[:50] for h in heading_list[:2]]
                    prompt_parts.append(f"  ç¤ºä¾‹: {', '.join(examples)}")
        
        # å¯ç”¨å˜é‡ä¿¡æ¯ - ä¸Excelä¿æŒä¸€è‡´çš„æ ¼å¼
        prompt_parts.append(f"")
        prompt_parts.append(f"## å¯ç”¨å˜é‡")
        prompt_parts.append(f"- document_path: åŸå§‹æ–‡æ¡£æ–‡ä»¶è·¯å¾„")
        prompt_parts.append(f"- document_name: æ–‡ä»¶å ({filename})")
        prompt_parts.append(f"- document_type: æ–‡æ¡£ç±»å‹ ({file_info.get('type', 'Unknown').upper()})")
        prompt_parts.append(f"- document_data: å®Œæ•´æ–‡æ¡£åˆ†ææ•°æ®å­—å…¸")
        prompt_parts.append(f"- file_info: æ–‡ä»¶åŸºæœ¬ä¿¡æ¯å­—å…¸")
        prompt_parts.append(f"- structure_analysis: ç»“æ„åˆ†æç»“æœå­—å…¸")
        prompt_parts.append(f"- document_analyzer: DocumentAnalyzerå®ä¾‹")
        prompt_parts.append(f"- document_processor: AdvancedDocumentProcessorå®ä¾‹")
        prompt_parts.append(f"- search_engine: DocumentSearchEngineå®ä¾‹")
        
        # ç”¨æˆ·å·¥ä½œç©ºé—´å˜é‡
        prompt_parts.append(f"")
        prompt_parts.append(f"## ç”¨æˆ·å·¥ä½œç©ºé—´å˜é‡")
        prompt_parts.append(f"- user_session_id: ç”¨æˆ·ä¼šè¯ID")
        prompt_parts.append(f"- user_workspace: ç”¨æˆ·ä¸“å±å·¥ä½œç©ºé—´è·¯å¾„")
        prompt_parts.append(f"- user_exports_dir: ç”¨æˆ·å¯¼å‡ºæ–‡ä»¶ç›®å½•")
        prompt_parts.append(f"- save_to_exports(filename, data): ä¿å­˜æ–‡ä»¶åˆ°å¯¼å‡ºç›®å½•çš„å‡½æ•°")
        
        # å¯ç”¨çš„å·¥å…·ç±»å’Œå‡½æ•°
        prompt_parts.append(f"")
        prompt_parts.append(f"## å¯ç”¨å·¥å…·å’Œæ–¹æ³•")
        prompt_parts.append(f"- document_analyzer.analyze_document(path): é‡æ–°åˆ†ææ–‡æ¡£")
        prompt_parts.append(f"- document_processor.search_content(keyword, context_lines): æœç´¢å…³é”®è¯")
        prompt_parts.append(f"- document_processor.get_document_preview(): è·å–æ–‡æ¡£é¢„è§ˆ")
        prompt_parts.append(f"- document_processor.get_structure_summary(): è·å–ç»“æ„æ‘˜è¦")
        prompt_parts.append(f"- search_engine.search_content(keyword, context_lines): é«˜çº§æœç´¢")
        
        prompt_parts.append(f"")
        prompt_parts.append(f"è¯·ç”Ÿæˆå®Œæ•´çš„Pythonä»£ç æ¥å®Œæˆä¸Šè¿°ä»»åŠ¡ã€‚ä»£ç è¦æ±‚ï¼š")
        prompt_parts.append(f"")
        prompt_parts.append(f"1. **æ–‡æ¡£æ–‡ä»¶çº§åˆ«æ“ä½œ**:")
        prompt_parts.append(f"   - å¯ä»¥ä½¿ç”¨document_pathè®¿é—®åŸå§‹æ–‡æ¡£æ–‡ä»¶")
        prompt_parts.append(f"   - å¯ä»¥ä½¿ç”¨document_analyzeré‡æ–°åˆ†ææ–‡æ¡£")
        prompt_parts.append(f"   - æ”¯æŒå¤æ‚çš„æ–‡æ¡£æ“ä½œï¼Œå¦‚ç»“æ„æå–ã€å†…å®¹åˆ†æç­‰")
        prompt_parts.append(f"")
        prompt_parts.append(f"2. **æœç´¢å’Œå†…å®¹æå–**:")
        prompt_parts.append(f"   - ä½¿ç”¨search_engineæˆ–document_processorè¿›è¡Œå…³é”®è¯æœç´¢")
        prompt_parts.append(f"   - æ”¯æŒæ‰¹é‡æœç´¢å¤šä¸ªå…³é”®è¯")
        prompt_parts.append(f"   - æå–ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç²¾ç¡®ä½ç½®")
        prompt_parts.append(f"")
        prompt_parts.append(f"3. **ç»¼åˆæ•°æ®å¤„ç†**:")
        prompt_parts.append(f"   - åˆ†ææ–‡æ¡£ç»“æ„å’Œæ ‡é¢˜å±‚çº§")
        prompt_parts.append(f"   - æå–å’Œå¤„ç†æ–‡æ¡£å†…å®¹")
        prompt_parts.append(f"   - åŒ…å«å¿…è¦çš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯")
        prompt_parts.append(f"   - æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šè¯´æ˜ä¸šåŠ¡é€»è¾‘")
        prompt_parts.append(f"")
        prompt_parts.append(f"4. **ç»“æœè¾“å‡º**:")
        prompt_parts.append(f"   - æä¾›æ¸…æ™°çš„å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯")
        prompt_parts.append(f"   - ä½¿ç”¨save_to_exports()å‡½æ•°ä¿å­˜ç»“æœæ–‡ä»¶")
        prompt_parts.append(f"   - åŒ…å«æ‰§è¡Œè¿›åº¦æç¤ºå’Œå…³é”®èŠ‚ç‚¹è¾“å‡º")
        prompt_parts.append(f"")
        prompt_parts.append(f"5. **ä»£ç ç»“æ„**:")
        prompt_parts.append(f"   - ç¡®ä¿ä»£ç å¯ä»¥ç›´æ¥æ‰§è¡Œ")
        prompt_parts.append(f"   - åŒ…å«å¿…è¦çš„å¯¼å…¥è¯­å¥")
        prompt_parts.append(f"   - ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘åˆ†æ˜")
        prompt_parts.append(f"")
        prompt_parts.append(f"ç‰¹åˆ«æ³¨æ„ï¼š")
        prompt_parts.append(f"- å……åˆ†åˆ©ç”¨document_dataã€file_infoã€structure_analysisç­‰å˜é‡")
        prompt_parts.append(f"- æ‰€æœ‰å¯¼å‡ºæ–‡ä»¶éƒ½è¦ä½¿ç”¨save_to_exports()å‡½æ•°ä¿å­˜")
        prompt_parts.append(f"- ä»£ç åº”è¯¥èƒ½å¤„ç†DOCXå’ŒPDFä¸¤ç§æ ¼å¼çš„å·®å¼‚")
        prompt_parts.append(f"- æä¾›è¯¦ç»†çš„ä¸­æ–‡è¾“å‡ºå’Œè¿›åº¦æç¤º")
        prompt_parts.append(f"")
        prompt_parts.append(f"è¯·åªè¿”å›çº¯Pythonä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•markdownæ ¼å¼æ ‡è®°ã€‚")
        
        return "\n".join(prompt_parts)
    
    def suggest_analysis_tasks(self, document_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        åŸºäºæ–‡æ¡£ç‰¹å¾å»ºè®®åˆ†æä»»åŠ¡
        
        Args:
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            å»ºè®®ä»»åŠ¡åˆ—è¡¨
        """
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        suggestions = []
        
        # åŸºæœ¬åˆ†æä»»åŠ¡
        suggestions.append({
            "title": "ğŸ” å…³é”®ä¿¡æ¯æå–",
            "description": "æœç´¢å’Œæå–æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¦‚æ—¥æœŸã€é‡‘é¢ã€äººåç­‰"
        })
        
        suggestions.append({
            "title": "ğŸ“‹ æ–‡æ¡£ç»“æ„åˆ†æ",
            "description": "åˆ†ææ–‡æ¡£çš„ç»„ç»‡ç»“æ„ï¼Œè¯†åˆ«ç« èŠ‚å’Œå±‚çº§å…³ç³»"
        })
        
        # åŸºäºæ–‡æ¡£ç±»å‹çš„å»ºè®®
        if file_info.get('type') == 'pdf':
            suggestions.append({
                "title": "ğŸ“„ PDFå†…å®¹æå–",
                "description": "æŒ‰é¡µé¢æå–PDFå†…å®¹ï¼Œåˆ†ææ¯é¡µçš„é‡ç‚¹ä¿¡æ¯"
            })
        
        # åŸºäºæ ‡é¢˜æ•°é‡çš„å»ºè®®
        headings = structure.get('headings', {})
        if headings and sum(len(h) for h in headings.values()) > 5:
            suggestions.append({
                "title": "ğŸ·ï¸ æ ‡é¢˜å¤§çº²ç”Ÿæˆ",
                "description": "åŸºäºæ ‡é¢˜å±‚çº§ç”Ÿæˆæ–‡æ¡£å¤§çº²å’Œç›®å½•ç»“æ„"
            })
        
        # åŸºäºæ–‡æ¡£å¤§å°çš„å»ºè®®
        if file_info.get('size_mb', 0) > 1:
            suggestions.append({
                "title": "ğŸ“Š å†…å®¹ç»Ÿè®¡åˆ†æ",
                "description": "ç»Ÿè®¡æ–‡æ¡£çš„å­—æ•°ã€æ®µè½æ•°ã€å…³é”®è¯é¢‘ç‡ç­‰"
            })
        
        suggestions.append({
            "title": "ğŸ”— å…³è”å†…å®¹å‘ç°",
            "description": "æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç›¸å…³å†…å®¹å’Œäº¤å‰å¼•ç”¨"
        })
        
        return suggestions

def main():
    """æµ‹è¯•å‡½æ•°"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    print("DocumentAIAnalyzer åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main() 