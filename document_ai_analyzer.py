#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£AIæ™ºèƒ½åˆ†æå™¨ - ä¸“é—¨ç”¨äºdocxã€pdfæ–‡æ¡£çš„AIåˆ†æ
ç±»ä¼¼äºExcelåˆ†æä¸­çš„EnhancedAIAnalyzerï¼Œæä¾›æ–‡æ¡£çš„æ·±åº¦AIåˆ†æåŠŸèƒ½
"""

import openai
import json
from typing import Dict, List, Any, Optional
from document_utils import AdvancedDocumentProcessor

class EnhancedDocumentAIAnalyzer:
    """å¢å¼ºå‹æ–‡æ¡£AIåˆ†æå™¨ - æä¾›æ·±åº¦æ–‡æ¡£åˆ†æå’ŒAIå¯¹è¯åŠŸèƒ½"""
    
    def __init__(self, api_key: str, base_url: str = None, model: str = "gpt-4o-mini"):
        """
        åˆå§‹åŒ–AIåˆ†æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
            model: ä½¿ç”¨çš„æ¨¡å‹
        """
        self.api_key = api_key
        self.base_url = base_url or "https://api.openai.com/v1"
        self.model = model
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # æ–‡æ¡£å¤„ç†å™¨
        self.document_processor = AdvancedDocumentProcessor()
    
    def analyze_document_structure(self, document_analysis: Dict[str, Any]) -> str:
        """
        ä½¿ç”¨AIåˆ†ææ–‡æ¡£ç»“æ„å’Œå†…å®¹
        
        Args:
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            AIåˆ†ææŠ¥å‘Š
        """
        try:
            # æ„å»ºAIåˆ†ææç¤ºè¯
            prompt = self._build_document_analysis_prompt(document_analysis)
            
            # è°ƒç”¨AIè¿›è¡Œåˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç†è§£å’Œåˆ†æå„ç§ç±»å‹çš„æ–‡æ¡£ã€‚
è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦ä¸šåŠ¡åˆ†æï¼ŒåŒ…æ‹¬ï¼š

1. æ–‡æ¡£ç±»å‹å’Œç”¨é€”è¯†åˆ«
2. å†…å®¹ä¸»é¢˜å’Œä¸šåŠ¡åœºæ™¯åˆ†æ
3. æ–‡æ¡£ç»“æ„ç‰¹ç‚¹åˆ†æ
4. å…³é”®ä¿¡æ¯æå–å»ºè®®
5. å¯èƒ½çš„åˆ†ææ–¹å‘å’Œä»·å€¼å‘ç°

è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ï¼Œæä¾›å®ç”¨çš„æ´å¯Ÿå’Œå»ºè®®ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"AIåˆ†æå¤±è´¥: {str(e)}"
    
    def _build_document_analysis_prompt(self, document_analysis: Dict[str, Any]) -> str:
        """æ„å»ºæ–‡æ¡£åˆ†æçš„AIæç¤ºè¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        preview_data = document_analysis.get('preview_data', {})
        ai_data = document_analysis.get('ai_analysis_data', {})
        
        prompt_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        prompt_parts.append(f"# æ–‡æ¡£åˆ†æè¯·æ±‚")
        prompt_parts.append(f"")
        prompt_parts.append(f"## æ–‡æ¡£åŸºæœ¬ä¿¡æ¯")
        prompt_parts.append(f"- æ–‡ä»¶å: {file_info.get('name', 'Unknown')}")
        prompt_parts.append(f"- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}")
        prompt_parts.append(f"- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB")
        
        # æ–‡æ¡£æ‘˜è¦
        doc_summary = ai_data.get('document_summary', {})
        if doc_summary:
            prompt_parts.append(f"- ä¼°ç®—é¡µæ•°: {doc_summary.get('estimated_pages', 'Unknown')}")
            prompt_parts.append(f"- å­—æ•°ç»Ÿè®¡: {doc_summary.get('word_count', 0)} è¯")
        
        # ç»“æ„åˆ†æ
        prompt_parts.append(f"")
        prompt_parts.append(f"## æ–‡æ¡£ç»“æ„ç‰¹å¾")
        
        if file_info.get('type') == 'docx':
            prompt_parts.append(f"- æ®µè½æ•°: {structure.get('total_paragraphs', 0)}")
            prompt_parts.append(f"- è¡¨æ ¼æ•°: {structure.get('tables_count', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        elif file_info.get('type') == 'pdf':
            prompt_parts.append(f"- é¡µæ•°: {structure.get('total_pages', 0)}")
            prompt_parts.append(f"- å›¾ç‰‡æ•°: {structure.get('images_count', 0)}")
        
        # æ ‡é¢˜å±‚çº§ç»“æ„
        headings = structure.get('headings', {})
        if headings:
            prompt_parts.append(f"")
            prompt_parts.append(f"## æ ‡é¢˜å±‚çº§ç»“æ„")
            for level in sorted(headings.keys()):
                heading_list = headings[level]
                prompt_parts.append(f"### {level}çº§æ ‡é¢˜ (å…±{len(heading_list)}ä¸ª)")
                for heading in heading_list[:3]:  # æ˜¾ç¤ºå‰3ä¸ªä½œä¸ºç¤ºä¾‹
                    text = heading.get('text', str(heading))[:100]
                    prompt_parts.append(f"- {text}")
                if len(heading_list) > 3:
                    prompt_parts.append(f"... è¿˜æœ‰{len(heading_list) - 3}ä¸ª")
        
        # å­—ä½“ä¿¡æ¯
        fonts = structure.get('fonts_used', [])
        if fonts:
            prompt_parts.append(f"")
            prompt_parts.append(f"## å­—ä½“ä½¿ç”¨æƒ…å†µ")
            prompt_parts.append(f"- ä½¿ç”¨äº†{len(fonts)}ç§å­—ä½“")
            prompt_parts.append(f"- ä¸»è¦å­—ä½“: {', '.join(fonts[:5])}")
        
        # å†…å®¹é¢„è§ˆï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        if preview_data.get('status') == 'success':
            content = preview_data.get('content', '')
            if content:
                preview_content = content[:2000]  # é™åˆ¶åœ¨2000å­—ç¬¦
                prompt_parts.append(f"")
                prompt_parts.append(f"## æ–‡æ¡£å†…å®¹é¢„è§ˆï¼ˆå‰2000å­—ç¬¦ï¼‰")
                prompt_parts.append(f"```")
                prompt_parts.append(preview_content)
                prompt_parts.append(f"```")
        
        return "\n".join(prompt_parts)
    
    def chat_with_document(self, message: str, document_analysis: Dict[str, Any], context: str = "") -> str:
        """
        ä¸æ–‡æ¡£è¿›è¡ŒAIå¯¹è¯
        
        Args:
            message: ç”¨æˆ·é—®é¢˜
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            context: å¯¹è¯ä¸Šä¸‹æ–‡
            
        Returns:
            AIå›å¤
        """
        try:
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            system_prompt = self._build_chat_system_prompt(document_analysis)
            
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                }
            ]
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if context:
                messages.append({
                    "role": "assistant", 
                    "content": f"åŸºäºä¹‹å‰çš„åˆ†æï¼š\n{context[:1000]}"  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                })
            
            # æ·»åŠ ç”¨æˆ·é—®é¢˜
            messages.append({
                "role": "user",
                "content": message
            })
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=1500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"AIå¯¹è¯å¤±è´¥: {str(e)}"
    
    def _build_chat_system_prompt(self, document_analysis: Dict[str, Any]) -> str:
        """æ„å»ºå¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œæ­£åœ¨å¸®åŠ©ç”¨æˆ·åˆ†æä¸€ä¸ª{file_info.get('type', 'Unknown').upper()}æ–‡æ¡£ã€‚

æ–‡æ¡£åŸºæœ¬ä¿¡æ¯ï¼š
- æ–‡ä»¶å: {file_info.get('name', 'Unknown')}
- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}
- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œç»“åˆæ–‡æ¡£çš„ç»“æ„å’Œå†…å®¹ä¿¡æ¯ï¼Œæä¾›ä¸“ä¸šã€å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚

å›ç­”æ—¶è¯·ï¼š
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. æä¾›å…·ä½“çš„åˆ†æå’Œå»ºè®®
3. å¿…è¦æ—¶ç»™å‡ºæ“ä½œæ­¥éª¤
4. ä¿æŒä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€é£æ ¼"""

        return system_prompt
    
    def generate_document_code_solution(self, task_description: str, document_analysis: Dict[str, Any], filename: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£å¤„ç†çš„ä»£ç è§£å†³æ–¹æ¡ˆ
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            filename: æ–‡æ¡£æ–‡ä»¶å
            
        Returns:
            ç”Ÿæˆçš„Pythonä»£ç 
        """
        try:
            # æ„å»ºä»£ç ç”Ÿæˆæç¤ºè¯
            prompt = self._build_code_generation_prompt(task_description, document_analysis, filename)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½Pythonç¼–ç¨‹ä¸“å®¶ï¼Œæ“…é•¿æ–‡æ¡£å¤„ç†å’Œæ•°æ®åˆ†æã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œç”Ÿæˆå®Œæ•´ã€å¯æ‰§è¡Œçš„Pythonä»£ç ã€‚

## ä¸¥æ ¼çš„ä»£ç æ ¼å¼è¦æ±‚ï¼š
1. **åªè¿”å›çº¯Pythonä»£ç **ï¼Œä¸è¦åŒ…å«ä»»ä½•markdownæ ¼å¼æ ‡è®°ï¼ˆå¦‚```python æˆ– ```ï¼‰
2. **ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—**ï¼Œåªè¿”å›å¯ç›´æ¥æ‰§è¡Œçš„ä»£ç 
3. **ä»£ç å¿…é¡»æ˜¯å®Œæ•´çš„**ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„å¯¼å…¥è¯­å¥
4. **æ¯è¡Œä»£ç å‰ä¸è¦æœ‰é¢å¤–çš„ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦**

## ä»£ç è´¨é‡è¦æ±‚ï¼š
1. ä½¿ç”¨document_analyzerå’Œdocument_utilsæ¨¡å—è¿›è¡Œæ–‡æ¡£å¤„ç†
2. åŒ…å«å®Œæ•´çš„å¯¼å…¥è¯­å¥å’Œé”™è¯¯å¤„ç†
3. æä¾›è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šï¼Œè§£é‡Šæ¯ä¸ªæ­¥éª¤çš„ä½œç”¨
4. ä»£ç ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘åˆ†æ˜ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
5. åŒ…å«ç»“æœè¾“å‡ºå’Œä¿å­˜åŠŸèƒ½
6. æ·»åŠ æ‰§è¡Œè¿›åº¦æç¤ºå’Œå…³é”®èŠ‚ç‚¹çš„çŠ¶æ€è¾“å‡º

## åŠŸèƒ½å®ç°è¦æ±‚ï¼š
1. **æ–‡æ¡£åŠ è½½å’ŒéªŒè¯**ï¼šæ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®
2. **æ•°æ®å¤„ç†é€»è¾‘**ï¼šå®ç°å…·ä½“çš„åˆ†æã€æå–ã€è½¬æ¢ç­‰åŠŸèƒ½
3. **ç»“æœè¾“å‡º**ï¼šæ¸…æ™°å±•ç¤ºå¤„ç†ç»“æœï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯
4. **æ–‡ä»¶ä¿å­˜**ï¼šå¦‚éœ€ä¿å­˜ç»“æœï¼Œæä¾›ä¿å­˜åˆ°æ–‡ä»¶çš„åŠŸèƒ½
5. **é”™è¯¯å¤„ç†**ï¼šå¯¹å¯èƒ½çš„å¼‚å¸¸è¿›è¡Œæ•è·å’Œå¤„ç†

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚ç”Ÿæˆä»£ç ï¼Œç¡®ä¿ä»£ç å¯ä»¥ç›´æ¥å¤åˆ¶ç²˜è´´è¿è¡Œã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            # è·å–ç”Ÿæˆçš„ä»£ç 
            code = response.choices[0].message.content
            
            # å½»åº•æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼æ ‡è®°
            import re
            
            # ç§»é™¤å¼€å¤´çš„å„ç§markdownä»£ç å—æ ‡è®°
            code = re.sub(r'^```(?:python|py)?\s*\n?', '', code, flags=re.IGNORECASE | re.MULTILINE)
            
            # ç§»é™¤ç»“å°¾çš„markdownä»£ç å—æ ‡è®°
            code = re.sub(r'\n?\s*```\s*$', '', code, flags=re.MULTILINE)
            
            # ç§»é™¤å¯èƒ½çš„è¡Œå†…ä»£ç æ ‡è®°
            code = re.sub(r'`{1,3}([^`]+)`{1,3}', r'\1', code)
            
            # ç§»é™¤å¯èƒ½çš„è§£é‡Šæ€§æ–‡å­—æ®µè½ï¼ˆæ£€æµ‹éä»£ç å†…å®¹ï¼‰
            lines = code.split('\n')
            filtered_lines = []
            
            for line in lines:
                stripped = line.strip()
                # è·³è¿‡çº¯è§£é‡Šæ€§æ–‡å­—è¡Œï¼ˆä¸åŒ…å«ä»£ç ç‰¹å¾çš„è¡Œï¼‰
                if stripped and not any([
                    stripped.startswith('#'),  # æ³¨é‡Š
                    stripped.startswith('import '),  # å¯¼å…¥è¯­å¥
                    stripped.startswith('from '),  # å¯¼å…¥è¯­å¥
                    '=' in stripped,  # èµ‹å€¼è¯­å¥
                    stripped.startswith('def '),  # å‡½æ•°å®šä¹‰
                    stripped.startswith('class '),  # ç±»å®šä¹‰
                    stripped.startswith('if '),  # æ¡ä»¶è¯­å¥
                    stripped.startswith('for '),  # å¾ªç¯è¯­å¥
                    stripped.startswith('while '),  # å¾ªç¯è¯­å¥
                    stripped.startswith('try:'),  # å¼‚å¸¸å¤„ç†
                    stripped.startswith('except'),  # å¼‚å¸¸å¤„ç†
                    stripped.startswith('finally:'),  # å¼‚å¸¸å¤„ç†
                    stripped.startswith('with '),  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                    stripped.endswith(':'),  # ä»£ç å—
                    'print(' in stripped,  # è¾“å‡ºè¯­å¥
                    stripped.startswith('    '),  # ç¼©è¿›ä»£ç 
                ]):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯è§£é‡Šæ€§æ–‡å­—
                    if not any(char in stripped for char in ['(', ')', '[', ']', '{', '}', '=', '.', ',']):
                        continue  # è·³è¿‡çº¯æ–‡å­—è¯´æ˜
                
                filtered_lines.append(line)
            
            # é‡æ–°ç»„åˆä»£ç 
            code = '\n'.join(filtered_lines)
            
            return code.strip()
            
        except Exception as e:
            return f"# ä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}\n\n# è¯·æ£€æŸ¥APIé…ç½®æˆ–ç½‘ç»œè¿æ¥"
    
    def _build_code_generation_prompt(self, task_description: str, document_analysis: Dict[str, Any], filename: str) -> str:
        """æ„å»ºä»£ç ç”Ÿæˆçš„æç¤ºè¯"""
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        prompt_parts = []
        
        prompt_parts.append(f"# ä»£ç ç”Ÿæˆè¯·æ±‚")
        prompt_parts.append(f"")
        prompt_parts.append(f"## ä»»åŠ¡æè¿°")
        prompt_parts.append(f"{task_description}")
        prompt_parts.append(f"")
        prompt_parts.append(f"## ç›®æ ‡æ–‡æ¡£ä¿¡æ¯")
        prompt_parts.append(f"- æ–‡ä»¶å: {filename}")
        prompt_parts.append(f"- æ–‡æ¡£ç±»å‹: {file_info.get('type', 'Unknown').upper()}")
        prompt_parts.append(f"- æ–‡ä»¶å¤§å°: {file_info.get('size_mb', 0)} MB")
        
        # æä¾›æ–‡æ¡£ç»“æ„ä¿¡æ¯
        if file_info.get('type') == 'docx':
            prompt_parts.append(f"- æ®µè½æ•°: {structure.get('total_paragraphs', 0)}")
            prompt_parts.append(f"- è¡¨æ ¼æ•°: {structure.get('tables_count', 0)}")
        elif file_info.get('type') == 'pdf':
            prompt_parts.append(f"- é¡µæ•°: {structure.get('total_pages', 0)}")
        
        # æ ‡é¢˜ä¿¡æ¯
        headings = structure.get('headings', {})
        if headings:
            prompt_parts.append(f"")
            prompt_parts.append(f"## æ–‡æ¡£æ ‡é¢˜ç»“æ„")
            for level in sorted(headings.keys()):
                heading_list = headings[level]
                prompt_parts.append(f"- {level}çº§æ ‡é¢˜: {len(heading_list)}ä¸ª")
        
        # å¯ç”¨çš„å·¥å…·ç±»å’Œå‡½æ•°
        prompt_parts.append(f"")
        prompt_parts.append(f"## å¯ç”¨å·¥å…·å’Œæ–¹æ³•")
        prompt_parts.append(f"### DocumentAnalyzer ç±»çš„å¯ç”¨æ–¹æ³•:")
        prompt_parts.append(f"- analyzer.analyze_document(file_path): å®Œæ•´åˆ†ææ–‡æ¡£")
        prompt_parts.append(f"- analyzer.get_page_count(file_path): è·å–æ–‡æ¡£é¡µæ•°")
        prompt_parts.append(f"- analyzer.analyze_structure(file_path): åˆ†ææ–‡æ¡£ç»“æ„")
        prompt_parts.append(f"- analyzer.get_document_info(file_path): è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯")
        prompt_parts.append(f"- analyzer.search_keyword_context(keyword, context_lines): æœç´¢å…³é”®è¯")
        prompt_parts.append(f"")
        prompt_parts.append(f"### AdvancedDocumentProcessor ç±»çš„å¯ç”¨æ–¹æ³•:")
        prompt_parts.append(f"- processor.load_document(file_path): åŠ è½½å¹¶åˆ†ææ–‡æ¡£")
        prompt_parts.append(f"- processor.search_content(keyword, context_lines): æœç´¢å†…å®¹")
        prompt_parts.append(f"- processor.export_analysis_result(): å¯¼å‡ºåˆ†æç»“æœ")
        prompt_parts.append(f"")
        prompt_parts.append(f"### é‡è¦æç¤º:")
        prompt_parts.append(f"- å¿…é¡»ä¼ å…¥å®Œæ•´çš„æ–‡æ¡£è·¯å¾„å‚æ•°")
        prompt_parts.append(f"- ä½¿ç”¨ doc_file_path å˜é‡ä½œä¸ºæ–‡æ¡£è·¯å¾„")
        prompt_parts.append(f"- æ‰€æœ‰æ–¹æ³•éƒ½æœ‰å®Œæ•´çš„é”™è¯¯å¤„ç†")
        prompt_parts.append(f"")
        prompt_parts.append(f"## å¯ç”¨å˜é‡å’Œå‡½æ•°")
        prompt_parts.append(f"### æ–‡æ¡£è·¯å¾„å˜é‡")
        prompt_parts.append(f"- doc_file_path: å½“å‰æ–‡æ¡£çš„å®Œæ•´æ–‡ä»¶è·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰")
        prompt_parts.append(f"- doc_file_name: å½“å‰æ–‡æ¡£çš„æ–‡ä»¶å")
        prompt_parts.append(f"- document_analysis: å®Œæ•´çš„æ–‡æ¡£åˆ†æç»“æœå­—å…¸")
        prompt_parts.append(f"- doc_info: æ–‡æ¡£åŸºæœ¬ä¿¡æ¯ï¼ˆfile_infoï¼‰")
        prompt_parts.append(f"- doc_structure: æ–‡æ¡£ç»“æ„ä¿¡æ¯ï¼ˆstructure_analysisï¼‰")
        prompt_parts.append(f"")
        prompt_parts.append(f"### å·¥ä½œç©ºé—´å˜é‡")
        prompt_parts.append(f"- user_workspace: ç”¨æˆ·å·¥ä½œç©ºé—´ç›®å½•")
        prompt_parts.append(f"- user_exports_dir: ç”¨æˆ·å¯¼å‡ºç›®å½•")
        prompt_parts.append(f"- user_uploads_dir: ç”¨æˆ·ä¸Šä¼ ç›®å½•")
        prompt_parts.append(f"- user_temp_dir: ç”¨æˆ·ä¸´æ—¶ç›®å½•")
        prompt_parts.append(f"")
        prompt_parts.append(f"### å®ç”¨å‡½æ•°")
        prompt_parts.append(f"- save_to_exports(filename, data): ä¿å­˜æ–‡ä»¶åˆ°å¯¼å‡ºç›®å½•")
        prompt_parts.append(f"- get_export_path(filename): è·å–å¯¼å‡ºæ–‡ä»¶è·¯å¾„")
        prompt_parts.append(f"- get_temp_path(filename): è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„")
        prompt_parts.append(f"")
        prompt_parts.append(f"### é‡è¦æç¤º")
        prompt_parts.append(f"- ä½¿ç”¨ doc_file_path å˜é‡è®¿é—®å½“å‰æ–‡æ¡£")
        prompt_parts.append(f"- é¦–å…ˆæ£€æŸ¥ doc_file_path æ˜¯å¦å­˜åœ¨å’Œæœ‰æ•ˆ")
        prompt_parts.append(f"- æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶éƒ½åº”ä¿å­˜åˆ°ç”¨æˆ·å¯¼å‡ºç›®å½•")
        prompt_parts.append(f"")
        prompt_parts.append(f"## å¯ç”¨åº“å’Œæ¨¡å—")
        prompt_parts.append(f"- æ ‡å‡†åº“: os, datetime, json, shutil, tempfile, sys, time, re, Path")
        prompt_parts.append(f"- æ•°æ®å¤„ç†: BytesIO, StringIO, base64, hashlib, uuid")
        prompt_parts.append(f"- PDFå¤„ç†: fitz (PyMuPDF), PyPDF2 (å¦‚æœéœ€è¦)")
        prompt_parts.append(f"- è¿›åº¦æ˜¾ç¤º: tqdm (è¿›åº¦æ¡)")
        prompt_parts.append(f"- å›¾åƒå¤„ç†: PIL/Pillow (é€šè¿‡æ–‡æ¡£åˆ†æå™¨)")
        prompt_parts.append(f"- æ–‡æ¡£å¤„ç†: python-docx, markitdown")
        
        prompt_parts.append(f"")
        prompt_parts.append(f"## ä»£ç ç”Ÿæˆè¦æ±‚")
        prompt_parts.append(f"è¯·ç”Ÿæˆå®Œæ•´çš„Pythonä»£ç æ¥å®Œæˆä¸Šè¿°ä»»åŠ¡ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹è§„èŒƒï¼š")
        prompt_parts.append(f"")
        prompt_parts.append(f"### æ ¼å¼è¦æ±‚ï¼š")
        prompt_parts.append(f"- åªè¿”å›çº¯Pythonä»£ç ï¼Œä¸è¦åŒ…å«markdownæ ¼å¼")
        prompt_parts.append(f"- ä¸è¦æ·»åŠ ä»»ä½•```python æˆ– ``` æ ‡è®°")
        prompt_parts.append(f"- ä»£ç å¯ä»¥ç›´æ¥å¤åˆ¶ç²˜è´´æ‰§è¡Œ")
        prompt_parts.append(f"")
        prompt_parts.append(f"### ä»£ç ç»“æ„ï¼š")
        prompt_parts.append(f"1. å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å—")
        prompt_parts.append(f"2. æ£€æŸ¥ doc_file_path å˜é‡æ˜¯å¦å­˜åœ¨å’Œæœ‰æ•ˆ")
        prompt_parts.append(f"3. æ–‡æ¡£åŠ è½½å’Œåˆå§‹åŒ–å¤„ç†å™¨")
        prompt_parts.append(f"4. å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘")
        prompt_parts.append(f"5. ç»“æœè¾“å‡ºå’Œç»Ÿè®¡ä¿¡æ¯")
        prompt_parts.append(f"6. ä½¿ç”¨ save_to_exports() ä¿å­˜ç»“æœ")
        prompt_parts.append(f"")
        prompt_parts.append(f"### ä»£ç æ¨¡æ¿ç¤ºä¾‹ï¼š")
        prompt_parts.append(f"```")
        prompt_parts.append(f"# æ­¥éª¤1: å¯¼å…¥æ¨¡å—")
        prompt_parts.append(f"import os")
        prompt_parts.append(f"from document_analyzer import DocumentAnalyzer")
        prompt_parts.append(f"from document_utils import AdvancedDocumentProcessor")
        prompt_parts.append(f"")
        prompt_parts.append(f"# æ­¥éª¤2: æ£€æŸ¥æ–‡æ¡£è·¯å¾„")
        prompt_parts.append(f"if not doc_file_path or not os.path.exists(doc_file_path):")
        prompt_parts.append(f"    print('âŒ æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆé€‰æ‹©æ–‡æ¡£')")
        prompt_parts.append(f"    exit()")
        prompt_parts.append(f"")
        prompt_parts.append(f"# æ­¥éª¤3: åˆå§‹åŒ–åˆ†æå™¨")
        prompt_parts.append(f"analyzer = DocumentAnalyzer()")
        prompt_parts.append(f"processor = AdvancedDocumentProcessor()")
        prompt_parts.append(f"")
        prompt_parts.append(f"# æ­¥éª¤4: è·å–åŸºæœ¬ä¿¡æ¯")
        prompt_parts.append(f"doc_info = analyzer.get_document_info(doc_file_path)")
        prompt_parts.append(f"page_count = analyzer.get_page_count(doc_file_path)")
        prompt_parts.append(f"print(f'æ–‡æ¡£é¡µæ•°: {{page_count}}')")
        prompt_parts.append(f"")
        prompt_parts.append(f"# æ­¥éª¤5: åˆ†æç»“æ„")
        prompt_parts.append(f"structure = analyzer.analyze_structure(doc_file_path)")
        prompt_parts.append(f"")
        prompt_parts.append(f"# æ­¥éª¤6: ä¿å­˜ç»“æœ")
        prompt_parts.append(f"save_to_exports('analysis_result.json', structure)")
        prompt_parts.append(f"```")
        prompt_parts.append(f"")
        prompt_parts.append(f"### å¿…éœ€åŠŸèƒ½ï¼š")
        prompt_parts.append(f"- æ·»åŠ ä¸­æ–‡æ³¨é‡Šè§£é‡Šæ¯ä¸ªæ­¥éª¤")
        prompt_parts.append(f"- åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†")
        prompt_parts.append(f"- æ˜¾ç¤ºæ‰§è¡Œè¿›åº¦å’ŒçŠ¶æ€ä¿¡æ¯")
        prompt_parts.append(f"- è¾“å‡ºè¯¦ç»†çš„å¤„ç†ç»“æœ")
        
        return "\n".join(prompt_parts)
    
    def suggest_analysis_tasks(self, document_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        åŸºäºæ–‡æ¡£ç‰¹å¾å»ºè®®åˆ†æä»»åŠ¡
        
        Args:
            document_analysis: æ–‡æ¡£åˆ†æç»“æœ
            
        Returns:
            å»ºè®®ä»»åŠ¡åˆ—è¡¨
        """
        file_info = document_analysis.get('file_info', {})
        structure = document_analysis.get('structure_analysis', {})
        
        suggestions = []
        
        # åŸºæœ¬åˆ†æä»»åŠ¡
        suggestions.append({
            "title": "ğŸ” å…³é”®ä¿¡æ¯æå–",
            "description": "æœç´¢å’Œæå–æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¦‚æ—¥æœŸã€é‡‘é¢ã€äººåç­‰"
        })
        
        suggestions.append({
            "title": "ğŸ“‹ æ–‡æ¡£ç»“æ„åˆ†æ",
            "description": "åˆ†ææ–‡æ¡£çš„ç»„ç»‡ç»“æ„ï¼Œè¯†åˆ«ç« èŠ‚å’Œå±‚çº§å…³ç³»"
        })
        
        # åŸºäºæ–‡æ¡£ç±»å‹çš„å»ºè®®
        if file_info.get('type') == 'pdf':
            suggestions.append({
                "title": "ğŸ“„ PDFå†…å®¹æå–",
                "description": "æŒ‰é¡µé¢æå–PDFå†…å®¹ï¼Œåˆ†ææ¯é¡µçš„é‡ç‚¹ä¿¡æ¯"
            })
        
        # åŸºäºæ ‡é¢˜æ•°é‡çš„å»ºè®®
        headings = structure.get('headings', {})
        if headings and sum(len(h) for h in headings.values()) > 5:
            suggestions.append({
                "title": "ğŸ·ï¸ æ ‡é¢˜å¤§çº²ç”Ÿæˆ",
                "description": "åŸºäºæ ‡é¢˜å±‚çº§ç”Ÿæˆæ–‡æ¡£å¤§çº²å’Œç›®å½•ç»“æ„"
            })
        
        # åŸºäºæ–‡æ¡£å¤§å°çš„å»ºè®®
        if file_info.get('size_mb', 0) > 1:
            suggestions.append({
                "title": "ğŸ“Š å†…å®¹ç»Ÿè®¡åˆ†æ",
                "description": "ç»Ÿè®¡æ–‡æ¡£çš„å­—æ•°ã€æ®µè½æ•°ã€å…³é”®è¯é¢‘ç‡ç­‰"
            })
        
        suggestions.append({
            "title": "ğŸ”— å…³è”å†…å®¹å‘ç°",
            "description": "æŸ¥æ‰¾æ–‡æ¡£ä¸­çš„ç›¸å…³å†…å®¹å’Œäº¤å‰å¼•ç”¨"
        })
        
        return suggestions

def main():
    """æµ‹è¯•å‡½æ•°"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    print("DocumentAIAnalyzer åˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    main() 