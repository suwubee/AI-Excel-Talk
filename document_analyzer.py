#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£æ™ºèƒ½åˆ†æå™¨ - æ”¯æŒdocxã€pdfåˆ†æ
- ä½¿ç”¨markitdownè¿›è¡Œæ•°æ®é¢„è§ˆå’Œæ¸…æ´—
- ç»“æ„åŒ–åˆ†ææ–‡æ¡£å±‚çº§ã€æ®µè½ã€å­—ä½“ä¿¡æ¯
- AIæç¤ºè¯å‹å¥½è¾“å‡º
- æ”¯æŒå…³é”®è¯æœç´¢å’Œä¸Šä¸‹æ–‡æå–
"""

import os
import re
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import json

# æ–‡æ¡£å¤„ç†ç›¸å…³imports
try:
    from markitdown import MarkItDown
    MARKITDOWN_AVAILABLE = True
except ImportError as e:
    MarkItDown = None
    MARKITDOWN_AVAILABLE = False
    print(f"âš ï¸ markitdown æœªå®‰è£…æˆ–ç¼ºå°‘ä¾èµ–: {e}")
    print("ğŸ“ è§£å†³æ–¹æ¡ˆ: pip install markitdown[all]")

try:
    from docx import Document
    from docx.shared import Inches
    from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
    DOCX_AVAILABLE = True
except ImportError as e:
    Document = None
    DOCX_AVAILABLE = False
    print(f"âš ï¸ python-docx æœªå®‰è£…: {e}")
    print("ğŸ“ è§£å†³æ–¹æ¡ˆ: pip install python-docx")

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError as e:
    fitz = None
    PYMUPDF_AVAILABLE = False
    print(f"âš ï¸ PyMuPDF æœªå®‰è£…: {e}")
    print("ğŸ“ è§£å†³æ–¹æ¡ˆ: pip install pymupdf")

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
except ImportError as e:
    PyPDF2 = None
    PYPDF2_AVAILABLE = False
    print(f"âš ï¸ PyPDF2 æœªå®‰è£…: {e}")
    print("ğŸ“ è§£å†³æ–¹æ¡ˆ: pip install PyPDF2")

# å¯é€‰ä¾èµ–
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    pdfplumber = None
    PDFPLUMBER_AVAILABLE = False

try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
except ImportError:
    docx2txt = None
    DOCX2TXT_AVAILABLE = False

from PIL import Image
import io
import base64

class DocumentAnalyzer:
    """æ–‡æ¡£æ™ºèƒ½åˆ†æå™¨ - æ”¯æŒdocxã€pdfæ–‡æ¡£çš„æ·±åº¦åˆ†æ"""
    
    def __init__(self):
        self.document_path = None
        self.document_type = None
        self.analysis_result = {}
        self.markdown_converter = MarkItDown() if MarkItDown else None
        
        # æ£€æŸ¥ä¾èµ–å¯ç”¨æ€§
        self._check_dependencies()
    
    def _check_dependencies(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€æœ‰ä¾èµ–çš„å¯ç”¨æ€§"""
        dependencies = {
            'markitdown': MARKITDOWN_AVAILABLE,
            'python-docx': DOCX_AVAILABLE,
            'pymupdf': PYMUPDF_AVAILABLE,
            'pypdf2': PYPDF2_AVAILABLE,
            'pdfplumber': PDFPLUMBER_AVAILABLE,
            'docx2txt': DOCX2TXT_AVAILABLE
        }
        return dependencies
    
    def get_missing_dependencies(self) -> List[str]:
        """è·å–ç¼ºå¤±çš„å…³é”®ä¾èµ–"""
        deps = self._check_dependencies()
        missing = []
        
        if not deps['markitdown']:
            missing.append('markitdown[all]')
        if not deps['python-docx']:
            missing.append('python-docx')
        if not deps['pymupdf'] and not deps['pypdf2']:
            missing.append('pymupdf æˆ– PyPDF2')
        
        return missing
    
    def is_ready_for_analysis(self, file_type: str = None) -> Tuple[bool, List[str]]:
        """æ£€æŸ¥æ˜¯å¦å‡†å¤‡å¥½è¿›è¡Œåˆ†æ"""
        missing = self.get_missing_dependencies()
        
        if file_type == 'docx' and not DOCX_AVAILABLE:
            return False, ['python-docx']
        elif file_type == 'pdf' and not (PYMUPDF_AVAILABLE or PYPDF2_AVAILABLE):
            return False, ['pymupdf æˆ– PyPDF2']
        elif not MARKITDOWN_AVAILABLE:
            return False, ['markitdown[all]']
        
        return len(missing) == 0, missing
    
    def analyze_document(self, file_path: str) -> Dict[str, Any]:
        """
        å…¨é¢åˆ†ææ–‡æ¡£
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœå­—å…¸
        """
        self.document_path = file_path
        self.document_type = self._detect_document_type(file_path)
        
        if self.document_type not in ['docx', 'pdf']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {self.document_type}")
        
        # æ£€æŸ¥ä¾èµ–æ˜¯å¦æ»¡è¶³
        is_ready, missing_deps = self.is_ready_for_analysis(self.document_type)
        if not is_ready:
            raise ImportError(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {', '.join(missing_deps)}. è¯·è¿è¡Œ: pip install {' '.join(missing_deps)}")
        
        analysis = {
            'file_info': {
                'path': file_path,
                'name': Path(file_path).name,
                'type': self.document_type,
                'size_mb': round(os.path.getsize(file_path) / (1024 * 1024), 2)
            },
            'preview_data': {},
            'structure_analysis': {},
            'ai_analysis_data': {},
            'search_capabilities': {}
        }
        
        # 1. æ•°æ®é¢„è§ˆé˜¶æ®µ - ä½¿ç”¨markitdown
        analysis['preview_data'] = self._generate_preview_with_markitdown(file_path)
        
        # 2. ç»“æ„åŒ–åˆ†æ - ä»åŸå§‹æ–‡æ¡£æå–
        analysis['structure_analysis'] = self._analyze_document_structure(file_path)
        
        # 3. AIåˆ†ææ•°æ® - ä¸ºAIæä¾›ç»“æ„åŒ–ä¿¡æ¯
        analysis['ai_analysis_data'] = self._prepare_ai_analysis_data(analysis)
        
        # 4. æœç´¢åŠŸèƒ½å‡†å¤‡
        analysis['search_capabilities'] = self._prepare_search_capabilities(file_path)
        
        self.analysis_result = analysis
        return analysis
    
    def _detect_document_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡æ¡£ç±»å‹"""
        ext = Path(file_path).suffix.lower()
        if ext == '.docx':
            return 'docx'
        elif ext == '.pdf':
            return 'pdf'
        else:
            return 'unknown'
    
    def _generate_preview_with_markitdown(self, file_path: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨markitdownç”Ÿæˆé¢„è§ˆæ•°æ®ï¼ˆæ¸…é™¤æ ¼å¼ï¼Œä¿ç•™å›¾ç‰‡ï¼Œé™åˆ¶10é¡µï¼‰
        """
        preview_data = {
            'status': 'success',
            'content': '',
            'images': [],
            'page_count': 0,
            'word_count': 0,
            'error': None,
            'has_images': False,
            'images_preview': []
        }
        
        try:
            if not self.markdown_converter:
                preview_data['status'] = 'error'
                preview_data['error'] = 'MarkItDown æœªå¯ç”¨'
                return preview_data
            
            # ä½¿ç”¨markitdownè½¬æ¢
            result = self.markdown_converter.convert(file_path)
            content = result.text_content if hasattr(result, 'text_content') else str(result)
            
            # é™åˆ¶å†…å®¹é•¿åº¦ï¼ˆæ¨¡æ‹Ÿ10é¡µï¼‰
            max_chars = 20000  # çº¦10é¡µå†…å®¹
            if len(content) > max_chars:
                content = content[:max_chars] + "\n\n... [å†…å®¹è¢«æˆªæ–­ï¼Œè¶…è¿‡10é¡µé¢„è§ˆé™åˆ¶]"
            
            preview_data['content'] = content
            preview_data['word_count'] = len(content.split())
            
            # ä¼°ç®—é¡µæ•°ï¼ˆåŸºäºå­—ç¬¦æ•°ï¼‰
            chars_per_page = 2000
            preview_data['page_count'] = max(1, len(content) // chars_per_page)
            
            # æå–å›¾ç‰‡ä¿¡æ¯ç”¨äºé¢„è§ˆ
            images_info = self._extract_images_info(file_path)
            preview_data['images'] = [img['base64'] for img in images_info[:5]]  # å‰5å¼ å›¾ç‰‡çš„base64
            preview_data['images_preview'] = images_info[:5]  # å›¾ç‰‡è¯¦ç»†ä¿¡æ¯
            preview_data['has_images'] = len(images_info) > 0
            
        except Exception as e:
            preview_data['status'] = 'error'
            preview_data['error'] = f'MarkItDownå¤„ç†å¤±è´¥: {str(e)}'
            
            # å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
            try:
                fallback_content = self._fallback_content_extraction(file_path)
                if fallback_content:
                    preview_data['content'] = fallback_content[:20000]
                    preview_data['word_count'] = len(fallback_content.split())
                    preview_data['status'] = 'fallback'
                    preview_data['error'] = f'ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}'
            except:
                preview_data['content'] = 'å†…å®¹æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œä¾èµ–å®‰è£…'
        
        return preview_data
    
    def _fallback_content_extraction(self, file_path: str) -> str:
        """å¤‡ç”¨å†…å®¹æå–æ–¹æ¡ˆ"""
        content = ""
        
        try:
            if self.document_type == 'docx' and Document:
                doc = Document(file_path)
                content = "\n".join([para.text for para in doc.paragraphs])
            elif self.document_type == 'pdf' and fitz:
                doc = fitz.open(file_path)
                for page_num in range(min(10, len(doc))):  # é™åˆ¶10é¡µ
                    page = doc[page_num]
                    content += page.get_text() + "\n"
                doc.close()
        except:
            pass
        
        return content
    
    def _analyze_document_structure(self, file_path: str) -> Dict[str, Any]:
        """
        ä»åŸå§‹æ–‡æ¡£æå–ç»“æ„åŒ–ä¿¡æ¯
        ä¸ä½¿ç”¨markitdownçš„ç»“æœï¼Œç›´æ¥ä»æ–‡æ¡£æ ¼å¼ä¸­æå–
        """
        if self.document_type == 'docx':
            return self._analyze_docx_structure(file_path)
        elif self.document_type == 'pdf':
            return self._analyze_pdf_structure(file_path)
        else:
            return {'error': f'ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {self.document_type}'}
    
    def _analyze_docx_structure(self, file_path: str) -> Dict[str, Any]:
        """åˆ†æDOCXæ–‡æ¡£ç»“æ„"""
        try:
            if Document is None:
                return {'error': 'python-docxæœªå®‰è£…'}
            
            doc = Document(file_path)
            
            structure = {
                'total_paragraphs': len(doc.paragraphs),
                'headings': {},
                'fonts_used': set(),
                'paragraph_analysis': [],
                'tables_count': len(doc.tables),
                'images_count': 0,
                'page_breaks': 0,
                'images_info': [],
                'watermark_analysis': {}
            }
            
            # åˆ†ææ®µè½å’Œæ ‡é¢˜å±‚çº§
            heading_levels = {}
            for i, para in enumerate(doc.paragraphs):
                para_info = {
                    'index': i,
                    'text': para.text.strip()[:100],  # å‰100å­—ç¬¦
                    'style': para.style.name if para.style else 'Normal',
                    'alignment': str(para.alignment) if para.alignment else 'LEFT',
                    'is_heading': False,
                    'heading_level': 0
                }
                
                # æ£€æµ‹æ ‡é¢˜
                if para.style and 'Heading' in para.style.name:
                    para_info['is_heading'] = True
                    level_match = re.search(r'Heading (\d+)', para.style.name)
                    if level_match:
                        level = int(level_match.group(1))
                        para_info['heading_level'] = level
                        
                        if level not in heading_levels:
                            heading_levels[level] = []
                        heading_levels[level].append({
                            'text': para.text.strip(),
                            'index': i
                        })
                
                # åˆ†æå­—ä½“ä¿¡æ¯
                for run in para.runs:
                    if run.font.name:
                        structure['fonts_used'].add(run.font.name)
                
                structure['paragraph_analysis'].append(para_info)
            
            structure['headings'] = heading_levels
            structure['fonts_used'] = list(structure['fonts_used'])
            
            # æå–å’Œåˆ†æå›¾ç‰‡
            images_info = self._extract_images_info(file_path)
            structure['images_info'] = images_info
            structure['images_count'] = len(images_info)
            
            # æ°´å°åˆ†æ
            structure['watermark_analysis'] = self._analyze_watermarks(images_info)
            
            return structure
            
        except Exception as e:
            return {'error': f'DOCXç»“æ„åˆ†æå¤±è´¥: {str(e)}'}
    
    def _analyze_pdf_structure(self, file_path: str) -> Dict[str, Any]:
        """åˆ†æPDFæ–‡æ¡£ç»“æ„"""
        try:
            if fitz is None:
                return {'error': 'PyMuPDFæœªå®‰è£…'}
            
            doc = fitz.open(file_path)
            
            structure = {
                'total_pages': len(doc),
                'total_text_blocks': 0,
                'fonts_used': set(),
                'headings': {},
                'page_analysis': [],
                'images_count': 0,
                'tables_detected': 0,
                'images_info': [],
                'watermark_analysis': {}
            }
            
            heading_levels = {}
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                page_info = {
                    'page_number': page_num + 1,
                    'text_blocks': [],
                    'images_count': len(page.get_images()),
                    'fonts_on_page': set()
                }
                
                # åˆ†ææ–‡æœ¬å—
                blocks = page.get_text("dict")["blocks"]
                for block in blocks:
                    if "lines" in block:
                        for line in block["lines"]:
                            for span in line["spans"]:
                                font_info = f"{span.get('font', 'Unknown')}-{span.get('size', 0):.1f}"
                                structure['fonts_used'].add(font_info)
                                page_info['fonts_on_page'].add(font_info)
                                
                                text = span.get('text', '').strip()
                                if text:
                                    # ç®€å•çš„æ ‡é¢˜æ£€æµ‹ï¼ˆåŸºäºå­—ä½“å¤§å°ï¼‰
                                    font_size = span.get('size', 0)
                                    if font_size > 14 and len(text) < 200:  # å¯èƒ½æ˜¯æ ‡é¢˜
                                        level = self._estimate_heading_level(font_size)
                                        if level not in heading_levels:
                                            heading_levels[level] = []
                                        heading_levels[level].append({
                                            'text': text,
                                            'page': page_num + 1,
                                            'font_size': font_size
                                        })
                
                page_info['fonts_on_page'] = list(page_info['fonts_on_page'])
                structure['page_analysis'].append(page_info)
            
            structure['fonts_used'] = list(structure['fonts_used'])
            structure['headings'] = heading_levels
            
            # æå–å’Œåˆ†æå›¾ç‰‡
            doc.close()  # å…ˆå…³é—­å†é‡æ–°æ‰“å¼€è¿›è¡Œå›¾ç‰‡åˆ†æ
            images_info = self._extract_images_info(file_path)
            structure['images_info'] = images_info
            structure['images_count'] = len(images_info)
            
            # æ°´å°åˆ†æ
            structure['watermark_analysis'] = self._analyze_watermarks(images_info)
            
            return structure
            
        except Exception as e:
            return {'error': f'PDFç»“æ„åˆ†æå¤±è´¥: {str(e)}'}
    
    def _estimate_heading_level(self, font_size: float) -> int:
        """åŸºäºå­—ä½“å¤§å°ä¼°ç®—æ ‡é¢˜å±‚çº§"""
        if font_size >= 20:
            return 1
        elif font_size >= 16:
            return 2
        elif font_size >= 14:
            return 3
        else:
            return 4
    
    def _extract_images_info(self, file_path: str) -> List[Dict[str, Any]]:
        """æå–æ–‡æ¡£ä¸­çš„å›¾ç‰‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ°´å°æ£€æµ‹"""
        images_info = []
        
        try:
            if self.document_type == 'pdf' and fitz:
                doc = fitz.open(file_path)
                for page_num in range(min(10, len(doc))):  # å¤„ç†å‰10é¡µçš„å›¾ç‰‡
                    page = doc[page_num]
                    for img_index, img in enumerate(page.get_images()):
                        try:
                            xref = img[0]
                            pix = fitz.Pixmap(doc, xref)
                            if pix.n - pix.alpha < 4:  # ç¡®ä¿æ˜¯RGBå›¾åƒ
                                img_data = pix.tobytes("png")
                                img_base64 = base64.b64encode(img_data).decode()
                                
                                # æ°´å°æ£€æµ‹
                                watermark_result = self._detect_watermark_from_pixmap(pix)
                                
                                img_info = {
                                    'page': page_num + 1,
                                    'index': img_index,
                                    'base64': img_base64,
                                    'width': pix.width,
                                    'height': pix.height,
                                    'size_kb': len(img_data) / 1024,
                                    'format': 'PNG',
                                    'watermark_detected': watermark_result['has_watermark'],
                                    'watermark_confidence': watermark_result['confidence'],
                                    'watermark_type': watermark_result['type']
                                }
                                images_info.append(img_info)
                            pix = None
                        except Exception as e:
                            # è®°å½•å›¾ç‰‡å¤„ç†é”™è¯¯ï¼Œä½†ç»§ç»­å¤„ç†å…¶ä»–å›¾ç‰‡
                            print(f"å¤„ç†ç¬¬{page_num + 1}é¡µç¬¬{img_index}å¼ å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                            continue
                doc.close()
                
            elif self.document_type == 'docx' and Document:
                # DOCXå›¾ç‰‡å¤„ç†
                doc = Document(file_path)
                
                # ä»å…³ç³»ä¸­æå–å›¾ç‰‡
                img_index = 0
                for rel in doc.part.rels.values():
                    if "image" in rel.target_ref:
                        try:
                            img_data = rel.target_part.blob
                            
                            # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œåˆ†æ
                            img_pil = Image.open(io.BytesIO(img_data))
                            img_base64 = base64.b64encode(img_data).decode()
                            
                            # æ°´å°æ£€æµ‹
                            watermark_result = self._detect_watermark_from_pil(img_pil)
                            
                            img_info = {
                                'index': img_index,
                                'base64': img_base64,
                                'width': img_pil.width,
                                'height': img_pil.height,
                                'size_kb': len(img_data) / 1024,
                                'format': img_pil.format or 'Unknown',
                                'watermark_detected': watermark_result['has_watermark'],
                                'watermark_confidence': watermark_result['confidence'],
                                'watermark_type': watermark_result['type']
                            }
                            images_info.append(img_info)
                            img_index += 1
                            
                        except Exception as e:
                            print(f"å¤„ç†DOCXå›¾ç‰‡{img_index}æ—¶å‡ºé”™: {str(e)}")
                            continue
                        
        except Exception as e:
            print(f"å›¾ç‰‡æå–å¤±è´¥: {str(e)}")
        
        return images_info[:20]  # æœ€å¤šè¿”å›20å¼ å›¾ç‰‡
    
    def _detect_watermark_from_pixmap(self, pixmap) -> Dict[str, Any]:
        """ä»PyMuPDF Pixmapæ£€æµ‹æ°´å°"""
        try:
            # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œåˆ†æ
            img_data = pixmap.tobytes("png")
            img_pil = Image.open(io.BytesIO(img_data))
            return self._detect_watermark_from_pil(img_pil)
        except:
            return {'has_watermark': False, 'confidence': 0.0, 'type': 'unknown'}
    
    def _detect_watermark_from_pil(self, img_pil: Image.Image) -> Dict[str, Any]:
        """ä»PILå›¾åƒæ£€æµ‹æ°´å°"""
        try:
            import numpy as np
            
            # è½¬æ¢ä¸ºRGBæ¨¡å¼
            if img_pil.mode != 'RGB':
                img_pil = img_pil.convert('RGB')
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            img_array = np.array(img_pil)
            
            # æ°´å°æ£€æµ‹ç­–ç•¥
            result = {
                'has_watermark': False,
                'confidence': 0.0,
                'type': 'none'
            }
            
            # 1. æ£€æŸ¥é€æ˜åº¦ï¼ˆåŠé€æ˜æ°´å°ï¼‰
            if img_pil.mode in ['RGBA', 'LA']:
                alpha_channel = img_array[:, :, -1] if img_pil.mode == 'RGBA' else img_array[:, :, 1]
                alpha_variation = np.std(alpha_channel)
                if alpha_variation > 30:  # é€æ˜åº¦å˜åŒ–è¾ƒå¤§ï¼Œå¯èƒ½æœ‰æ°´å°
                    result['confidence'] += 0.3
                    result['type'] = 'transparent'
            
            # 2. æ£€æŸ¥é‡å¤æ¨¡å¼ï¼ˆé‡å¤æ°´å°ï¼‰
            gray = np.mean(img_array, axis=2) if len(img_array.shape) == 3 else img_array
            
            # ç®€å•çš„æ¨¡å¼æ£€æµ‹ï¼šæ£€æŸ¥å›¾åƒçš„æ–¹å·®
            variance = np.var(gray)
            if variance < 500:  # æ–¹å·®è¾ƒå°å¯èƒ½è¡¨ç¤ºæœ‰é‡å¤æ¨¡å¼
                result['confidence'] += 0.2
                if result['type'] == 'none':
                    result['type'] = 'pattern'
            
            # 3. æ£€æŸ¥è¾¹ç¼˜åŒºåŸŸï¼ˆè¾¹æ¡†æ°´å°ï¼‰
            height, width = gray.shape
            border_width = min(height, width) // 20
            
            if border_width > 5:
                top_border = gray[:border_width, :]
                bottom_border = gray[-border_width:, :]
                left_border = gray[:, :border_width]
                right_border = gray[:, -border_width:]
                
                # æ£€æŸ¥è¾¹ç¼˜æ˜¯å¦æœ‰ç‰¹æ®Šæ¨¡å¼
                borders = [top_border, bottom_border, left_border, right_border]
                for border in borders:
                    if np.std(border) > 20:  # è¾¹ç¼˜æœ‰å˜åŒ–
                        result['confidence'] += 0.1
                        if result['type'] == 'none':
                            result['type'] = 'border'
            
            # 4. æ£€æŸ¥ä¸­å¿ƒåŒºåŸŸï¼ˆä¸­å¿ƒæ°´å°ï¼‰
            center_h, center_w = height // 2, width // 2
            quarter_h, quarter_w = height // 4, width // 4
            
            center_region = gray[center_h-quarter_h:center_h+quarter_h, 
                               center_w-quarter_w:center_w+quarter_w]
            
            if center_region.size > 0:
                center_std = np.std(center_region)
                overall_std = np.std(gray)
                
                # å¦‚æœä¸­å¿ƒåŒºåŸŸçš„å˜åŒ–ä¸æ•´ä½“ä¸åŒï¼Œå¯èƒ½æœ‰æ°´å°
                if abs(center_std - overall_std) > 15:
                    result['confidence'] += 0.2
                    if result['type'] == 'none':
                        result['type'] = 'center'
            
            # 5. æ£€æŸ¥é¢œè‰²åˆ†å¸ƒï¼ˆå½©è‰²æ°´å°ï¼‰
            if len(img_array.shape) == 3:
                # è®¡ç®—å„é¢œè‰²é€šé“çš„åˆ†å¸ƒ
                r_std = np.std(img_array[:, :, 0])
                g_std = np.std(img_array[:, :, 1])
                b_std = np.std(img_array[:, :, 2])
                
                # å¦‚æœæŸä¸ªé€šé“ç‰¹åˆ«çªå‡ºï¼Œå¯èƒ½æœ‰å½©è‰²æ°´å°
                max_std = max(r_std, g_std, b_std)
                min_std = min(r_std, g_std, b_std)
                
                if max_std / (min_std + 1) > 2:  # é¢œè‰²åˆ†å¸ƒä¸å‡
                    result['confidence'] += 0.15
                    if result['type'] == 'none':
                        result['type'] = 'colored'
            
            # æœ€ç»ˆåˆ¤æ–­
            if result['confidence'] > 0.3:
                result['has_watermark'] = True
            
            # é™åˆ¶ç½®ä¿¡åº¦åœ¨0-1ä¹‹é—´
            result['confidence'] = min(1.0, result['confidence'])
            
            return result
            
        except Exception as e:
            print(f"æ°´å°æ£€æµ‹å¤±è´¥: {str(e)}")
            return {'has_watermark': False, 'confidence': 0.0, 'type': 'error'}
    
    def _analyze_watermarks(self, images_info: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªæ–‡æ¡£çš„æ°´å°æƒ…å†µ"""
        if not images_info:
            return {'has_watermark': False, 'total_images': 0}
        
        watermark_count = sum(1 for img in images_info if img.get('watermark_detected', False))
        total_images = len(images_info)
        
        # æ”¶é›†æ°´å°ç±»å‹
        watermark_types = []
        total_confidence = 0
        
        for img in images_info:
            if img.get('watermark_detected', False):
                watermark_types.append(img.get('watermark_type', 'unknown'))
                total_confidence += img.get('watermark_confidence', 0)
        
        # åˆ¤æ–­æ•´ä¸ªæ–‡æ¡£æ˜¯å¦æœ‰æ°´å°
        has_watermark = watermark_count > 0
        avg_confidence = total_confidence / max(watermark_count, 1)
        
        # ç¡®å®šä¸»è¦æ°´å°ç±»å‹
        if watermark_types:
            main_type = max(set(watermark_types), key=watermark_types.count)
        else:
            main_type = 'none'
        
        return {
            'has_watermark': has_watermark,
            'total_images': total_images,
            'watermark_images': watermark_count,
            'watermark_ratio': watermark_count / total_images if total_images > 0 else 0,
            'confidence': avg_confidence,
            'watermark_type': main_type,
            'watermark_types': list(set(watermark_types))
        }
    
    def _prepare_ai_analysis_data(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸ºAIåˆ†æå‡†å¤‡ç»“æ„åŒ–æ•°æ®"""
        ai_data = {
            'document_summary': {},
            'structure_insights': {},
            'content_themes': [],
            'analysis_suggestions': []
        }
        
        # æ–‡æ¡£æ‘˜è¦
        file_info = analysis['file_info']
        structure = analysis['structure_analysis']
        
        ai_data['document_summary'] = {
            'name': file_info['name'],
            'type': file_info['type'],
            'size_mb': file_info['size_mb'],
            'estimated_pages': structure.get('total_pages', analysis['preview_data'].get('page_count', 0)),
            'word_count': analysis['preview_data'].get('word_count', 0)
        }
        
        # ç»“æ„æ´å¯Ÿ
        if 'headings' in structure:
            heading_summary = {}
            for level, headings in structure['headings'].items():
                heading_summary[f'level_{level}'] = {
                    'count': len(headings),
                    'examples': [h['text'][:50] for h in headings[:3]]
                }
            ai_data['structure_insights']['headings'] = heading_summary
        
        if 'fonts_used' in structure:
            ai_data['structure_insights']['fonts_count'] = len(structure['fonts_used'])
            ai_data['structure_insights']['main_fonts'] = structure['fonts_used'][:5]
        
        # åˆ†æå»ºè®®
        ai_data['analysis_suggestions'] = [
            "æ–‡æ¡£ç”¨é€”åˆ†æï¼šåŸºäºæ ‡é¢˜ç»“æ„å’Œå†…å®¹åˆ¤æ–­æ–‡æ¡£ç±»å‹ï¼ˆåˆåŒã€æŠ¥å‘Šã€æ‰‹å†Œç­‰ï¼‰",
            "å…³é”®ä¿¡æ¯æå–ï¼šè¯†åˆ«é‡è¦çš„æ•°æ®ç‚¹ã€æ—¥æœŸã€é‡‘é¢ã€äººåç­‰",
            "ç»“æ„åŒ–å†…å®¹æå–ï¼šæŒ‰ç« èŠ‚ç»„ç»‡å†…å®¹ï¼Œä¾¿äºåç»­åˆ†æ",
            "å…³é”®è¯æœç´¢ï¼šæ”¯æŒç²¾ç¡®æœç´¢å’Œä¸Šä¸‹æ–‡æå–"
        ]
        
        return ai_data
    
    def _prepare_search_capabilities(self, file_path: str) -> Dict[str, Any]:
        """å‡†å¤‡æœç´¢åŠŸèƒ½"""
        search_data = {
            'indexed_content': {},
            'search_ready': False
        }
        
        try:
            # æå–å…¨æ–‡å†…å®¹ç”¨äºæœç´¢
            full_text = ""
            
            if self.document_type == 'docx' and Document:
                doc = Document(file_path)
                full_text = "\n".join([para.text for para in doc.paragraphs])
            elif self.document_type == 'pdf' and fitz:
                doc = fitz.open(file_path)
                full_text = ""
                for page in doc:
                    full_text += page.get_text() + "\n"
                doc.close()
            
            if full_text:
                search_data['indexed_content'] = {
                    'full_text': full_text,
                    'paragraph_count': len(full_text.split('\n')),
                    'char_count': len(full_text)
                }
                search_data['search_ready'] = True
            
        except Exception as e:
            search_data['error'] = f"æœç´¢ç´¢å¼•å‡†å¤‡å¤±è´¥: {str(e)}"
        
        return search_data
    
    def get_page_count(self, file_path: str = None) -> int:
        """
        è·å–æ–‡æ¡£é¡µæ•°
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰åˆ†æçš„æ–‡æ¡£
            
        Returns:
            æ–‡æ¡£é¡µæ•°
        """
        if file_path is None:
            file_path = self.document_path
        
        if not file_path or not os.path.exists(file_path):
            return 0
            
        doc_type = self._detect_document_type(file_path)
        
        if doc_type == 'pdf':
            try:
                if PYMUPDF_AVAILABLE:
                    doc = fitz.open(file_path)
                    page_count = len(doc)
                    doc.close()
                    return page_count
                elif PYPDF2_AVAILABLE:
                    with open(file_path, 'rb') as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        return len(pdf_reader.pages)
            except Exception as e:
                print(f"âš ï¸ è·å–PDFé¡µæ•°å¤±è´¥: {e}")
                return 0
        
        elif doc_type == 'docx':
            try:
                if DOCX_AVAILABLE:
                    # DOCXæ²¡æœ‰æ˜ç¡®çš„"é¡µæ•°"æ¦‚å¿µï¼Œè¿™é‡Œä¼°ç®—
                    doc = Document(file_path)
                    paragraph_count = len(doc.paragraphs)
                    # ç²—ç•¥ä¼°ç®—ï¼šæ¯é¡µçº¦15-20ä¸ªæ®µè½
                    estimated_pages = max(1, paragraph_count // 18)
                    return estimated_pages
            except Exception as e:
                print(f"âš ï¸ è·å–DOCXé¡µæ•°å¤±è´¥: {e}")
                return 0
        
        return 0
    
    def analyze_structure(self, file_path: str = None) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£ç»“æ„ï¼ˆä¸ºäº†å…¼å®¹AIç”Ÿæˆçš„ä»£ç ï¼‰
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰åˆ†æçš„æ–‡æ¡£
            
        Returns:
            æ–‡æ¡£ç»“æ„åˆ†æç»“æœ
        """
        if file_path is None:
            file_path = self.document_path
            
        if not file_path or not os.path.exists(file_path):
            return {}
            
        try:
            return self._analyze_document_structure(file_path)
        except Exception as e:
            print(f"âš ï¸ ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {}
    
    def get_document_info(self, file_path: str = None) -> Dict[str, Any]:
        """
        è·å–æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨å½“å‰åˆ†æçš„æ–‡æ¡£
            
        Returns:
            æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
        """
        if file_path is None:
            file_path = self.document_path
            
        if not file_path or not os.path.exists(file_path):
            return {}
        
        return {
            'name': Path(file_path).name,
            'type': self._detect_document_type(file_path),
            'size_mb': round(os.path.getsize(file_path) / (1024 * 1024), 2),
            'page_count': self.get_page_count(file_path)
        }

    def search_keyword_context(self, keyword: str, context_lines: int = 3) -> List[Dict[str, Any]]:
        """
        æœç´¢å…³é”®è¯å¹¶è¿”å›ä¸Šä¸‹æ–‡
        
        Args:
            keyword: è¦æœç´¢çš„å…³é”®è¯
            context_lines: ä¸Šä¸‹æ–‡è¡Œæ•°
            
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œä¸Šä¸‹æ–‡çš„åˆ—è¡¨
        """
        if not self.analysis_result or 'search_capabilities' not in self.analysis_result:
            return []
        
        search_data = self.analysis_result['search_capabilities']
        if not search_data.get('search_ready'):
            return []
        
        full_text = search_data['indexed_content']['full_text']
        lines = full_text.split('\n')
        
        results = []
        for i, line in enumerate(lines):
            if keyword.lower() in line.lower():
                # æå–ä¸Šä¸‹æ–‡
                start_idx = max(0, i - context_lines)
                end_idx = min(len(lines), i + context_lines + 1)
                
                context = '\n'.join(lines[start_idx:end_idx])
                
                results.append({
                    'line_number': i + 1,
                    'matched_line': line.strip(),
                    'context': context,
                    'keyword_position': line.lower().find(keyword.lower())
                })
        
        return results
    
    def generate_analysis_code(self, task_description: str) -> str:
        """
        æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆæ™ºèƒ½åˆ†æä»£ç 
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            
        Returns:
            ç”Ÿæˆçš„Pythonä»£ç 
        """
        if not self.analysis_result:
            return "# è¯·å…ˆåˆ†ææ–‡æ¡£"
        
        # åŸºç¡€ä»£ç æ¨¡æ¿
        code_template = f'''
# æ–‡æ¡£æ™ºèƒ½åˆ†æä»£ç 
# æ–‡æ¡£: {self.analysis_result["file_info"]["name"]}
# ä»»åŠ¡: {task_description}

from document_analyzer import DocumentAnalyzer
import json

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = DocumentAnalyzer()

# åˆ†ææ–‡æ¡£
file_path = "{self.document_path}"
analysis_result = analyzer.analyze_document(file_path)

# æ‰“å°åŸºæœ¬ä¿¡æ¯
print("=== æ–‡æ¡£åŸºæœ¬ä¿¡æ¯ ===")
file_info = analysis_result["file_info"]
print(f"æ–‡ä»¶å: {{file_info['name']}}")
print(f"ç±»å‹: {{file_info['type']}}")
print(f"å¤§å°: {{file_info['size_mb']}} MB")

# ç»“æ„åˆ†æ
print("\\n=== æ–‡æ¡£ç»“æ„åˆ†æ ===")
structure = analysis_result["structure_analysis"]
'''
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ·»åŠ ç‰¹å®šä»£ç 
        if "å…³é”®è¯" in task_description or "æœç´¢" in task_description:
            # æå–å¯èƒ½çš„å…³é”®è¯
            potential_keywords = self._extract_potential_keywords(task_description)
            
            code_template += f'''
# å…³é”®è¯æœç´¢åˆ†æ
keywords = {potential_keywords}

print("\\n=== å…³é”®è¯æœç´¢ç»“æœ ===")
for keyword in keywords:
    print(f"\\næœç´¢å…³é”®è¯: {{keyword}}")
    results = analyzer.search_keyword_context(keyword, context_lines=2)
    if results:
        for i, result in enumerate(results[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
            print(f"  ç»“æœ {{i}}:")
            print(f"    è¡Œå·: {{result['line_number']}}")
            print(f"    åŒ¹é…è¡Œ: {{result['matched_line'][:100]}}")
            print(f"    ä¸Šä¸‹æ–‡:")
            print("    " + "\\n    ".join(result['context'].split('\\n')))
            print("-" * 50)
    else:
        print(f"  æœªæ‰¾åˆ°å…³é”®è¯: {{keyword}}")
'''
        
        if "æ ‡é¢˜" in task_description or "ç»“æ„" in task_description:
            code_template += '''
# æ ‡é¢˜ç»“æ„åˆ†æ
if "headings" in structure:
    print("\\n=== æ ‡é¢˜å±‚çº§ç»“æ„ ===")
    headings = structure["headings"]
    for level in sorted(headings.keys()):
        print(f"\\n{level}çº§æ ‡é¢˜ (å…±{len(headings[level])}ä¸ª):")
        for heading in headings[level][:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {heading.get('text', heading)}")
'''
        
        if "å­—ä½“" in task_description or "æ ¼å¼" in task_description:
            code_template += '''
# å­—ä½“ä½¿ç”¨åˆ†æ
if "fonts_used" in structure:
    print("\\n=== å­—ä½“ä½¿ç”¨åˆ†æ ===")
    fonts = structure["fonts_used"]
    print(f"å…±ä½¿ç”¨äº† {len(fonts)} ç§å­—ä½“:")
    for font in fonts[:10]:  # æ˜¾ç¤ºå‰10ç§å­—ä½“
        print(f"  - {font}")
'''
        
        code_template += '''
# ä¿å­˜åˆ†æç»“æœ
output_file = f"{file_path}_analysis_result.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(analysis_result, f, ensure_ascii=False, indent=2)

print(f"\\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
'''
        
        return code_template
    
    def _extract_potential_keywords(self, task_description: str) -> List[str]:
        """ä»ä»»åŠ¡æè¿°ä¸­æå–å¯èƒ½çš„å…³é”®è¯"""
        # é¢„å®šä¹‰çš„å¸¸è§å…³é”®è¯
        common_keywords = [
            "åˆåŒç¼–å·", "åˆåŒå·", "åè®®ç¼–å·", "æ–‡æ¡£ç¼–å·",
            "ç”²æ–¹", "ä¹™æ–¹", "å§”æ‰˜æ–¹", "å—æ‰˜æ–¹",
            "é‡‘é¢", "è´¹ç”¨", "ä»·æ ¼", "æ€»ä»·", "å•ä»·",
            "æ—¥æœŸ", "æ—¶é—´", "æœŸé™", "æœ‰æ•ˆæœŸ",
            "è´£ä»»", "ä¹‰åŠ¡", "æƒåˆ©", "æ¡æ¬¾",
            "ç­¾å­—", "ç›–ç« ", "ç¡®è®¤", "æ‰¹å‡†"
        ]
        
        # ä»ä»»åŠ¡æè¿°ä¸­æå–å¼•å·å†…çš„å†…å®¹
        quoted_keywords = re.findall(r'"([^"]*)"', task_description)
        quoted_keywords.extend(re.findall(r"'([^']*)'", task_description))
        quoted_keywords.extend(re.findall(r"ã€Œ([^ã€]*)ã€", task_description))
        
        # åˆå¹¶å…³é”®è¯
        all_keywords = quoted_keywords + common_keywords[:5]  # å–å‰5ä¸ªå¸¸è§å…³é”®è¯
        
        return list(set(all_keywords))[:10]  # å»é‡å¹¶é™åˆ¶åœ¨10ä¸ªä»¥å†…

def main():
    """æµ‹è¯•å‡½æ•°"""
    analyzer = DocumentAnalyzer()
    
    # æµ‹è¯•æ–‡æ¡£åˆ†æ
    test_file = "test_document.docx"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    if os.path.exists(test_file):
        result = analyzer.analyze_document(test_file)
        print("æ–‡æ¡£åˆ†æå®Œæˆï¼š")
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        print("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")

if __name__ == "__main__":
    main() 