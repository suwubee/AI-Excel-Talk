#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡æ¡£å¤„ç†å·¥å…·ç±» - æ”¯æŒdocxã€pdfæ–‡æ¡£çš„é«˜çº§å¤„ç†
ç±»ä¼¼äºexcel_utils.pyï¼Œä¸ºæ–‡æ¡£åˆ†ææä¾›å·¥å…·æ”¯æŒ
"""

import os
import json
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

# æ–‡æ¡£åˆ†æå™¨
from document_analyzer import DocumentAnalyzer

class AdvancedDocumentProcessor:
    """é«˜çº§æ–‡æ¡£å¤„ç†å™¨ - æä¾›æ–‡æ¡£åŠ è½½ã€åˆ†æã€å¤„ç†çš„å®Œæ•´åŠŸèƒ½"""
    
    def __init__(self):
        self.analyzer = DocumentAnalyzer()
        self.current_document = None
        self.analysis_result = None
        self.supported_formats = ['.docx', '.pdf']
    
    def load_document(self, file_path: str) -> Dict[str, Any]:
        """
        åŠ è½½æ–‡æ¡£å¹¶è¿›è¡Œåˆæ­¥åˆ†æ
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡æ¡£åˆ†æç»“æœ
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}, æ”¯æŒçš„æ ¼å¼: {self.supported_formats}")
        
        try:
            # ä½¿ç”¨æ–‡æ¡£åˆ†æå™¨è¿›è¡Œå…¨é¢åˆ†æ
            self.analysis_result = self.analyzer.analyze_document(file_path)
            self.current_document = file_path
            
            return self.analysis_result
            
        except Exception as e:
            raise Exception(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
    
    def get_document_preview(self, max_chars: int = 10000) -> str:
        """
        è·å–æ–‡æ¡£é¢„è§ˆå†…å®¹
        
        Args:
            max_chars: æœ€å¤§å­—ç¬¦æ•°
            
        Returns:
            é¢„è§ˆå†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        if not self.analysis_result:
            return "è¯·å…ˆåŠ è½½æ–‡æ¡£"
        
        preview_data = self.analysis_result.get('preview_data', {})
        if preview_data.get('status') == 'error':
            return f"é¢„è§ˆç”Ÿæˆå¤±è´¥: {preview_data.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        content = preview_data.get('content', '')
        if len(content) > max_chars:
            content = content[:max_chars] + "\n\n... (å†…å®¹å·²æˆªæ–­)"
        
        return content
    
    def get_structure_summary(self) -> str:
        """
        è·å–æ–‡æ¡£ç»“æ„æ‘˜è¦
        
        Returns:
            ç»“æ„åŒ–çš„æ–‡æ¡£æ‘˜è¦ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        if not self.analysis_result:
            return "è¯·å…ˆåŠ è½½æ–‡æ¡£"
        
        file_info = self.analysis_result.get('file_info', {})
        structure = self.analysis_result.get('structure_analysis', {})
        
        summary_lines = []
        summary_lines.append(f"# ğŸ“„ æ–‡æ¡£ç»“æ„åˆ†æ")
        summary_lines.append(f"**æ–‡ä»¶å**: {file_info.get('name', 'Unknown')}")
        summary_lines.append(f"**ç±»å‹**: {file_info.get('type', 'Unknown').upper()}")
        summary_lines.append(f"**å¤§å°**: {file_info.get('size_mb', 0)} MB")
        summary_lines.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        if file_info.get('type') == 'docx':
            summary_lines.append("## ğŸ“Š æ–‡æ¡£ç»Ÿè®¡")
            summary_lines.append(f"- **æ®µè½æ•°**: {structure.get('total_paragraphs', 0)}")
            summary_lines.append(f"- **è¡¨æ ¼æ•°**: {structure.get('tables_count', 0)}")
            summary_lines.append(f"- **å›¾ç‰‡æ•°**: {structure.get('images_count', 0)}")
            summary_lines.append(f"- **å­—ä½“ç§ç±»**: {len(structure.get('fonts_used', []))}")
            
        elif file_info.get('type') == 'pdf':
            summary_lines.append("## ğŸ“Š æ–‡æ¡£ç»Ÿè®¡")
            summary_lines.append(f"- **é¡µæ•°**: {structure.get('total_pages', 0)}")
            summary_lines.append(f"- **å›¾ç‰‡æ•°**: {structure.get('images_count', 0)}")
            summary_lines.append(f"- **å­—ä½“ç§ç±»**: {len(structure.get('fonts_used', []))}")
        
        # æ ‡é¢˜ç»“æ„ - 100ä¸ªä»¥å†…å…¨éƒ¨æ˜¾ç¤º
        headings = structure.get('headings', {})
        if headings:
            summary_lines.append("")
            summary_lines.append("## ğŸ·ï¸ æ ‡é¢˜å±‚çº§ç»“æ„")
            for level in sorted(headings.keys()):
                heading_list = headings[level]
                summary_lines.append(f"### {level}çº§æ ‡é¢˜ (å…±{len(heading_list)}ä¸ª)")
                
                # å¦‚æœæ ‡é¢˜æ•°é‡è¶…è¿‡100ä¸ªï¼Œæ˜¾ç¤ºå‰100ä¸ªå¹¶æç¤º
                if len(heading_list) > 100:
                    display_headings = heading_list[:100]
                    for heading in display_headings:
                        text = heading.get('text', str(heading))[:80]  # å¢åŠ æ˜¾ç¤ºé•¿åº¦
                        summary_lines.append(f"- {text}")
                    summary_lines.append(f"- ... è¿˜æœ‰{len(heading_list) - 100}ä¸ª{level}çº§æ ‡é¢˜ï¼ˆè¶…è¿‡100ä¸ªé™åˆ¶ï¼‰")
                else:
                    # 100ä¸ªä»¥å†…å…¨éƒ¨æ˜¾ç¤º
                    for heading in heading_list:
                        text = heading.get('text', str(heading))[:80]  # å¢åŠ æ˜¾ç¤ºé•¿åº¦
                        summary_lines.append(f"- {text}")
        
        # å­—ä½“ä¿¡æ¯ - 100ä¸ªä»¥å†…å…¨éƒ¨æ˜¾ç¤º
        fonts = structure.get('fonts_used', [])
        if fonts:
            summary_lines.append("")
            summary_lines.append("## ğŸ”¤ å­—ä½“ä½¿ç”¨æƒ…å†µ")
            
            # å¦‚æœå­—ä½“æ•°é‡è¶…è¿‡100ä¸ªï¼Œæ˜¾ç¤ºå‰100ä¸ªå¹¶æç¤º
            if len(fonts) > 100:
                display_fonts = fonts[:100]
                for font in display_fonts:
                    summary_lines.append(f"- {font}")
                summary_lines.append(f"- ... è¿˜æœ‰{len(fonts) - 100}ç§å­—ä½“ï¼ˆè¶…è¿‡100ä¸ªé™åˆ¶ï¼‰")
            else:
                # 100ä¸ªä»¥å†…å…¨éƒ¨æ˜¾ç¤º
                for font in fonts:
                    summary_lines.append(f"- {font}")
        
        # å›¾ç‰‡åˆ†æä¿¡æ¯
        images_info = structure.get('images_info', [])
        if images_info:
            summary_lines.append("")
            summary_lines.append("## ğŸ–¼ï¸ å›¾ç‰‡åˆ†æ")
            summary_lines.append(f"- **å›¾ç‰‡æ€»æ•°**: {len(images_info)}")
            for i, img_info in enumerate(images_info[:10], 1):  # æ˜¾ç¤ºå‰10å¼ å›¾ç‰‡çš„è¯¦ç»†ä¿¡æ¯
                summary_lines.append(f"- **å›¾ç‰‡{i}**: {img_info.get('width', 0)}x{img_info.get('height', 0)}px")
                if 'page' in img_info:
                    summary_lines.append(f"  - ä½ç½®: ç¬¬{img_info['page']}é¡µ")
                if 'watermark_detected' in img_info:
                    if img_info['watermark_detected']:
                        summary_lines.append(f"  - âš ï¸ æ£€æµ‹åˆ°å¯èƒ½çš„æ°´å°")
                    else:
                        summary_lines.append(f"  - âœ… æœªæ£€æµ‹åˆ°æ°´å°")
        
        # æ°´å°æ£€æµ‹æ€»ç»“
        watermark_summary = structure.get('watermark_analysis', {})
        if watermark_summary:
            summary_lines.append("")
            summary_lines.append("## ğŸ” æ°´å°æ£€æµ‹")
            if watermark_summary.get('has_watermark', False):
                summary_lines.append("- âš ï¸ **æ£€æµ‹åˆ°å¯èƒ½çš„æ°´å°**")
                summary_lines.append(f"- æ°´å°ç±»å‹: {watermark_summary.get('watermark_type', 'æœªçŸ¥')}")
                summary_lines.append(f"- æ£€æµ‹ç½®ä¿¡åº¦: {watermark_summary.get('confidence', 0):.2f}")
            else:
                summary_lines.append("- âœ… **æœªæ£€æµ‹åˆ°æ˜æ˜¾æ°´å°**")
        
        return "\n".join(summary_lines)
    
    def search_content(self, keyword: str, context_lines: int = 3) -> List[Dict[str, Any]]:
        """
        åœ¨æ–‡æ¡£ä¸­æœç´¢å…³é”®è¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            context_lines: ä¸Šä¸‹æ–‡è¡Œæ•°
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.analysis_result:
            return []
        
        return self.analyzer.search_keyword_context(keyword, context_lines)
    
    def generate_analysis_code(self, task_description: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£åˆ†æä»£ç 
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            
        Returns:
            ç”Ÿæˆçš„Pythonä»£ç 
        """
        if not self.analysis_result:
            return "# è¯·å…ˆåŠ è½½æ–‡æ¡£"
        
        return self.analyzer.generate_analysis_code(task_description)
    
    def export_analysis_result(self, output_dir: str = None) -> Tuple[str, str]:
        """
        å¯¼å‡ºåˆ†æç»“æœ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            (JSONæ–‡ä»¶è·¯å¾„, MarkdownæŠ¥å‘Šè·¯å¾„)
        """
        if not self.analysis_result:
            raise ValueError("æ²¡æœ‰åˆ†æç»“æœå¯å¯¼å‡º")
        
        if output_dir is None:
            output_dir = tempfile.gettempdir()
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        file_name = Path(self.current_document).stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¯¼å‡ºJSONæ•°æ®
        json_file = output_dir / f"{file_name}_analysis_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_result, f, ensure_ascii=False, indent=2, default=str)
        
        # å¯¼å‡ºMarkdownæŠ¥å‘Š
        md_file = output_dir / f"{file_name}_report_{timestamp}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_markdown_report())
        
        return str(json_file), str(md_file)
    
    def _generate_markdown_report(self) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        if not self.analysis_result:
            return "# åˆ†ææŠ¥å‘Š\n\næ— æ•°æ®"
        
        file_info = self.analysis_result.get('file_info', {})
        preview_data = self.analysis_result.get('preview_data', {})
        structure = self.analysis_result.get('structure_analysis', {})
        ai_data = self.analysis_result.get('ai_analysis_data', {})
        
        report_lines = []
        
        # æŠ¥å‘Šå¤´éƒ¨
        report_lines.extend([
            f"# ğŸ“„ æ–‡æ¡£æ™ºèƒ½åˆ†ææŠ¥å‘Š",
            f"",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**æ–‡æ¡£åç§°**: {file_info.get('name', 'Unknown')}",
            f"**æ–‡æ¡£ç±»å‹**: {file_info.get('type', 'Unknown').upper()}",
            f"**æ–‡ä»¶å¤§å°**: {file_info.get('size_mb', 0)} MB",
            f"",
            f"---",
            f""
        ])
        
        # æ–‡æ¡£æ¦‚è¦
        report_lines.extend([
            f"## ğŸ“Š æ–‡æ¡£æ¦‚è¦",
            f"",
            f"### åŸºæœ¬ä¿¡æ¯"
        ])
        
        doc_summary = ai_data.get('document_summary', {})
        if doc_summary:
            report_lines.extend([
                f"- **ä¼°ç®—é¡µæ•°**: {doc_summary.get('estimated_pages', 'Unknown')}",
                f"- **å­—æ•°ç»Ÿè®¡**: {doc_summary.get('word_count', 0)} è¯",
                f"- **æ–‡æ¡£è§„æ¨¡**: {'å¤§å‹æ–‡æ¡£' if doc_summary.get('estimated_pages', 0) > 20 else 'ä¸­å°å‹æ–‡æ¡£'}",
                f""
            ])
        
        # ç»“æ„åˆ†æ
        structure_insights = ai_data.get('structure_insights', {})
        if structure_insights:
            report_lines.extend([
                f"### ç»“æ„ç‰¹å¾",
                f"- **å­—ä½“ç§ç±»**: {structure_insights.get('fonts_count', 0)} ç§",
                f"- **ä¸»è¦å­—ä½“**: {', '.join(structure_insights.get('main_fonts', [])[:3])}",
                f""
            ])
            
            headings_info = structure_insights.get('headings', {})
            if headings_info:
                report_lines.append(f"### æ ‡é¢˜å±‚çº§")
                for level, info in headings_info.items():
                    report_lines.append(f"- **{level}**: {info['count']}ä¸ª")
                    examples = info.get('examples', [])
                    if examples:
                        report_lines.append(f"  - ç¤ºä¾‹: {' | '.join(examples)}")
                report_lines.append("")
        
        # å†…å®¹é¢„è§ˆ
        if preview_data.get('status') == 'success':
            content = preview_data.get('content', '')
            if content:
                report_lines.extend([
                    f"## ğŸ“ å†…å®¹é¢„è§ˆ",
                    f"",
                    f"### æ–‡æ¡£å†…å®¹ï¼ˆå‰{min(1000, len(content))}å­—ç¬¦ï¼‰",
                    f"```",
                    content[:1000],
                    f"```",
                    f"",
                    f"---",
                    f""
                ])
        
        # AIåˆ†æå»ºè®®
        suggestions = ai_data.get('analysis_suggestions', [])
        if suggestions:
            report_lines.extend([
                f"## ğŸ¯ AIåˆ†æå»ºè®®",
                f""
            ])
            for i, suggestion in enumerate(suggestions, 1):
                report_lines.append(f"{i}. {suggestion}")
            report_lines.append("")
        
        # æŠ€æœ¯ä¿¡æ¯
        report_lines.extend([
            f"## ğŸ”§ æŠ€æœ¯ä¿¡æ¯",
            f"",
            f"### åˆ†ææ–¹æ³•",
            f"- **é¢„è§ˆç”Ÿæˆ**: MarkItDown (æ¸…é™¤æ ¼å¼ï¼Œä¿ç•™ç»“æ„)",
            f"- **ç»“æ„åˆ†æ**: åŸç”Ÿæ–‡æ¡£æ ¼å¼è§£æ",
            f"- **æœç´¢åŠŸèƒ½**: å…¨æ–‡ç´¢å¼•",
            f"- **ä»£ç ç”Ÿæˆ**: æ™ºèƒ½æ¨¡æ¿",
            f"",
            f"### åˆ†æè¦†ç›–",
            f"- âœ… æ–‡æ¡£åŸºæœ¬ä¿¡æ¯",
            f"- âœ… æ ‡é¢˜å±‚çº§ç»“æ„", 
            f"- âœ… å­—ä½“ä½¿ç”¨æƒ…å†µ",
            f"- âœ… å›¾ç‰‡æ•°é‡ç»Ÿè®¡",
            f"- âœ… å…³é”®è¯æœç´¢æ”¯æŒ",
            f"",
            f"---",
            f"",
            f"*æŠ¥å‘Šç”±AI Excelæ™ºèƒ½åˆ†æå·¥å…· - æ–‡æ¡£åˆ†ææ¨¡å—ç”Ÿæˆ*"
        ])
        
        return "\n".join(report_lines)
    
    def get_analysis_cache(self) -> Optional[Dict[str, Any]]:
        """è·å–åˆ†æç»“æœç¼“å­˜"""
        return self.analysis_result
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.analysis_result = None
        self.current_document = None

class DocumentSearchEngine:
    """æ–‡æ¡£æœç´¢å¼•æ“ - ä¸“é—¨å¤„ç†å…³é”®è¯æœç´¢å’Œä¸Šä¸‹æ–‡æå–"""
    
    def __init__(self, document_processor: AdvancedDocumentProcessor):
        self.processor = document_processor
    
    def search_multiple_keywords(self, keywords: List[str], context_lines: int = 3) -> Dict[str, List[Dict[str, Any]]]:
        """
        æœç´¢å¤šä¸ªå…³é”®è¯
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            context_lines: ä¸Šä¸‹æ–‡è¡Œæ•°
            
        Returns:
            ä»¥å…³é”®è¯ä¸ºkeyçš„æœç´¢ç»“æœå­—å…¸
        """
        results = {}
        for keyword in keywords:
            results[keyword] = self.processor.search_content(keyword, context_lines)
        return results
    
    def generate_search_report(self, keywords: List[str]) -> str:
        """
        ç”Ÿæˆæœç´¢æŠ¥å‘Š
        
        Args:
            keywords: æœç´¢çš„å…³é”®è¯åˆ—è¡¨
            
        Returns:
            Markdownæ ¼å¼çš„æœç´¢æŠ¥å‘Š
        """
        search_results = self.search_multiple_keywords(keywords)
        
        report_lines = []
        report_lines.append(f"# ğŸ” å…³é”®è¯æœç´¢æŠ¥å‘Š")
        report_lines.append(f"")
        report_lines.append(f"**æœç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"**æœç´¢å…³é”®è¯**: {', '.join(keywords)}")
        report_lines.append(f"")
        
        for keyword, results in search_results.items():
            report_lines.append(f"## ğŸ¯ å…³é”®è¯: \"{keyword}\"")
            report_lines.append(f"")
            
            if not results:
                report_lines.append(f"âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
                report_lines.append(f"")
                continue
            
            report_lines.append(f"âœ… æ‰¾åˆ° {len(results)} å¤„åŒ¹é…")
            report_lines.append(f"")
            
            for i, result in enumerate(results[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ªç»“æœ
                report_lines.append(f"### ç»“æœ {i}")
                report_lines.append(f"**ä½ç½®**: ç¬¬ {result['line_number']} è¡Œ")
                report_lines.append(f"**åŒ¹é…å†…å®¹**: {result['matched_line']}")
                report_lines.append(f"")
                report_lines.append(f"**ä¸Šä¸‹æ–‡**:")
                report_lines.append(f"```")
                report_lines.append(result['context'])
                report_lines.append(f"```")
                report_lines.append(f"")
            
            if len(results) > 5:
                report_lines.append(f"... è¿˜æœ‰ {len(results) - 5} ä¸ªç»“æœ")
                report_lines.append(f"")
        
        return "\n".join(report_lines)

def main():
    """æµ‹è¯•å‡½æ•°"""
    processor = AdvancedDocumentProcessor()
    
    # æµ‹è¯•æ–‡æ¡£å¤„ç†
    test_file = "test_document.docx"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    if os.path.exists(test_file):
        try:
            result = processor.load_document(test_file)
            print("âœ… æ–‡æ¡£åŠ è½½æˆåŠŸ")
            
            # è·å–ç»“æ„æ‘˜è¦
            summary = processor.get_structure_summary()
            print(summary)
            
            # æµ‹è¯•æœç´¢
            search_results = processor.search_content("åˆåŒ")
            print(f"\næœç´¢ç»“æœ: {len(search_results)} ä¸ª")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    else:
        print("âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")

if __name__ == "__main__":
    main() 