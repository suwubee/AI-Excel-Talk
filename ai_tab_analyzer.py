#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AIåˆ†æTabä¸“ç”¨åˆ†æå™¨ - å¢å¼ºç‰ˆ
- æ™ºèƒ½è¯†åˆ«æ ‡å‡†è¡¨æ ¼ vs å¤æ‚è¡¨æ ¼
- å¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œå¤šè¡Œè¡¨å¤´
- AIæç¤ºè¯å‹å¥½è¾“å‡º
- è‡ªåŠ¨è¯†åˆ«ç­›é€‰é¡¹å­—æ®µ
"""

import openpyxl
from openpyxl.utils import get_column_letter
from collections import Counter

class AITabAnalyzer:
    """AIåˆ†æTabä¸“ç”¨çš„Excelåˆ†æå™¨ - å¢å¼ºç‰ˆ"""
    
    def analyze_for_ai(self, file_path: str) -> str:
        """
        ä¸ºAIåˆ†æç”Ÿæˆæ™ºèƒ½çš„Excelç»“æ„æè¿°
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            AIå‹å¥½çš„markdownæ ¼å¼æè¿°
        """
        try:
            wb = openpyxl.load_workbook(file_path, data_only=True)
            
            lines = []
            lines.append(f"# ğŸ“Š Excelæ•°æ®ç»“æ„åˆ†æ")
            lines.append(f"**æ–‡ä»¶**: {file_path.split('/')[-1].split('\\')[-1]}")
            lines.append("")
            
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                sheet_analysis = self._analyze_sheet_intelligent(ws, sheet_name)
                lines.append(sheet_analysis)
                lines.append("")
                
            return "\n".join(lines)
            
        except Exception as e:
            return f"# âŒ åˆ†æå¤±è´¥\n\né”™è¯¯ä¿¡æ¯: {str(e)}"
    
    def _analyze_sheet_intelligent(self, ws, sheet_name: str) -> str:
        """æ™ºèƒ½åˆ†æå·¥ä½œè¡¨"""
        # åŸºæœ¬ä¿¡æ¯
        rows, cols = ws.max_row, ws.max_column
        merged_ranges = list(ws.merged_cells.ranges)
        has_merges = len(merged_ranges) > 0
        
        # åˆ¤æ–­è¡¨æ ¼ç±»å‹
        is_standard = self._is_standard_table(ws, merged_ranges)
        
        result = []
        result.append(f"## ğŸ“‹ {sheet_name}")
        result.append(f"**è§„æ¨¡**: {rows}è¡Œ Ã— {cols}åˆ—")
        
        if is_standard:
            result.append(f"**ç±»å‹**: ğŸŸ¢æ ‡å‡†äºŒç»´è¡¨æ ¼")
            analysis = self._analyze_standard_table(ws)
        else:
            result.append(f"**ç±»å‹**: ğŸŸ¡å¤æ‚è¡¨æ ¼ (å«{len(merged_ranges)}ä¸ªåˆå¹¶å•å…ƒæ ¼)")
            analysis = self._analyze_complex_table(ws, merged_ranges)
        
        result.extend(analysis)
        return "\n".join(result)
    
    def _is_standard_table(self, ws, merged_ranges) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡å‡†äºŒç»´è¡¨æ ¼"""
        # å¦‚æœæœ‰åˆå¹¶å•å…ƒæ ¼ï¼Œä¸æ˜¯æ ‡å‡†è¡¨æ ¼
        if len(merged_ranges) > 0:
            return False
        
        # æ£€æŸ¥å‰5è¡Œï¼Œæ‰¾åˆ°æœ€å¯èƒ½çš„è¡¨å¤´è¡Œ
        best_header_row = self._find_header_by_data_consistency(ws)
        
        # å¦‚æœæ‰¾åˆ°äº†æ˜ç¡®çš„è¡¨å¤´è¡Œï¼Œå°±æ˜¯æ ‡å‡†è¡¨æ ¼
        return best_header_row is not None
    
    def _find_header_by_data_consistency(self, ws) -> int:
        """é€šè¿‡æ•°æ®ä¸€è‡´æ€§åˆ†ææ‰¾åˆ°è¡¨å¤´è¡Œ"""
        for row in range(1, min(6, ws.max_row + 1)):
            if self._is_likely_header_row(ws, row):
                return row
        return None
    
    def _is_likely_header_row(self, ws, row: int) -> bool:
        """åˆ¤æ–­æŸä¸€è¡Œæ˜¯å¦å¯èƒ½æ˜¯è¡¨å¤´è¡Œï¼ˆåŸºäºç»Ÿè®¡å­¦åˆ†æçš„é€šç”¨æ–¹æ³•ï¼‰"""
        if row >= ws.max_row:
            return False
            
        # 1. æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦æœ‰è¶³å¤Ÿçš„éç©ºå•å…ƒæ ¼
        current_row_data = []
        for col in range(1, min(ws.max_column + 1, 20)):
            cell = ws.cell(row, col)
            value = str(cell.value).strip() if cell.value else ""
            current_row_data.append(value)
        
        non_empty_current = sum(1 for v in current_row_data if v)
        if non_empty_current < 3:  # è‡³å°‘è¦æœ‰3ä¸ªéç©ºå­—æ®µ
            return False
        
        # 2. å…³é”®ï¼šæ¯”è¾ƒå½“å‰è¡Œä¸åç»­æ•°æ®è¡Œçš„ç‰¹å¾å·®å¼‚
        if row + 2 > ws.max_row:  # è‡³å°‘éœ€è¦2è¡Œæ•°æ®æ¥åšæ¯”è¾ƒ
            # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¡Œï¼Œç”¨æ›´å®½æ¾çš„åˆ¤æ–­
            return self._analyze_single_row_header_likelihood(current_row_data) >= 0.6
            
        # 3. ç»Ÿè®¡å­¦åˆ†æï¼šæ¯”è¾ƒå½“å‰è¡Œä¸æ•°æ®è¡Œçš„å·®å¼‚
        header_likelihood = self._analyze_header_vs_data_pattern(ws, row, current_row_data)
        
        return header_likelihood >= 0.5  # é™ä½é˜ˆå€¼ï¼Œæ›´å®½æ¾çš„åˆ¤æ–­
    
    def _analyze_single_row_header_likelihood(self, row_data: list) -> float:
        """å½“æ²¡æœ‰è¶³å¤Ÿæ•°æ®è¡Œæ—¶ï¼Œåˆ†æå•è¡Œä½œä¸ºè¡¨å¤´çš„å¯èƒ½æ€§"""
        if not row_data:
            return 0.0
        
        scores = []
        for value in row_data:
            if not value.strip():
                continue
                
            score = 0.0
            
            # é•¿åº¦è¯„åˆ†ï¼šè¡¨å¤´å­—æ®µé€šå¸¸è¾ƒçŸ­
            if len(value) <= 10:
                score += 0.3
            elif len(value) <= 20:
                score += 0.1
            
            # å¤æ‚åº¦è¯„åˆ†ï¼šè¡¨å¤´å­—æ®µé€šå¸¸è¾ƒç®€å•
            if not any(c in value for c in ['ã€‚', 'ï¼Œ', 'ï¼›', 'ï¼š', 'ï¼Ÿ', 'ï¼', '.', ',', ';', ':', '?', '!']):
                score += 0.3
            
            # æ•°å­—è¯„åˆ†ï¼šçº¯é•¿æ•°å­—é€šå¸¸ä¸æ˜¯è¡¨å¤´
            if self._is_numeric(value) and len(value) > 4:
                score -= 0.2
            
            # æ—¥æœŸè¯„åˆ†ï¼šæ—¥æœŸæ ¼å¼é€šå¸¸ä¸æ˜¯è¡¨å¤´
            if self._looks_like_date(value):
                score -= 0.3
            
            scores.append(max(0.0, min(1.0, score)))  # é™åˆ¶åœ¨0-1ä¹‹é—´
        
        return sum(scores) / len(scores) if scores else 0.0
    
    def _analyze_header_vs_data_pattern(self, ws, header_row: int, header_data: list) -> float:
        """åˆ†æè¡¨å¤´è¡Œä¸æ•°æ®è¡Œçš„æ¨¡å¼å·®å¼‚"""
        # æ”¶é›†åç»­æ•°æ®è¡Œ
        data_rows = []
        for row in range(header_row + 1, min(header_row + 6, ws.max_row + 1)):
            row_data = []
            for col in range(1, len(header_data) + 1):
                cell = ws.cell(row, col)
                value = str(cell.value).strip() if cell.value else ""
                row_data.append(value)
            if any(v for v in row_data):  # åªè¦æœ‰éç©ºæ•°æ®
                data_rows.append(row_data)
        
        if not data_rows:
            return 0.0
        
        # åˆ†æå·®å¼‚å¾—åˆ†
        differences = []
        
        for col_idx in range(len(header_data)):
            if not header_data[col_idx]:
                continue
                
            header_value = header_data[col_idx]
            data_values = [row[col_idx] for row in data_rows if col_idx < len(row) and row[col_idx]]
            
            if not data_values:
                continue
            
            # è®¡ç®—è¯¥åˆ—è¡¨å¤´ä¸æ•°æ®çš„å·®å¼‚ç¨‹åº¦
            diff_score = self._calculate_column_difference(header_value, data_values)
            differences.append(diff_score)
        
        return sum(differences) / len(differences) if differences else 0.0
    
    def _calculate_column_difference(self, header_value: str, data_values: list) -> float:
        """è®¡ç®—è¡¨å¤´å€¼ä¸æ•°æ®å€¼çš„å·®å¼‚ç¨‹åº¦"""
        score = 0.0
        
        # 1. é•¿åº¦å·®å¼‚ï¼šè¡¨å¤´é€šå¸¸æ¯”æ•°æ®çŸ­æˆ–ç›¸è¿‘
        avg_data_len = sum(len(v) for v in data_values) / len(data_values)
        if len(header_value) <= avg_data_len:
            score += 0.3
        
        # 2. ç±»å‹å·®å¼‚ï¼šè¡¨å¤´ä¸æ•°æ®ç±»å‹ä¸åŒå¾—åˆ†é«˜
        header_type = self._classify_data_type(header_value)
        data_types = [self._classify_data_type(v) for v in data_values]
        data_type_counts = {}
        for dt in data_types:
            data_type_counts[dt] = data_type_counts.get(dt, 0) + 1
        
        if data_type_counts:
            main_data_type = max(data_type_counts.keys(), key=lambda k: data_type_counts[k])
            if header_type != main_data_type:
                score += 0.4
        
        # 3. é‡å¤æ€§å·®å¼‚ï¼šè¡¨å¤´é€šå¸¸ä¸é‡å¤ï¼Œæ•°æ®å¯èƒ½é‡å¤
        data_unique_ratio = len(set(data_values)) / len(data_values)
        if data_unique_ratio < 0.8:  # æ•°æ®æœ‰é‡å¤
            score += 0.3
        
        return min(1.0, score)
    
    def _analyze_data_consistency(self, ws, start_row: int, num_cols: int) -> float:
        """åˆ†ææ•°æ®çš„ä¸€è‡´æ€§ç¨‹åº¦"""
        if start_row > ws.max_row:
            return 0.0
        
        # æ£€æŸ¥åç»­5è¡Œçš„æ•°æ®ç±»å‹ä¸€è‡´æ€§
        col_types = []  # æ¯åˆ—çš„æ•°æ®ç±»å‹ç»Ÿè®¡
        
        for col in range(1, min(num_cols + 1, 20)):
            types_in_col = []
            
            # å–æ ·5è¡Œæ•°æ®
            for row in range(start_row, min(start_row + 5, ws.max_row + 1)):
                cell = ws.cell(row, col)
                if cell.value is not None:
                    value_str = str(cell.value).strip()
                    if value_str:
                        data_type = self._classify_data_type(value_str)
                        types_in_col.append(data_type)
            
            if types_in_col:
                # è®¡ç®—è¯¥åˆ—æ•°æ®ç±»å‹çš„ä¸€è‡´æ€§
                type_counts = {}
                for t in types_in_col:
                    type_counts[t] = type_counts.get(t, 0) + 1
                
                # ä¸»è¦ç±»å‹å æ¯”
                max_count = max(type_counts.values())
                consistency = max_count / len(types_in_col)
                col_types.append(consistency)
        
        # è¿”å›å¹³å‡ä¸€è‡´æ€§
        return sum(col_types) / len(col_types) if col_types else 0.0
    
    def _classify_data_type(self, value_str: str) -> str:
        """åˆ†ç±»æ•°æ®ç±»å‹"""
        if self._is_numeric(value_str):
            return "numeric"
        elif self._looks_like_date(value_str):
            return "date"
        elif len(value_str) <= 5:
            return "short_text"
        elif len(value_str) <= 20:
            return "medium_text"
        else:
            return "long_text"
    
    def _analyze_header_quality(self, row_data: list) -> float:
        """åˆ†æè¡¨å¤´è´¨é‡"""
        if not row_data:
            return 0.0
        
        valid_headers = 0
        total_non_empty = 0
        
        for value in row_data:
            if value.strip():
                total_non_empty += 1
                # åªä½¿ç”¨ä¸¥æ ¼çš„è¡¨å¤´æ£€æµ‹é€»è¾‘ï¼Œå»æ‰é•¿åº¦å…œåº•
                if self._looks_like_header(value):
                    valid_headers += 1
        
        return valid_headers / total_non_empty if total_non_empty > 0 else 0.0
    
    def _analyze_standard_table(self, ws) -> list:
        """åˆ†ææ ‡å‡†äºŒç»´è¡¨æ ¼"""
        result = []
        
        # æ‰¾è¡¨å¤´
        header_row = self._find_best_header_row(ws)
        header_cell = f"{get_column_letter(1)}{header_row}"
        result.append(f"**ğŸ“ è¡¨å¤´ä½ç½®**: `{header_cell}` (ç¬¬{header_row}è¡Œ)")
        
        # æ•°æ®èµ·å§‹ä½ç½®
        data_start_row = header_row + 1
        data_start_cell = f"{get_column_letter(1)}{data_start_row}"
        result.append(f"**ğŸ“ æ•°æ®èµ·å§‹ä½ç½®**: `{data_start_cell}` (ç¬¬{data_start_row}è¡Œç¬¬1åˆ—)")
        
        # æå–å­—æ®µ
        fields = self._extract_standard_fields(ws, header_row)
        
        result.append("")
        result.append("**ğŸ·ï¸ å­—æ®µç»“æ„**:")
        
        for i, field in enumerate(fields, 1):
            field_info = f"{i}. `{field['col']}åˆ—` **{field['name']}**"
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç­›é€‰é¡¹
            if field['unique_values']:
                field_info += f" (ç­›é€‰é¡¹: {', '.join(map(str, field['unique_values']))})"
            elif field['sample_values']:
                field_info += f" (ç¤ºä¾‹: {', '.join(map(str, field['sample_values'][:3]))}...)"
            
            result.append(field_info)
        
        return result
    
    def _analyze_complex_table(self, ws, merged_ranges) -> list:
        """åˆ†æå¤æ‚è¡¨æ ¼"""
        result = []
        
        # åˆ†æåˆå¹¶å•å…ƒæ ¼ç»“æ„
        merge_analysis = self._analyze_merged_structure(ws, merged_ranges)
        
        # æ·»åŠ åˆå¹¶å•å…ƒæ ¼è¯¦ç»†ä¿¡æ¯
        if merged_ranges:
            result.append("**ğŸ”— åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯**:")
            for i, merged_range in enumerate(merged_ranges[:15], 1):  # æœ€å¤šæ˜¾ç¤º15ä¸ª
                min_row, min_col = merged_range.min_row, merged_range.min_col
                max_row, max_col = merged_range.max_row, merged_range.max_col
                
                start_cell = f"{get_column_letter(min_col)}{min_row}"
                end_cell = f"{get_column_letter(max_col)}{max_row}"
                
                cell = ws.cell(min_row, min_col)
                value = str(cell.value)[:30] if cell.value else "(ç©º)"
                if len(str(cell.value or "")) > 30:
                    value += "..."
                
                result.append(f"  {i}. `{start_cell}:{end_cell}` â†’ \"{value}\"")
            
            if len(merged_ranges) > 15:
                result.append(f"  ... è¿˜æœ‰{len(merged_ranges) - 15}ä¸ªåˆå¹¶å•å…ƒæ ¼")
            result.append("")
        
        # æ‰¾åˆ°æ•°æ®åŒºåŸŸ
        data_start = self._find_data_start(ws, merged_ranges)
        data_start_cell = f"{get_column_letter(data_start['col'])}{data_start['row']}"
        result.append(f"**ğŸ“ æ•°æ®èµ·å§‹ä½ç½®**: `{data_start_cell}` (ç¬¬{data_start['row']}è¡Œç¬¬{data_start['col']}åˆ—)")
        
        # æå–å¤æ‚å­—æ®µç»“æ„
        fields = self._extract_complex_fields(ws, merge_analysis, data_start)
        
        result.append("")
        result.append("**ğŸ·ï¸ å­—æ®µç»“æ„** (æ™ºèƒ½è§£æ):")
        
        # ä¸»è¦å­—æ®µ
        main_fields = [f for f in fields if f['importance'] == 'main']
        if main_fields:
            result.append("*æ ¸å¿ƒå­—æ®µ* (æ’é™¤åˆå¹¶å•å…ƒæ ¼å):")
            for i, field in enumerate(main_fields, 1):
                # æ·»åŠ å­—æ®µèµ·å§‹ä½ç½®ä¿¡æ¯
                field_start_pos = self._get_field_start_position(ws, field['col'], merged_ranges)
                field_info = f"  {i}. `{field['col']}åˆ—` **{field['name']}** _(ä»{field_start_pos}å¼€å§‹)_"
                
                if field['unique_values']:
                    field_info += f" (ç­›é€‰é¡¹: {', '.join(map(str, field['unique_values']))})"
                elif field['sample_values']:
                    field_info += f" (ç¤ºä¾‹: {', '.join(map(str, field['sample_values'][:2]))}...)"
                result.append(field_info)
        
        # è¾…åŠ©å­—æ®µ
        aux_fields = [f for f in fields if f['importance'] == 'aux']
        if aux_fields:
            result.append("")
            result.append("*è¾…åŠ©å­—æ®µ*:")
            for field in aux_fields:
                result.append(f"  â€¢ `{field['col']}åˆ—` {field['name']}")
        
        # å­—æ®µå…³ç³»è¯´æ˜
        if len(fields) > 8:
            result.append("")
            result.append(f"**ğŸ“ˆ å­—æ®µé€’è¿›å…³ç³»**: Aåˆ—â†’Båˆ—â†’Cåˆ—...â†’{fields[-1]['col']}åˆ—")
            result.append(f"Fåˆ—ä¹‹åçš„å­—æ®µ: {' â†’ '.join([f['name'] for f in fields[5:]])}")
        
        return result
    
    def _analyze_merged_structure(self, ws, merged_ranges):
        """åˆ†æåˆå¹¶å•å…ƒæ ¼ç»“æ„"""
        merge_info = {}
        for merged_range in merged_ranges:
            min_row, min_col = merged_range.min_row, merged_range.min_col
            max_row, max_col = merged_range.max_row, merged_range.max_col
            
            cell = ws.cell(min_row, min_col)
            value = str(cell.value) if cell.value else ""
            
            merge_info[f"{min_row}_{min_col}"] = {
                'range': (min_row, min_col, max_row, max_col),
                'value': value,
                'span_rows': max_row - min_row + 1,
                'span_cols': max_col - min_col + 1
            }
        
        return merge_info
    
    def _find_data_start(self, ws, merged_ranges):
        """æ‰¾åˆ°çœŸå®æ•°æ®å¼€å§‹çš„ä½ç½®"""
        # è·³è¿‡æ ‡é¢˜å’Œè¡¨å¤´åŒºåŸŸï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰è¿ç»­æ•°æ®çš„è¡Œ
        for row in range(1, min(10, ws.max_row + 1)):
            consecutive_data = 0
            start_col = 1
            
            for col in range(1, min(ws.max_column + 1, 20)):
                cell = ws.cell(row, col)
                if cell.value is not None and str(cell.value).strip():
                    consecutive_data += 1
                    if consecutive_data >= 5:  # è¿ç»­5ä¸ªæœ‰æ•°æ®çš„å•å…ƒæ ¼
                        return {'row': row, 'col': start_col}
                else:
                    consecutive_data = 0
                    start_col = col + 1
        
        return {'row': 2, 'col': 1}  # é»˜è®¤å€¼
    
    def _extract_complex_fields(self, ws, merge_analysis, data_start):
        """æå–å¤æ‚è¡¨æ ¼çš„å­—æ®µ"""
        fields = []
        header_rows = [2, 3]  # demo2ä¸­ä¸»è¦æ˜¯ç¬¬2ã€3è¡Œä½œä¸ºè¡¨å¤´
        
        for col in range(1, min(ws.max_column + 1, 50)):  # é™åˆ¶åœ¨50åˆ—å†…
            field_name = self._get_complex_field_name(ws, col, header_rows, merge_analysis)
            
            if field_name:
                # åˆ¤æ–­å­—æ®µé‡è¦æ€§
                importance = 'main' if col <= 16 else 'aux'  # å‰16åˆ—ä¸ºä¸»è¦å­—æ®µ
                
                # æå–æ ·æœ¬æ•°æ®
                sample_values, unique_values = self._extract_field_data(ws, col, data_start['row'])
                
                fields.append({
                    'col': get_column_letter(col),
                    'name': field_name,
                    'importance': importance,
                    'sample_values': sample_values,
                    'unique_values': unique_values
                })
        
        return fields
    
    def _get_complex_field_name(self, ws, col, header_rows, merge_analysis):
        """è·å–å¤æ‚è¡¨æ ¼çš„å­—æ®µå"""
        names = []
        
        for row in header_rows:
            cell = ws.cell(row, col)
            if cell.value is not None:
                value = str(cell.value).strip()
                if value and len(value) > 0:
                    names.append(value)
        
        if not names:
            return None
        
        # æ™ºèƒ½åˆå¹¶å¤šä¸ªè¡¨å¤´å±‚çº§
        if len(names) == 1:
            return names[0]
        elif len(names) == 2:
            # å¦‚æœç¬¬äºŒä¸ªåç§°æ˜¯ç¬¬ä¸€ä¸ªçš„ç»†åˆ†ï¼Œç”¨"-"è¿æ¥
            if names[0] and names[1]:
                return f"{names[0]}-{names[1]}"
            else:
                return names[0] or names[1]
        else:
            return " | ".join([n for n in names if n])
    
    def _extract_standard_fields(self, ws, header_row):
        """æå–æ ‡å‡†è¡¨æ ¼å­—æ®µ"""
        fields = []
        
        for col in range(1, min(ws.max_column + 1, 30)):
            cell = ws.cell(header_row, col)
            if cell.value is not None:
                field_name = str(cell.value).strip()
                if field_name:
                    # æå–å­—æ®µæ•°æ®ï¼ˆä»è¡¨å¤´ä¸‹ä¸€è¡Œå¼€å§‹ï¼‰
                    sample_values, unique_values = self._extract_field_data(ws, col, header_row + 1)
                    
                    fields.append({
                        'col': get_column_letter(col),
                        'name': field_name,
                        'sample_values': sample_values,
                        'unique_values': unique_values
                    })
        
        return fields
    
    def _extract_field_data(self, ws, col, start_row):
        """æå–å­—æ®µçš„æ ·æœ¬æ•°æ®å’Œå”¯ä¸€å€¼"""
        values = []
        
        # æå–å‰100è¡Œçš„æ•°æ®ä½œä¸ºæ ·æœ¬
        for row in range(start_row, min(start_row + 100, ws.max_row + 1)):
            cell = ws.cell(row, col)
            if cell.value is not None:
                value = str(cell.value).strip()
                if value:
                    values.append(value)
        
        if not values:
            return [], []
        
        # ç»Ÿè®¡å”¯ä¸€å€¼
        unique_counts = Counter(values)
        unique_values = list(unique_counts.keys())
        
        # å¦‚æœå”¯ä¸€å€¼å°‘äºç­‰äº10ä¸ªï¼Œè¿”å›æ‰€æœ‰å”¯ä¸€å€¼ä½œä¸ºç­›é€‰é¡¹
        if len(unique_values) <= 10:
            return [], unique_values[:10]  # è¿”å›å‰10ä¸ªä½œä¸ºç­›é€‰é¡¹
        else:
            return values[:5], []  # è¿”å›å‰5ä¸ªä½œä¸ºæ ·æœ¬
    
    def _find_best_header_row(self, ws) -> int:
        """æ‰¾åˆ°æœ€ä½³è¡¨å¤´è¡Œï¼ˆåŸºäºæ•°æ®ä¸€è‡´æ€§åˆ†æï¼‰"""
        # ä¼˜å…ˆä½¿ç”¨æ•°æ®ä¸€è‡´æ€§åˆ†æ
        header_row = self._find_header_by_data_consistency(ws)
        if header_row:
            return header_row
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ä½œä¸ºå…œåº•
        best_row = 1
        best_score = 0
        
        for row in range(1, min(6, ws.max_row + 1)):
            score = 0
            non_empty = 0
            
            for col in range(1, min(ws.max_column + 1, 20)):
                cell = ws.cell(row, col)
                if cell.value is not None:
                    non_empty += 1
                    value_str = str(cell.value).strip()
                    
                    # ä½¿ç”¨æ”¹è¿›çš„è¡¨å¤´æ£€æµ‹
                    if self._looks_like_header(value_str):
                        score += 2.0
                    elif not self._is_numeric(value_str) and len(value_str) <= 20:
                        score += 1.0
                    else:
                        score += 0.3
            
            if non_empty > 0:
                final_score = score * non_empty
                if final_score > best_score:
                    best_score = final_score
                    best_row = row
        
        return best_row
    
    def _looks_like_header(self, value_str: str) -> bool:
        """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦åƒè¡¨å¤´å­—æ®µåï¼ˆé€šç”¨æ–¹æ³•ï¼‰"""
        if not value_str or len(value_str) > 100:  # å¤ªé•¿çš„ä¸å¤ªåƒå­—æ®µå
            return False
        
        value_str = value_str.strip()
        
        # 1. æ—¥æœŸæ ¼å¼é€šå¸¸ä¸æ˜¯è¡¨å¤´å­—æ®µå
        if self._looks_like_date(value_str):
            return False
        
        # 2. çº¯æ•°å­—é€šå¸¸ä¸æ˜¯è¡¨å¤´ï¼ˆé™¤éå¾ˆçŸ­ï¼‰
        if self._is_numeric(value_str):
            if len(value_str) > 4:  # è¶…è¿‡4ä½æ•°å­—ä¸å¤ªåƒè¡¨å¤´
                return False
            elif len(value_str) > 2 and not any(c.isalpha() for c in value_str):
                return False  # çº¯æ•°å­—ä¸”è¶…è¿‡2ä½
        
        # 3. åŒ…å«å¤æ‚æ ‡ç‚¹çš„ä¸æ˜¯è¡¨å¤´
        complex_patterns = [
            'ã€‚', 'ï¼Œ', 'ï¼›', 'ï¼š', 'ï¼Ÿ', 'ï¼',  # ä¸­æ–‡æ ‡ç‚¹
            '.', ',', ';', '?', '!',           # è‹±æ–‡æ ‡ç‚¹  
            '\n', '\r', '\t',                 # æ¢è¡Œç¬¦
            '-', '/', '\\', '|'               # åˆ†éš”ç¬¦ï¼ˆæ—¥æœŸæ—¶é—´å¸¸ç”¨ï¼‰
        ]
        if any(pattern in value_str for pattern in complex_patterns):
            return False
        
        # 4. æ’é™¤æ˜æ˜¾çš„æ•°æ®å†…å®¹æ¨¡å¼
        if self._looks_like_data_content(value_str):
            return False
        
        # 5. é•¿åº¦ç‰¹å¾ï¼šè¡¨å¤´å­—æ®µé€šå¸¸æ¯”è¾ƒç®€æ´
        if len(value_str) <= 20:
            return True
            
        return False
    
    def _looks_like_data_content(self, value_str: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åƒæ•°æ®å†…å®¹è€Œä¸æ˜¯è¡¨å¤´ï¼ˆé€šç”¨æ–¹æ³•ï¼Œä¸ä¾èµ–å…·ä½“è¯æ±‡ï¼‰"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨åªåšæœ€åŸºæœ¬çš„æ¨¡å¼è¯†åˆ«ï¼Œé¿å…ç¡¬ç¼–ç 
        return False  # æš‚æ—¶ç¦ç”¨ï¼Œè®©å…¶ä»–é€»è¾‘æ¥åˆ¤æ–­
    
    def _looks_like_date(self, value_str: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åƒæ—¥æœŸæ ¼å¼"""
        import re
        date_patterns = [
            r'^\d{8}$',           # 20250527
            r'^\d{4}-\d{2}-\d{2}$', # 2025-05-27
            r'^\d{4}/\d{2}/\d{2}$', # 2025/05/27
            r'^\d{4}\.\d{2}\.\d{2}$' # 2025.05.27
        ]
        for pattern in date_patterns:
            if re.match(pattern, value_str):
                return True
        return False
    
    def _is_numeric(self, value_str: str) -> bool:
        """æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºæ•°å­—"""
        try:
            float(value_str)
            return True
        except (ValueError, TypeError):
            return False
    
    def _get_field_start_position(self, ws, col_letter, merged_ranges):
        """è·å–å­—æ®µå®é™…å¼€å§‹çš„ä½ç½®ï¼ˆæ’é™¤åˆå¹¶å•å…ƒæ ¼ï¼‰"""
        from openpyxl.utils import column_index_from_string
        col_num = column_index_from_string(col_letter)
        
        # æ£€æŸ¥å‰å‡ è¡Œï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªéåˆå¹¶ä¸”æœ‰å†…å®¹çš„å•å…ƒæ ¼
        for row in range(1, min(10, ws.max_row + 1)):
            cell = ws.cell(row, col_num)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨åˆå¹¶å•å…ƒæ ¼ä¸­
            is_in_merge = False
            for merged_range in merged_ranges:
                if (merged_range.min_row <= row <= merged_range.max_row and 
                    merged_range.min_col <= col_num <= merged_range.max_col):
                    is_in_merge = True
                    break
            
            # å¦‚æœä¸åœ¨åˆå¹¶å•å…ƒæ ¼ä¸­ä¸”æœ‰å†…å®¹ï¼Œè¿™å°±æ˜¯èµ·å§‹ä½ç½®
            if not is_in_merge and cell.value is not None:
                return f"{col_letter}{row}"
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤ä½ç½®
        return f"{col_letter}1"

# ä½¿ç”¨ç¤ºä¾‹
def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    analyzer = AITabAnalyzer()
    
    print("ğŸ¤– AIåˆ†æTab - æ™ºèƒ½Excelåˆ†æå™¨")
    print("=" * 60)
    
    # åˆ†ædemo1
    print("\nğŸ“„ demo1.xlsx åˆ†æç»“æœ:")
    result1 = analyzer.analyze_for_ai('demo1.xlsx')
    print(result1)
    
    print("\n" + "="*60)
    
    # åˆ†ædemo2  
    print("\nğŸ“„ demo2.xlsx åˆ†æç»“æœ:")
    result2 = analyzer.analyze_for_ai('demo2.xlsx')
    print(result2)

if __name__ == "__main__":
    main() 